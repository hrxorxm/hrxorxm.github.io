---
layout: post
title:  "[Boostcamp AI Tech] 25일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day25"
categories: "Boostcamp-AI-Tech"
tags: [6주차, Level2-U-Stage]
use_math: true
---

# 부스트캠프 25일차

## 📝 오늘 일정 정리

* 9/6(월)
  - [x] MeetUP 10:00~11:00
  - [x] NLP 이론 강의
    - [x] (01강) Introduction to NLP & Bag-of-Words
    - [x] (02강) Word Embedding : Word2Vec, GloVe
    - [x] [실습] NaiveBayes Classifier, Word2Vec
    - [x] [필수 과제 1] Data Preprocessing
  - [x] MeetUP - 부캠에서 살아남기

## 📢 MeetUP

* 그라운드 룰
  * 피어세션 : 4시 반
  * 피어세션 정리 : 노션
  * 슬랙에서 메세지 확인하면 이모지 남겨주기
  * 미리 말하면 괜찮고, 말 없이 지각/결석하면 깊티 랜덤 1명 쏘기 (편의점 깊티)
* 피어세션 때 할 것
  * 1주차는 지금 배운 것들을 정리해서 설명
  * 2주차는 Transformer, BERT, GPT 계열 논문 읽으면 어떨까
  * 강의 수강 절차
    * 월 / 화 / 수 : part1,2,3
    * 목 / 금 : 보충 학습 or 트랜스포머 예습
  * 강의 들은거 질의응답
  * 과제 코드 리뷰 : 본인 코드 설명, 다르게 짠 사람들 위주로 발표

## 📚 강의 내용 정리

### [1강] Introduction to NLP & Bag-of-Words

#### NLP 분야

* NLP(Natural language processing) (major conferences: ACL, EMNLP, NAACL)
  * 종류
    * NLU(Natural Language Understanding) : 컴퓨터가 자연어를 이해하는 것
    * NLG(Natural Language Generation) : 자연어를 상황에 따라 적절히 생성
  * Low-level parsing
    * Tokenization : 주어진 문장을 단어 단위로 쪼개나가는 과정
    * stemming : 단어의 어근을 추출하는 과정
  * Word and phrase level
    * NER(Named entity recognition) : 고유명사 인식
    * POS(part of speech) tagging : 문장 내 품사나 성분을 알아내는 것
    * noun-phrase chunking, dependency parsing, coreference resolution 등
  * Sentence lebel
    * Sentiment analysis : 긍정/부정 감정분석
    * machine translation : 기계번역
  * Multi-sentence and paragraph level
    * Entailment prediction : 두 문장간의 논리적인 내포, 모순관계 파악
    * question answering : 독해기반의 질의응답
    * dialog systems : 챗봇과 같이 대화를 수행할 수 있는 기술
    * summarization : 주어진 문서를 요약
* Text mining (major conferences: KDD, The WebConf(formerly, WWW), WSDM, CIKM, ICWSM)
  * 텍스트, 문서 데이터를 통해 유용한 (주로 사회과학적인) 인사이트를 얻는 것
  * Document clustering (topic modeling) : 문서 군집화
* Information retrieval (major conferences: SIGIR, WSDM, CIKM, RecSys)
  * 구글, 네이버 등이 사용하는 검색기술
  * 추천 시스템 기술 : 사용자의 수동 검색 없이 정보 제공, 적극적이고 자동화된 검색 시스템의 새로운 형태

#### NLP 발전 과정

* 주어진 텍스트를 벡터로 나타내는 기술 : Word2Vec / GloVe
* 주요 아키텍쳐 : RNN-family models : LSTMs, GRUs
* 트랜스포머의 등장 : attention modules, Transformer models
* self-supervised 방식으로 사전학습 시킨 범용적인 모델 : BERT, GPT-3
* 위 사전학습 모델을 이용한 전이학습

#### 문서 분류 예제

* Bag-of-Words
  * Step 1 : unique한 단어들을 모아서 vocabulary를 만든다.
  * Step 2 : 각 categorical 변수들을 one-hot 벡터로 표현한다.
    * 단어의 의미에 상관없이 모두 동일한 관계를 가진 형태가 된다.
    * 단어 간의 거리는 모두 $\sqrt{2}$, 단어 간의 코사인 유사도는 모두 0
  * Step 3 : 문장/문서에 나온 단어들의 one-hot 벡터들을 모두 더한다. = Bag-of-Words 벡터
* NaiveBayes Classifier for Document Classification
  * 문서 d, 클래스 c
  * $C_{MAP} = \underset{c \in C}{argmax} P(c \| d)$ ← MAP(Maximum a posterion) : 가장 예상되는 클래스
    * $= \underset{c \in C}{argmax} \frac{P(d \| c) P(c)}{P(d)}$ ← Bayes Rule
    * $= \underset{c \in C}{argmax} P(d \| c) P(c)$ ← Dropping the denominator
  * $P(d \| c) p(c) = P(w_1, w_2, ..., w_n) P(c) \rightarrow P(c) \Pi_{w_i \in W} P(w_i \| c)$ ← 조건부 독립 가정
  * 특정한 단어가 전혀 나타나지 않았을 경우 그 클래스가 될 확률이 무조건 0이 되는 경우가 발생할 수 있으므로 추가적인 regularizer와 함께 쓰이기도 한다.

### [02강] Word Embedding

* Word Embedding : 각 단어들을 특정 차원으로 이루어진 공간상의 한 점의 좌표를 나타내는 벡터로 변환해주는 기법

#### [Word2Vec](https://arxiv.org/abs/1310.4546)

* 알고리즘
  * 비슷한 의미를 가진 단어가 좌표공간 상에서 비슷한 위치로 맵핑되도록 하기 위한 알고리즘
  * 같은 문장에서 나타난 인접한 문장들 간의 의미가 비슷할 것이라는 가정
  * 한 단어의 주변에 등장하는 단어들을 통해 그 단어의 의미를 알 수 있다.
* 학습 방법
  * 단어 주변에 나타나는 단어들의 확률 분포를 예측한다.
  * 단어를 입력으로 주고 주변 단어들을 가린 채 이를 예측하도록 하는 방식으로 학습

#### [GloVe](https://aclanthology.org/D14-1162/)

* Global Vectors for Word Representation
* 새로운 형태의 loss function 사용, 단어가 같이 등장한 횟수를 미리 계산 (학습이 더 빠름)
* $J(\Theta) = \frac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^T v_j - \log{P_{ij}})^2$

## 🔎 과제 수행 과정

* 필수과제1 : 데이터 전처리
  * [Spacy](https://spacy.io/) : 영어 데이터 전처리
    * `en_core_web_sm` : "영어(en)", "사용목적(core)", "훈련 데이터셋 출처(web)", "모델 크기(small)" 의미
    * [token class의 attributes](https://spacy.io/api/token#attributes)
  * [Konlpy](https://konlpy.org/ko/latest/) : 한국어 데이터 전처리

## 🌱 피어 세션 정리

* 강의 관련 논의
  * Word2Vec과 GloVe 알고리즘이 가지고 있는 단점은 무엇일까요?
    * https://wikidocs.net/22885
* 과제 관련 논의
  * 필수과제1번
    * spacy 3.0 최신버전에서는 tokenizer에서 s(복수형)을 지워버리기 때문에 answer하고 맞지 않게 된다.
  * 한국어 전처리
    * mecab : 훨씬 빠르고, 제일 많이 사용됨
    * twitter → okt : normalize, stemming 가능
* 기타
  * 자연어 처리 공부 추천 : [딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)
  * 회의록 기록 : [클로바 노트](https://clovanote.naver.com/home)

## 🐣 MeetUp - 부캠에서 살아남기

### 부스트캠프와 공유문화

* 부스트캠프 수료증은 취업 프리패스권이 아니다.
* 부스트캠프 수료 후에 나는 무엇을 얻어갈 것인가?
* 의미있는 무언가에 집중하자
* 결과보다는 무엇을/어떻게/왜 가 더 중요하다.
* 잘못된 정보를 공유할까 두려움 -> 오히려 좋아! 내가 성장할 기회

### 협업과 프로젝트 진행 꿀팁

* P-stage에 임하는 Tip
  * 순위는 그저 동기부여, 순위보다는 기록에 집중하기
  * 논리적 근거가 있는 실험의 밑바탕
* 좋은 Team을 꾸리는 Tip
  * 케미가 잘 맞는 Team!
  * 내가 어떤 사람인지 먼저 보여주자!
  * 랜덤 피어세션도 핵심 시간
* 피어세션 시간을 재밌게 보내기
  * TMI 시간으로 아이스브레이킹
  * 본인 아이디어나 상황을 잘 정리하고 효율적으로 회의시간 활용하기
  * 회의록 작성하기

### 취업 팁

* 자기소개서
  * 소소한 성과, 기본기와 태도, 가독성
  * Wrap-up 레포트 재구성해보기
* 이력서
  * 핵심만 간단히 담백하게
  * 결과보다는 과정 요약, 수행 역할 명확히 (간략한 스토리라인)
* 깃헙
  * 깃헙은 거의 필수
  * 개인 프로젝트 결과 정리
* 포트폴리오
  * 이력서에 담지 못한 것, 태도, 시각자료
* 면접 준비
  * 기술 질문 : 기본적이지만 대답하기 어려운 질문, 답 못하면 광탈
  * 프로젝트 질문 : 프로젝트 요약 질문, 활용 기술 알고 사용했는지, 왜 사용했는지
  * 생각 정리하기 : 면접 스터디, 부족한 개념 정리해놓고 U스테이지 강의 복습하기
* 프로젝트 (=협업)
  * 협업에 대한 증거 : 노션, 깃헙 등 어떤 형태로든 기록
  * 협업 도구 잘 다루기 : 프로젝트 몰입도 상승, 실험 결과 기록 활용도, 작업 신뢰도
  * 팀 내 자신의 포지션, 팀원들의 강점, 협업 도구 사용법
  * 능동적인 다양한 시도

## 🚀 학습 회고

* 공부할게 참 많지만 기본기를 차근차근 탄탄하게 쌓는 것을 최우선으로 하자

