---
layout: post
title:  "[Boostcamp AI Tech] 33ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day33"
categories: "Boostcamp-AI-Tech"
tags: [7ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 33ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/16(ëª©)
  - [x] ë…¼ë¬¸ ìŠ¤í„°ë””(BERT, MT-CNN) 13:00~14:00
  - [x] í”¼ì–´ì„¸ì…˜ ì£¼ì œ : huggingface tutorial ê³µë¶€í•˜ê³  ì§ˆì˜ì‘ë‹µ
  - [x] ì˜¤í”¼ìŠ¤ì•„ì›Œ 9/16 (ëª©) 18:00~19:30 ê³¼ì œ í•´ì„¤ (ë¬¸ì˜ê¸° ë©˜í† ë‹˜)

## ğŸ“– ë…¼ë¬¸ ìŠ¤í„°ë””

### [BERT](https://arxiv.org/abs/1810.04805)

* ì£¼ì œ : Bidirectional Encoder Representation from Transformer
* ëª¨ë¸ ë¹„êµ
  * ELMo : ì–‘ë°©í–¥ì´ì§€ë§Œ shallow í•˜ë‹¤.
  * GPT-1 : LM taskë¡œ ì¸í•´, decoderì˜ masked self-attention ì‚¬ìš© (ë‹¨ë°©í–¥)
  * BERT
    * ë™ê¸° : ì–‘ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ë˜ cheating ì´ ì•„ë‹Œ ë°©ë²• ì—†ì„ê¹Œ?
    * í•´ê²° : Cloze task (ë¹ˆì¹¸ ì±„ìš°ê¸°) ë¥¼ í•™ìŠµí•˜ë„ë¡!
    * ê²°ë¡  : encoderì˜ self-attention ì‚¬ìš© (ì–‘ë°©í–¥) <- PLMì˜ ì´ì  í™œìš© ê·¹ëŒ€í™”!
* Pre-training task
  * **Masked Language Modeling (MLM)**
  * Next Sentence Prediction (NSP) -> í›„ì† ë…¼ë¬¸ë“¤ì—ì„œ ë°”ê¾¸ê¸°ë„ í•œë‹¤.
* êµ¬í˜„ : [huggingface transformers - bert](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py)
  * í•µì‹¬ ëª¨ë¸ : [`BertModel`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L842)
  * positional embedding : [`alsolute`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L219) \| [`relative`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L246)
  * fine tuning : [`BertForSequenceClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L1481) \| [`BertForTokenClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L1676)
* ì°¸ê³  : [How does BERT deal with catastrophic forgetting?](https://datascience.stackexchange.com/questions/49313/how-does-bert-deal-with-catastrophic-forgetting)
  * 3 Epochs + Small Learning Rate

### [MT-CNN](https://arxiv.org/abs/1901.11504)

![image](https://user-images.githubusercontent.com/35680202/133551512-3ab4edd2-5215-4311-bcc1-a4b76c31ee9c.png)

* ì£¼ì œ : BERT + Multi-Task Learning
* Multi-Task Learning : ì—¬ëŸ¬ taskë¥¼ ë™ì‹œì— ìˆ˜í–‰
  * ë°©ë²• : ê°ê° taskë³„ë¡œ ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ë§Œë“¤ê³  ë”í•´ì„œ ëª¨ë¸ì— ë„£ì–´ì„œ í•™ìŠµì‹œí‚¤ë©´ ë!
  * ì°¸ê³  : torch Datasets ê°ì²´ì— ë”í•˜ê¸° ì—°ì‚° ì§€ì›! ([ì½”ë“œ í™•ì¸í•´ë³´ê¸°](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py#L71))
  * ì—¬ê¸°ì„œë„ Maximum Epoch 5 ì •ë„ë¡œë§Œ í•™ìŠµ
* [Pytorchë¡œ êµ¬í˜„í•œ MT-DNN](https://github.com/namisan/mt-dnn)

## ğŸ¤— [Huggingface Tutorial](https://huggingface.co/course/chapter1)

### [1] Transformer models

* Transformers
  * íŠ¹ì§•
    * ëª¨ë“  Transformer ëª¨ë¸ë“¤ì€ *language model* ë¡œ í•™ìŠµë¨
    * *transfer learning* ê³¼ì •ì„ í†µí•´ ì£¼ì–´ì§„ ì‘ì—…ì— ë§ê²Œ *fine-tuning* í•˜ê¸°
    * í¸ê²¬ê³¼ í•œê³„ : ì›ë³¸ ëª¨ë¸ì´ ì„±ì°¨ë³„, ì¸ì¢…ì°¨ë³„ ë˜ëŠ” ë™ì„±ì•  í˜ì˜¤ ì½˜í…ì¸ ë¥¼ ë§¤ìš° ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. fine-tuningì„ í•˜ë”ë¼ë„ ì´ëŸ¬í•œ ë‚´ì¬ì  í¸í–¥ì´ ì‚¬ë¼ì§€ì§€ ì•ŠëŠ”ë‹¤.
  * ì£¼ìš” êµ¬ì„±ìš”ì†Œ
    * Encoder : ëª¨ë¸ì´ ì…ë ¥ìœ¼ë¡œë¶€í„° ì´í•´ë¥¼ ì–»ë„ë¡ ìµœì í™”
    * Decoder : Encoderì˜ í‘œí˜„ì„ ë‹¤ë¥¸ ì…ë ¥ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ ìƒì„±ì— ìµœì í™”
  * **BERT-like** \| *auto-encoding* models \| Encoder-only models
    * ì¢…ë¥˜ : [ALBERT](https://huggingface.co/transformers/model_doc/albert.html) \| [BERT](https://huggingface.co/transformers/model_doc/bert.html) \| [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) \| [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html) \| [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
    * íŠ¹ì§• : ë¬¸ì¥ ë¶„ë¥˜(Sentence Classification) ë° ëª…ëª…ëœ ê°œì²´ ì¸ì‹(Named Entity Recognition), ì¶”ì¶œ ì§ˆë¬¸ ë‹µë³€(Extractive Question Answering)ê³¼ ê°™ì´ ì…ë ¥ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ì‘ì—…ì— ì í•© (bi-directional attention)
  * **GPT-like** \| *auto-regressive* models \| Decoder-only models
    * ì¢…ë¥˜ : [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html) \| [GPT](https://huggingface.co/transformers/model_doc/gpt.html) \| [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html) \| [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)
    * íŠ¹ì§• : í…ìŠ¤íŠ¸ ìƒì„±(Text Generation)ê³¼ ê°™ì€ ìƒì„± ì‘ì—…ì— ì í•©
  * **BART/T5-like** \| *sequence-to-sequence* models \| Encoder-Decoder models
    * ì¢…ë¥˜ : [BART](https://huggingface.co/transformers/model_doc/bart.html) \| [mBART](https://huggingface.co/transformers/model_doc/mbart.html) \| [Marian](https://huggingface.co/transformers/model_doc/marian.html) \| [T5](https://huggingface.co/transformers/model_doc/t5.html)
    * íŠ¹ì§• : ìš”ì•½(Summarization), ë²ˆì—­(Translation) ë˜ëŠ” ìƒì„±ì  ì§ˆë¬¸ ë‹µë³€(Generative Question Answering)ê³¼ ê°™ì´ ì£¼ì–´ì§„ ì…ë ¥ì— ë”°ë¼ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ì‘ì—…ì— ê°€ì¥ ì í•©

* High-Level API
  * [pipeline](https://huggingface.co/transformers/main_classes/pipelines.html) : ëª¨ë¸ì„ ì „ì²˜ë¦¬ë¶€í„° í›„ì²˜ë¦¬ê¹Œì§€ ì—°ê²°í•˜ì—¬ ê°„í¸í•˜ê²Œ ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŒ

    ```python
    from transformers import pipeline
    classifier = pipeline("sentiment-analysis")
    classifier("I've been waiting for a HuggingFace course my whole life.")
    ```

  * [Inference API](https://huggingface.co/inference-api) : ë¸Œë¼ìš°ì €ë¥¼ í†µí•´ ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* TransformerëŠ” teacher forcing ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ë‚˜? yes
* ê·¸ëŸ¬ë©´ Inferenceí•  ë•ŒëŠ” seq2seq ì²˜ëŸ¼ í•œ ë‹¨ì–´ì”© ì˜ˆì¸¡í•˜ëŠ” ê±´ê°€ìš”? yes [ì°¸ê³  ë§í¬](https://wikidocs.net/31379)
* ì„ íƒê³¼ì œ3 ì°¸ê³  ë§í¬ : [ë°”ì´íŠ¸ í˜ì–´ ì¸ì½”ë”©(Byte Pair Encoding, BPE)](https://wikidocs.net/22592)

## ğŸŒ¼ ë©˜í† ë§

### ì§ˆë¬¸ Time

* Q. "attention is not explanation" vs "attention is not not explanation" ì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?
  * ë…¼ë¬¸ì„ ë³´ì‹œì§„ ì•Šì•˜ì§€ë§Œ, attentionë§Œìœ¼ë¡œëŠ” ì„¤ëª… ë¶ˆê°€ëŠ¥í•œê²Œ ë§ëŠ” ê²ƒ ê°™ë‹¤ê³  ìƒê°
  * ì‚¬ëŒì˜ ë„ë©”ì¸ ì§€ì‹ íŒ¨í„´ê³¼, ê¸°ê³„ê°€ í•™ìŠµí•œ íŒ¨í„´ì´ ë‹¤ë¥¼ ìˆ˜ë„ ìˆë‹¤.
* Q. gpt-2 ì˜ zero shot learning ì´ ì–´ë–»ê²Œ í•™ìŠµëœë‹¤ëŠ”ê±´ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤.
  * zero shot learning : ì˜ˆì‹œë¥¼ ì•ˆ ì¤€ë‹¤. taskì— ëŒ€í•œ ì„¤ëª…ë§Œ ë„£ì–´ì¤€ë‹¤.
  * one shot learning : ì˜ˆì‹œë¥¼ í•œ ê°œ ì£¼ëŠ” ê²ƒ
  * few shot learning : ì˜ˆì‹œë¥¼ ëª‡ ê°œ ì£¼ëŠ” ê²ƒ
  * ì˜ˆì‹œë¥¼ gradient íƒœìš°ëŠ”ê²Œ ì•„ë‹ˆë¼ input ì•ì— ì¤€ë‹¤ëŠ” ëœ»
* Q. positional encoding vs positional embedding
  * positional embeddingì´ learnableí•˜ë©´, dataê°€ ë§ê³  í•™ìŠµì´ ì˜ë˜ì—ˆë‹¤ë©´, taskë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ representationì´ ë” ëª…í™•í•˜ê²Œ ë  ê²ƒ ê°™ë‹¤.

### ì¡°ì–¸ Time

* fairseq ë„ ë§ì´ ì“°ì´ê¸° ë•Œë¬¸ì— huggingface ì™€ ê°™ì´ ì»¤ìŠ¤í…€í•´ì„œ ì“°ëŠ”ê±¸ ì—°ìŠµí•˜ë©´ ì¢‹ë‹¤.
* ë°‘ë‹¨ë¶€í„° ì§œê¸°ë³´ë‹¤ëŠ” ìœ„ ë‘ê°œë¥¼ ì‘ìš©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íŠ¸ë Œë“œ!
* ê¿€íŒ : ë‚®ì„  ëª¨ë¸ ì§¤ ë•Œ ëŒ€ì¶© ë”ë¯¸ ë°ì´í„° (input) ë§Œë“¤ê³  ì›í•˜ëŠ” tensor shape ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”ì§€ ë¨¼ì € í™•ì¸í•˜ê¸°

## ğŸ’ ì˜¤í”¼ìŠ¤ ì•„ì›Œ

* í•„ìˆ˜ê³¼ì œ 4
  * list comprehension : ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬í•  ë•Œ ë§ì´ ì‚¬ìš©! (ì œì¼ ë¹ ë¥´ë‹¤)
  * bucketing : íŒ¨ë”©ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë„£ëŠ” ë²•

## ğŸš€ í•™ìŠµ íšŒê³ 

* í—ˆê¹…í˜ì´ìŠ¤ íŠœí† ë¦¬ì–¼ ë„ˆë¬´ ì¢‹ë‹¤! ì™„ì „ ì…ë¬¸ìì—ê²Œë„ ì¢‹ì„ ê²ƒ ê°™ë‹¤.
* ë‚´ìš©ì •ë¦¬ê°€ í˜ë“¤ì—ˆì§€ë§Œ ê·¸ë§Œí¼ ë¿Œë“¯í•˜ë‹¤.

