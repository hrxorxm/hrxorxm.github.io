---
layout: post
title:  "[Boostcamp AI Tech] 33일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day33"
categories: "Boostcamp-AI-Tech"
tags: [7주차]
use_math: true
---

# 부스트캠프 33일차

## 📝 오늘 일정 정리

* 9/16(목)
  - [x] 논문 스터디(BERT, MT-CNN) 13:00~14:00
  - [x] 피어세션 주제 : huggingface tutorial 공부하고 질의응답
  - [x] 오피스아워 9/16 (목) 18:00~19:30 과제 해설 (문영기 멘토님)

## 📖 논문 스터디

### [BERT](https://arxiv.org/abs/1810.04805)

* 주제 : Bidirectional Encoder Representation from Transformer
* 모델 비교
  * ELMo : 양방향이지만 shallow 하다.
  * GPT-1 : LM task로 인해, decoder의 masked self-attention 사용 (단방향)
  * BERT
    * 동기 : 양방향으로 학습하되 cheating 이 아닌 방법 없을까?
    * 해결 : Cloze task (빈칸 채우기) 를 학습하도록!
    * 결론 : encoder의 self-attention 사용 (양방향) <- PLM의 이점 활용 극대화!
* Pre-training task
  * **Masked Language Modeling (MLM)**
  * Next Sentence Prediction (NSP) -> 후속 논문들에서 바꾸기도 한다.
* 구현 : [huggingface transformers - bert](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py)
  * 핵심 모델 : [`BertModel`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L842)
  * positional embedding : [`alsolute`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L219) \| [`relative`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L246)
  * fine tuning : [`BertForSequenceClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L1481) \| [`BertForTokenClassification`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L1676)
* 참고 : [How does BERT deal with catastrophic forgetting?](https://datascience.stackexchange.com/questions/49313/how-does-bert-deal-with-catastrophic-forgetting)
  * 3 Epochs + Small Learning Rate

### [MT-CNN](https://arxiv.org/abs/1901.11504)

![image](https://user-images.githubusercontent.com/35680202/133551512-3ab4edd2-5215-4311-bcc1-a4b76c31ee9c.png)

* 주제 : BERT + Multi-Task Learning
* Multi-Task Learning : 여러 task를 동시에 수행
  * 방법 : 각각 task별로 데이터셋 객체를 만들고 더해서 모델에 넣어서 학습시키면 끝!
  * 참고 : torch Datasets 객체에 더하기 연산 지원! ([코드 확인해보기](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py#L71))
  * 여기서도 Maximum Epoch 5 정도로만 학습
* [Pytorch로 구현한 MT-DNN](https://github.com/namisan/mt-dnn)

## 🤗 [Huggingface Tutorial](https://huggingface.co/course/chapter1)

### [1] Transformer models

* Transformers
  * 특징
    * 모든 Transformer 모델들은 *language model* 로 학습됨
    * *transfer learning* 과정을 통해 주어진 작업에 맞게 *fine-tuning* 하기
    * 편견과 한계 : 원본 모델이 성차별, 인종차별 또는 동성애 혐오 콘텐츠를 매우 쉽게 생성할 수 있다. fine-tuning을 하더라도 이러한 내재적 편향이 사라지지 않는다.
  * 주요 구성요소
    * Encoder : 모델이 입력으로부터 이해를 얻도록 최적화
    * Decoder : Encoder의 표현을 다른 입력과 함께 사용하여 출력 생성에 최적화
  * **BERT-like** \| *auto-encoding* models \| Encoder-only models
    * 종류 : [ALBERT](https://huggingface.co/transformers/model_doc/albert.html) \| [BERT](https://huggingface.co/transformers/model_doc/bert.html) \| [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) \| [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html) \| [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
    * 특징 : 문장 분류(Sentence Classification) 및 명명된 개체 인식(Named Entity Recognition), 추출 질문 답변(Extractive Question Answering)과 같이 입력에 대한 이해가 필요한 작업에 적합 (bi-directional attention)
  * **GPT-like** \| *auto-regressive* models \| Decoder-only models
    * 종류 : [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html) \| [GPT](https://huggingface.co/transformers/model_doc/gpt.html) \| [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html) \| [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)
    * 특징 : 텍스트 생성(Text Generation)과 같은 생성 작업에 적합
  * **BART/T5-like** \| *sequence-to-sequence* models \| Encoder-Decoder models
    * 종류 : [BART](https://huggingface.co/transformers/model_doc/bart.html) \| [mBART](https://huggingface.co/transformers/model_doc/mbart.html) \| [Marian](https://huggingface.co/transformers/model_doc/marian.html) \| [T5](https://huggingface.co/transformers/model_doc/t5.html)
    * 특징 : 요약(Summarization), 번역(Translation) 또는 생성적 질문 답변(Generative Question Answering)과 같이 주어진 입력에 따라 새로운 문장을 생성하는 작업에 가장 적합

* High-Level API
  * [pipeline](https://huggingface.co/transformers/main_classes/pipelines.html) : 모델을 전처리부터 후처리까지 연결하여 간편하게 답을 얻을 수 있음

    ```python
    from transformers import pipeline
    classifier = pipeline("sentiment-analysis")
    classifier("I've been waiting for a HuggingFace course my whole life.")
    ```

  * [Inference API](https://huggingface.co/inference-api) : 브라우저를 통해 직접 테스트 가능

## 🌱 피어 세션 정리

* Transformer는 teacher forcing 방식으로 학습하나? yes
* 그러면 Inference할 때는 seq2seq 처럼 한 단어씩 예측하는 건가요? yes [참고 링크](https://wikidocs.net/31379)
* 선택과제3 참고 링크 : [바이트 페어 인코딩(Byte Pair Encoding, BPE)](https://wikidocs.net/22592)

## 🌼 멘토링

### 질문 Time

* Q. "attention is not explanation" vs "attention is not not explanation" 에 대해서 어떻게 생각하시나요?
  * 논문을 보시진 않았지만, attention만으로는 설명 불가능한게 맞는 것 같다고 생각
  * 사람의 도메인 지식 패턴과, 기계가 학습한 패턴이 다를 수도 있다.
* Q. gpt-2 의 zero shot learning 이 어떻게 학습된다는건지 궁금합니다.
  * zero shot learning : 예시를 안 준다. task에 대한 설명만 넣어준다.
  * one shot learning : 예시를 한 개 주는 것
  * few shot learning : 예시를 몇 개 주는 것
  * 예시를 gradient 태우는게 아니라 input 앞에 준다는 뜻
* Q. positional encoding vs positional embedding
  * positional embedding이 learnable하면, data가 많고 학습이 잘되었다면, task를 해결하기 위한 representation이 더 명확하게 될 것 같다.

### 조언 Time

* fairseq 도 많이 쓰이기 때문에 huggingface 와 같이 커스텀해서 쓰는걸 연습하면 좋다.
* 밑단부터 짜기보다는 위 두개를 응용해서 사용하는 것이 트렌드!
* 꿀팁 : 낮선 모델 짤 때 대충 더미 데이터 (input) 만들고 원하는 tensor shape 으로 나오는지 먼저 확인하기

## 💎 오피스 아워

* 필수과제 4
  * list comprehension : 대용량 데이터 처리할 때 많이 사용! (제일 빠르다)
  * bucketing : 패딩을 효율적으로 넣는 법

## 🚀 학습 회고

* 허깅페이스 튜토리얼 너무 좋다! 완전 입문자에게도 좋을 것 같다.
* 내용정리가 힘들었지만 그만큼 뿌듯하다.

