---
layout: post
title:  "[Boostcamp AI Tech] 27ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day27"
categories: "Boostcamp-AI-Tech"
tags: [6ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 27ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/8(ìˆ˜)
  - [x] NLP ì´ë¡  ê°•ì˜
    - [x] (05ê°•) Sequence to sequence with attention
    - [x] (06ê°•) Beam search and BLEU score
    - [x] [ì‹¤ìŠµ] Seq2Seq, Seq2Seq with attention
    - [x] [í•„ìˆ˜ ê³¼ì œ 4] Preprocessing for NMT Model

## ğŸ“š ê°•ì˜ ë‚´ìš© ì •ë¦¬

### [5ê°•] Sequence to Sequence with Attention

* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
* [CS224n(2019) Lecture8 NMT](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)

#### Seq2Seq Model

* Sequence-to-Sequence : RNNì˜ êµ¬ì¡° ì¤‘ì—ì„œ many to manyì˜ í˜•íƒœ (ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ëª¨ë‘ ì½ì€ í›„ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±/ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸)
* ![image](https://user-images.githubusercontent.com/35680202/132442735-e004a799-a2bf-423a-a571-4f80933a873c.png)
  * Encoder / Decoder : ì„œë¡œ share í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ê°–ëŠ” RNN ëª¨ë“ˆ (LSTM ëª¨ë“ˆ)
  * `<start>` `<end>` í† í°
* ë¬¸ì œ : Encoderì˜ RNN ëª¨ë“ˆì˜ ë§ˆì§€ë§‰ hidden stateì—ë§Œ ì˜ì¡´í•œë‹¤ë©´, ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆ ë•Œ, dimensionì´ í•œì •ë˜ì–´ ìˆì–´ì„œ ì •ë³´ê°€ ë‹¤ ë‹´ê¸°ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.

#### Seq2Seq Model with Attention

* í•´ê²° : ê° time stepì—ì„œ ë‚˜ì˜¨ hidden stateë“¤ì„ ì´ìš©í•˜ì (=>Attention)
* ë°©ë²• : Decoderì—ì„œ ê° ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ í•„ìš”ë¡œí•˜ëŠ” Encoderì˜ hidden state vectorë¥¼ ì ì ˆíˆ ì„ íƒí•´ì„œ ì˜ˆì¸¡ì— í™œìš©í•œë‹¤.
* ![image](https://user-images.githubusercontent.com/35680202/132442525-cf5b6bc5-1d16-4325-b94c-c7f47f54059a.png)
  * Attention scores : Decoderì˜ hidden state vectorì™€ Encoderì˜ hidden state vectorë“¤ì„ ë‚´ì í•œ ê²ƒ. (ë‚´ì  ë§ê³  ë‹¤ë¥¸ ë°©ë²•ì´ ìˆì„ ìˆ˜ ìˆìŒ => Attention variants)
  * Attention distridution (=Attention vector) : Attention scoresë¥¼ í™•ë¥ ê°’(í•©ì´ 1ì¸ í˜•íƒœ)ìœ¼ë¡œ ë³€í™˜ì‹œí‚¨ ê²ƒ. Attention distributionëŠ” Encoder hidden state vectorì— ë¶€ì—¬ë˜ëŠ” ê°€ì¤‘ì¹˜ë¡œì„œ ì‚¬ìš©ëœë‹¤.
  * Attention output (=Context vector) : Encoder hidden state vectorë“¤ì˜ ê°€ì¤‘ í‰ê· ì„ êµ¬í•œ ë²¡í„°.
  * Decoderì˜ hidden state vectorì™€ context vectorê°€ concat ë˜ì–´ì„œ output layerì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.
  * Teacher forcing : í•™ìŠµí•  ë•Œ Decoderì—ì„œ ê° time stepì—ì„œ ì´ ì „ time stepì˜ ì˜ˆì¸¡ê°’ì´ ì•„ë‹ˆë¼, ì˜¬ë°”ë¥¸ tokenì„ ì…ë ¥ìœ¼ë¡œ ì£¼ëŠ” ê²ƒ (í•™ìŠµì´ ìš©ì´í•˜ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ë•Œ ê´´ë¦¬ê°€ ìˆì„ ìˆ˜ ìˆë‹¤.)
* Attention variants (compute $e \in \R^{N}$ from $h_1, ..., h_N \in \R^{d_1}$ and $s \in \R^{d_2}$)
  * (dot) Basic dot-product attention : $e_i = s^T h_i$
    * $d_1 = d_2$ë¼ê³  ê°€ì •
  * (general) Multiplicative attention : $e_i = s^T W h_i$
    * $W \in \R^{d_2 \times d_1}$ : weight matrix
  * (concat) Addictive attention (=Bahdanau attention) : $e_i = v^T \tanh (W_1 h_i + W_2 s) = v^T \tanh (W [s; h_i])$
* Attention is Great!
  * NMT ì„±ëŠ¥ì„ ë†’ì—¬ì£¼ì—ˆê³ , í•´ì„ ê°€ëŠ¥ì„±(interpretability)ì„ ì œê³µí•œë‹¤.
    * [How to Visualize Your Recurrent Neural Network with Attention in Keras](https://medium.com/datalogue/attention-in-keras-1892773a4f22)
    * [Visualizing Attention in PyTorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#visualizing-attention)
  * bottleneck problem, vanishing gradient problem í•´ê²°

### [6ê°•] Beam search and BLEU score

- [Deep learning.ai-BeamSearch](https://www.youtube.com/watch?v=RLWuzLLSIgw&feature=youtu.be)
- [Deep learning.ai-RefiningBeamSearch](https://www.youtube.com/watch?v=gb__z7LlN_4&feature=youtu.be)
- [OpenNMT-beam search](https://opennmt.net/OpenNMT/translation/beam_search/)

#### Beam search

* Greedy decoding
  * ë°©ë²• : í˜„ì¬ time stepì—ì„œ ê°€ì¥ ì¢‹ì•„ë³´ì´ëŠ” ë‹¨ì–´ë¥¼ ê·¸ë•Œê·¸ë•Œ ì„ íƒí•˜ëŠ” ë°©ì‹
  * ë¬¸ì œ : ì¤‘ê°„ì— ì˜ëª»ì„ íƒí–ˆë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•„ë„ ë’¤ë¡œ ëŒì•„ê°ˆ ìˆ˜ ì—†ìŒ
* Exhaustive search
  * ë°©ë²• : ëª¨ë“  ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ yë¥¼ ê³„ì‚°í•œë‹¤.
    * $P(y \| x) = P(y_1 \| x) P(y_2 \| y1, x) P(y_3 \| y_2, y_1, x) ... P(y_T \| y_1, ..., y_{T-1}, x) = \Pi_{1}^{T} P(y_t \| y_1, ..., y_{t-1}, x)$
    * ìœ„ ì‹ì„ ìµœëŒ€í™”í•˜ëŠ” yë¥¼ ì°¾ëŠ” ê²ƒ
  * ë¬¸ì œ : í˜„ì‹¤ì ìœ¼ë¡œ ê³„ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ì•„ ë¶ˆê°€ëŠ¥
* Beam search
  * ë°©ë²• : ë‘ ì•„ì´ë””ì–´ì˜ ì°¨ì„ ì±…, top k ê°œì˜ ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•˜ëŠ” ë°©ì‹
    * k : beam size (ì¼ë°˜ì ìœ¼ë¡œ 5~10)
    * $score(y_1, ..., y_t) = \log P_{LM} (y_1, ..., y_t \| x) = \sum_{i=1}^{t} \log P_{LM} (y_i \| y_1, ..., y_{i-1}, x)$
  * ![image](https://user-images.githubusercontent.com/35680202/132449278-05f6176e-9e5d-46d8-bce9-ff481899e399.png)
  * Stopping criterion
    * ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë“¤ ì¤‘ì—ì„œ `<end>` í† í°ì„ ë‹¤ë¥¸ ì‹œì ì— ë§Œë“¤ì–´ë‚¼ ìˆ˜ë„ ìˆë‹¤. `<end>` í† í°ì„ ë§Œë“¤ë©´ ê·¸ ê²½ìš°ëŠ” searchë¥¼ ì¤‘ë‹¨(ì™„ë£Œ)í•˜ê³  ì„ì‹œ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ë‘”ë‹¤.
    * timestep Tì— ì´ë¥´ë €ì„ ë•Œ ë©ˆì¶”ê±°ë‚˜, nê°œì˜ hypothesesê°€ ì™„ë£Œë˜ë©´ ë©ˆì¶˜ë‹¤.
  * Finishing up
    * ì™„ë£Œëœ hypothesesë“¤ì˜ ë¦¬ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œ, ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë‚´ëŠ” í•˜ë‚˜ë¥¼ ë½‘ëŠ”ë‹¤.
    * ìŒìˆ˜ê°€ ë”í•´ì§€ê¸° ë•Œë¬¸ì— hypothesesê°€ ê¸¸ìˆ˜ë¡ score(joint probability)ì˜ ê°’ì´ ì‘ì•„ì§€ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ length ë¥¼ ì´ìš©í•´ì„œ normalize í•œë‹¤.
    * $score(y_1, ..., y_t) = \frac{1}{t} \sum_{i=1}^{t} \log P_{LM} (y_i \| y_1, ..., y_{i-1}, x)$

#### [BLEU score](https://aclanthology.org/P02-1040.pdf)

* ìì—°ì–´ ìƒì„± ëª¨ë¸ì˜ í’ˆì§ˆ, ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•
* Precision and Recall
  * ì •ë°€ë„(Precision) = number_of_correct_words / length_of_prediction
  * ì¬í˜„ìœ¨(Recall) = number_of_correct_words / length_of_reference
  * F-measure = 2 x precision x recall / (precision + recall)
    * ì‚°ìˆ í‰ê·  = $\frac{a + b}{2}$
    * ê¸°í•˜í‰ê·  = $(a \times b)^{\frac{1}{2}}$
    * ì¡°í™”í‰ê·  = $\frac{1}{\frac{\frac{1}{a} + \frac{1}{b}}{2}}$
    * ì‚°ìˆ í‰ê·  >= ê¸°í•˜í‰ê·  >= **ì¡°í™”í‰ê· **
  * ë¬¸ì œ : ë¬¸ë²•ì ìœ¼ë¡œ ë§ì´ ë˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì„ ì•Œ ìˆ˜ ì—†ìŒ
* BLEU(BiLingual Evaluation Understudy)
  * N-gram(Nê°œì˜ ì—°ì†ëœ ë‹¨ì–´)ê°€ ê²¹ì¹˜ëŠ”ì§€ í‰ê°€ë„ ì¶”ê°€ (n = 1~4)
  * precisionë§Œ ê³ ë ¤í•˜ê³ , recallì€ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.
  * ì¡°í™”í‰ê· ì´ ì•„ë‹Œ **ê¸°í•˜í‰ê· **ì„ ì‚¬ìš©í•œë‹¤.
  * $BLEU = \min (1, \frac{length\_of\_prediction}{length\_of\_reference}) (\Pi_{i=1}^{4} {precision}_i)^{\frac{1}{4}}$

## ğŸ” ê³¼ì œ ìˆ˜í–‰ ê³¼ì •

* í•„ìˆ˜ ê³¼ì œ 4 : Preprocessing for NMT Model
  * ì˜ì–´-í•œê¸€ ë²ˆì—­ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì˜ì–´-í•œê¸€ ë²ˆì—­ ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ ë°©ë²• í•™ìŠµ
  * `process` : ì£¼ì–´ì§„ ë¬¸ì¥ìŒ(Source, Target)ì„ ë‹¨ì–´ index ë‹¨ìœ„ë¡œ ë°”ê¾¸ì–´ì£¼ëŠ” í•¨ìˆ˜
    ```python
    # [ì„ íƒ] List Comprehensionì„ í™œìš©í•´ì„œ ì§§ì€ ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”. (~2 lines)
    src_sentence = [src_word2idx[word] if word in src_word2idx else UNK for word in raw_src_sentence][:max_len]
    tgt_sentence = [SOS] + [tgt_word2idx[word] if word in tgt_word2idx else UNK for word in raw_tgt_sentence][:max_len-2] + [EOS]
    ```
  * `bucketed_batch_indices`
    * Bucketing : ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê¸¸ì´ì— ë”°ë¼ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ paddingì„ ì ìš©í•˜ëŠ” ê¸°ë²•, ëª¨ë¸ì˜ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê¸° ìœ„í•´ ê³ ì•ˆ
    * bucketingì„ ì ìš©í•˜ì§€ ì•Šì€ ê²½ìš°, batchë³„ pad tokenì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚˜ í•™ìŠµí•˜ëŠ” ë°ì— ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦°ë‹¤.
    * ì£¼ì–´ì§„ ë¬¸ì¥ë“¤ì„ ë¬¸ì¥ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ bucketed_batch_indices í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ê¸°
  * `collate_fn`
    * Collate function : ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì„ ì›í•˜ëŠ” í˜•íƒœì˜ batchë¡œ ê°€ê³µí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜
    * Batch ë‹¨ìœ„ë³„ë¡œ max sequence lengthì— ë§ê²Œ pad tokenì„ ì¶”ê°€í•˜ê³  ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê¸°

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* Further Question
  * BLEU scoreê°€ ë²ˆì—­ ë¬¸ì¥ í‰ê°€ì— ìˆì–´ì„œ ê°–ëŠ” ë‹¨ì ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”?
    * ì°¸ê³ : [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://arxiv.org/abs/2006.06264?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+arxiv%2FQSXk+%28ExcitingAds%21+cs+updates+on+arXiv.org%29)
* í•„ìˆ˜ê³¼ì œ 4 : try, exceptì„ í™œìš©í•´ì„œ ì¡°ê¸ˆ ë” ë¹ ë¥´ê²Œ ë™ì‘í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆë‚˜ìš”?
  * ifë¬¸ì„ ì“°ë©´ dictionaryì˜ keyê°’ì„ ìˆœíšŒí•´ë´ì•¼í•˜ë‹ˆê¹Œ, ifë¬¸ ëŒ€ì‹  try except ì„ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¢€ ë” ë¹ ë¥´ë‹¤.
* êµ¬í˜„ í›„ í…ŒìŠ¤íŠ¸í•´ë³´ê¸° ì¢‹ì€ ë°ì´í„°ì…‹
  * [SQuAD(Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/)
  * [KorQuAD(The Korean Question Answering Dataset)](https://korquad.github.io/)
* DKT(Dynamic Knowledge Tracing) / DST(Dialog State Tracking)

## ğŸš€ í•™ìŠµ íšŒê³ 

* ì´ë²ˆ ì£¼ ì´ë¡ ì€ ì´ì œ ëë‚¬ìœ¼ë‹ˆê¹Œ ë‚¨ì€ ì‹œê°„ì—ëŠ” êµ¬í˜„ ìœ„ì£¼ë¡œ ê³µë¶€í•´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.

