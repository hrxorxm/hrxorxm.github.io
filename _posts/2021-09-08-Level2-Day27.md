---
layout: post
title:  "[Boostcamp AI Tech] 27일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day27"
categories: "Boostcamp-AI-Tech"
tags: [6주차]
use_math: true
---

# 부스트캠프 27일차

## 📝 오늘 일정 정리

* 9/8(수)
  - [x] NLP 이론 강의
    - [x] (05강) Sequence to sequence with attention
    - [x] (06강) Beam search and BLEU score
    - [x] [실습] Seq2Seq, Seq2Seq with attention
    - [x] [필수 과제 4] Preprocessing for NMT Model

## 📚 강의 내용 정리

### [5강] Sequence to Sequence with Attention

* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
* [CS224n(2019) Lecture8 NMT](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)

#### Seq2Seq Model

* Sequence-to-Sequence : RNN의 구조 중에서 many to many의 형태 (입력 시퀀스를 모두 읽은 후 출력 시퀀스를 생성/예측하는 모델)
* ![image](https://user-images.githubusercontent.com/35680202/132442735-e004a799-a2bf-423a-a571-4f80933a873c.png)
  * Encoder / Decoder : 서로 share 하지 않는 파라미터를 갖는 RNN 모듈 (LSTM 모듈)
  * `<start>` `<end>` 토큰
* 문제 : Encoder의 RNN 모듈의 마지막 hidden state에만 의존한다면, 입력 문장의 길이가 길어질 때, dimension이 한정되어 있어서 정보가 다 담기지 못할 수 있다.

#### Seq2Seq Model with Attention

* 해결 : 각 time step에서 나온 hidden state들을 이용하자 (=>Attention)
* 방법 : Decoder에서 각 단어를 생성할 때 필요로하는 Encoder의 hidden state vector를 적절히 선택해서 예측에 활용한다.
* ![image](https://user-images.githubusercontent.com/35680202/132442525-cf5b6bc5-1d16-4325-b94c-c7f47f54059a.png)
  * Attention scores : Decoder의 hidden state vector와 Encoder의 hidden state vector들을 내적한 것. (내적 말고 다른 방법이 있을 수 있음 => Attention variants)
  * Attention distridution (=Attention vector) : Attention scores를 확률값(합이 1인 형태)으로 변환시킨 것. Attention distribution는 Encoder hidden state vector에 부여되는 가중치로서 사용된다.
  * Attention output (=Context vector) : Encoder hidden state vector들의 가중 평균을 구한 벡터.
  * Decoder의 hidden state vector와 context vector가 concat 되어서 output layer의 입력으로 들어가서 다음 단어를 예측한다.
  * Teacher forcing : 학습할 때 Decoder에서 각 time step에서 이 전 time step의 예측값이 아니라, 올바른 token을 입력으로 주는 것 (학습이 용이하지만, 테스트 때 괴리가 있을 수 있다.)
* Attention variants (compute $e \in \R^{N}$ from $h_1, ..., h_N \in \R^{d_1}$ and $s \in \R^{d_2}$)
  * (dot) Basic dot-product attention : $e_i = s^T h_i$
    * $d_1 = d_2$라고 가정
  * (general) Multiplicative attention : $e_i = s^T W h_i$
    * $W \in \R^{d_2 \times d_1}$ : weight matrix
  * (concat) Addictive attention (=Bahdanau attention) : $e_i = v^T \tanh (W_1 h_i + W_2 s) = v^T \tanh (W [s; h_i])$
* Attention is Great!
  * NMT 성능을 높여주었고, 해석 가능성(interpretability)을 제공한다.
    * [How to Visualize Your Recurrent Neural Network with Attention in Keras](https://medium.com/datalogue/attention-in-keras-1892773a4f22)
    * [Visualizing Attention in PyTorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#visualizing-attention)
  * bottleneck problem, vanishing gradient problem 해결

### [6강] Beam search and BLEU score

- [Deep learning.ai-BeamSearch](https://www.youtube.com/watch?v=RLWuzLLSIgw&feature=youtu.be)
- [Deep learning.ai-RefiningBeamSearch](https://www.youtube.com/watch?v=gb__z7LlN_4&feature=youtu.be)
- [OpenNMT-beam search](https://opennmt.net/OpenNMT/translation/beam_search/)

#### Beam search

* Greedy decoding
  * 방법 : 현재 time step에서 가장 좋아보이는 단어를 그때그때 선택하는 방식
  * 문제 : 중간에 잘못선택했다는 것을 깨달아도 뒤로 돌아갈 수 없음
* Exhaustive search
  * 방법 : 모든 가능한 시퀀스 y를 계산한다.
    * $P(y \| x) = P(y_1 \| x) P(y_2 \| y1, x) P(y_3 \| y_2, y_1, x) ... P(y_T \| y_1, ..., y_{T-1}, x) = \Pi_{1}^{T} P(y_t \| y_1, ..., y_{t-1}, x)$
    * 위 식을 최대화하는 y를 찾는 것
  * 문제 : 현실적으로 계산량이 너무 많아 불가능
* Beam search
  * 방법 : 두 아이디어의 차선책, top k 개의 가능한 경우의 수를 고려하는 방식
    * k : beam size (일반적으로 5~10)
    * $score(y_1, ..., y_t) = \log P_{LM} (y_1, ..., y_t \| x) = \sum_{i=1}^{t} \log P_{LM} (y_i \| y_1, ..., y_{i-1}, x)$
  * ![image](https://user-images.githubusercontent.com/35680202/132449278-05f6176e-9e5d-46d8-bce9-ff481899e399.png)
  * Stopping criterion
    * 가능한 경우의 수들 중에서 `<end>` 토큰을 다른 시점에 만들어낼 수도 있다. `<end>` 토큰을 만들면 그 경우는 search를 중단(완료)하고 임시 메모리에 저장해둔다.
    * timestep T에 이르렀을 때 멈추거나, n개의 hypotheses가 완료되면 멈춘다.
  * Finishing up
    * 완료된 hypotheses들의 리스트가 있을 때, 가장 높은 점수를 내는 하나를 뽑는다.
    * 음수가 더해지기 때문에 hypotheses가 길수록 score(joint probability)의 값이 작아지고, 이를 해결하기 위해 length 를 이용해서 normalize 한다.
    * $score(y_1, ..., y_t) = \frac{1}{t} \sum_{i=1}^{t} \log P_{LM} (y_i \| y_1, ..., y_{i-1}, x)$

#### [BLEU score](https://aclanthology.org/P02-1040.pdf)

* 자연어 생성 모델의 품질, 결과의 정확도를 측정하는 방법
* Precision and Recall
  * 정밀도(Precision) = number_of_correct_words / length_of_prediction
  * 재현율(Recall) = number_of_correct_words / length_of_reference
  * F-measure = 2 x precision x recall / (precision + recall)
    * 산술평균 = $\frac{a + b}{2}$
    * 기하평균 = $(a \times b)^{\frac{1}{2}}$
    * 조화평균 = $\frac{1}{\frac{\frac{1}{a} + \frac{1}{b}}{2}}$
    * 산술평균 >= 기하평균 >= **조화평균**
  * 문제 : 문법적으로 말이 되지 않는 문장을 알 수 없음
* BLEU(BiLingual Evaluation Understudy)
  * N-gram(N개의 연속된 단어)가 겹치는지 평가도 추가 (n = 1~4)
  * precision만 고려하고, recall은 고려하지 않는다.
  * 조화평균이 아닌 **기하평균**을 사용한다.
  * $BLEU = \min (1, \frac{length\_of\_prediction}{length\_of\_reference}) (\Pi_{i=1}^{4} {precision}_i)^{\frac{1}{4}}$

## 🔎 과제 수행 과정

* 필수 과제 4 : Preprocessing for NMT Model
  * 영어-한글 번역 모델을 학습하기 위해 영어-한글 번역 데이터셋의 전처리 방법 학습
  * `process` : 주어진 문장쌍(Source, Target)을 단어 index 단위로 바꾸어주는 함수
    ```python
    # [선택] List Comprehension을 활용해서 짧은 코드를 작성해보세요. (~2 lines)
    src_sentence = [src_word2idx[word] if word in src_word2idx else UNK for word in raw_src_sentence][:max_len]
    tgt_sentence = [SOS] + [tgt_word2idx[word] if word in tgt_word2idx else UNK for word in raw_tgt_sentence][:max_len-2] + [EOS]
    ```
  * `bucketed_batch_indices`
    * Bucketing : 주어진 문장의 길이에 따라 데이터를 그룹화하여 padding을 적용하는 기법, 모델의 학습 시간을 단축하기 위해 고안
    * bucketing을 적용하지 않은 경우, batch별 pad token의 개수가 늘어나 학습하는 데에 오랜 시간이 걸린다.
    * 주어진 문장들을 문장 길이를 기준으로 나누어 bucketed_batch_indices 함수를 완성하기
  * `collate_fn`
    * Collate function : 주어진 데이터셋을 원하는 형태의 batch로 가공하기 위해 사용되는 함수
    * Batch 단위별로 max sequence length에 맞게 pad token을 추가하고 내림차순으로 정렬하기

## 🌱 피어 세션 정리

* Further Question
  * BLEU score가 번역 문장 평가에 있어서 갖는 단점은 무엇이 있을까요?
    * 참고: [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://arxiv.org/abs/2006.06264?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+arxiv%2FQSXk+%28ExcitingAds%21+cs+updates+on+arXiv.org%29)
* 필수과제 4 : try, except을 활용해서 조금 더 빠르게 동작하는 코드를 작성할 수 있나요?
  * if문을 쓰면 dictionary의 key값을 순회해봐야하니까, if문 대신 try except 을 사용하는게 좀 더 빠르다.
* 구현 후 테스트해보기 좋은 데이터셋
  * [SQuAD(Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/)
  * [KorQuAD(The Korean Question Answering Dataset)](https://korquad.github.io/)
* DKT(Dynamic Knowledge Tracing) / DST(Dialog State Tracking)

## 🚀 학습 회고

* 이번 주 이론은 이제 끝났으니까 남은 시간에는 구현 위주로 공부해야겠다는 생각이 들었다.

