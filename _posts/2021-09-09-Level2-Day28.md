---
layout: post
title:  "[Boostcamp AI Tech] 28일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day28"
categories: "Boostcamp-AI-Tech"
tags: [6주차, Level2-U-Stage, NLP]
use_math: true
---

# 부스트캠프 28일차

## 📝 오늘 일정 정리

* 9/9(목)
  - [x] RNN 모델 구현 시작
  - [x] 멘토링 5시~6시
  - [x] 마스터클래스 9/9 (목) 18:00~19:00 주재걸 마스터님

## 🚩 모델 구현 과정

* top-down 방식으로 일단 큰 틀만 짜놓은 상태
* 모델과 데이터셋을 추가적으로 구현 후 마무리할 계획

## 🌱 피어 세션 정리

* 멘토링 질문 준비
* [추천시스템 자료 공유](https://root-decimal-c5d.notion.site/Recommender-System-KR-5b773a06e99145e6855bae391c94dc44)

## 🌼 멘토링

### 질문 Time

* Q. Word2Vec에서 꼭 원핫벡터를 사용해야하나요?
  * 단어들을 구분만 할 수 있다면 원핫벡터가 아니어도 괜찮다.
  * 예를 들면 원핫벡터가 아니라 0.1, 0.9 이런 식으로 사실 라벨 스무싱 할 수도 있다.
* Q. transformer에서도 pack_padded_sequence / Bucketing 를 이용해서 연산량을 효율적으로 만들 수 있을지
  * 연산량을 더 효율적으로 만들 수는 있지만, 사실 sorting 하는 것보다 데이터 분포의 길이를 전체적으로 섞어주는 것이 좋다.
* Q. Batch first 를 사용하지 않는 이유 / 순서가 바뀌는데 어떤 차이가 있는지 (어떤 것이 더 효율적인지)
  * 사실 그냥 사용자 마음이다. 순서가 헷갈리는 분들은 batch first를 사용한다.

### 현업 조언 Time

* 아무래도 요즘 프로덕트 개발에 관심있는 기업들이 많아지다보니, 논문으로 실적낸다기보다 실제 프로덕트화 시켜서 가치창출하는 것이 트렌드인 것 같다.
* 엔지니어나 mlops 쪽은 석사학위가 꼭 없어도 되는 것 같다. 서비스화, 프로덕트화, 인프라 관리 등에 관심이 많다면 취업 기회가 많을 것이다.
* 스타트업을 가면 하나의 골에 포커스해서 나아가고, 그룹사로 가게 되면 계열사 니즈를 파악해서 서포트 하기도 한다.

## 💎 마스터 클래스

### 최신 NLP 트렌드

* [Open-Domain Question Answering System](https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html)
  * [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)
  * Open domain : 다양한 주제로 대화 가능 (ex. [Blender Bot 2.0](https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/))
  * Closed domain : 특정 주제에 대해서 대화 가능 (ex. 금융 비서)
* [Unsupervised Neural Machine Translation](https://arxiv.org/abs/1710.11041)
* [Text Style Transfer](https://blog.diyaml.com/teampost/Text-Style-Transfer/)
  * [Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation](https://arxiv.org/abs/1905.05621)
* Quality Estimation
  * [DeepQuest: a framework for neural-based quality estimation](https://aclanthology.org/C18-1266.pdf)
  * [TransQuest: Translation Quality Estimation with Cross-lingual Transformers](https://arxiv.org/abs/2011.01536)
  * [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)
* Large-scale Language Models
  * [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
  * [oLMpics -- On what Language Model Pre-training Captures](https://arxiv.org/abs/1912.13283)
* Transfer Learning
  * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* In-Context Learning
  * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
* Prompt Tuning
  * [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
  * [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
* Language Models Trained on Code
  * [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
* Multi-Modal Models
  * [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092)
  * [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

## 🚀 학습 회고

* 구현을 하려고 하니까 확실히 제대로 구조를 공부하게 되는 것 같아서 좋은 시도였던 것 같다.
* 다음 주에 배우게 될 트랜스포머도 어떻게 구현할 수 있을지 구조를 정확하게 이해하면서 공부해야겠다는 생각이 들었다.
