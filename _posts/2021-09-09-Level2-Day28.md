---
layout: post
title:  "[Boostcamp AI Tech] 28ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day28"
categories: "Boostcamp-AI-Tech"
tags: [6ì£¼ì°¨, Level2-U-Stage, NLP]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 28ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/9(ëª©)
  - [x] RNN ëª¨ë¸ êµ¬í˜„ ì‹œì‘
  - [x] ë©˜í† ë§ 5ì‹œ~6ì‹œ
  - [x] ë§ˆìŠ¤í„°í´ë˜ìŠ¤ 9/9 (ëª©) 18:00~19:00 ì£¼ì¬ê±¸ ë§ˆìŠ¤í„°ë‹˜

## ğŸš© ëª¨ë¸ êµ¬í˜„ ê³¼ì •

* top-down ë°©ì‹ìœ¼ë¡œ ì¼ë‹¨ í° í‹€ë§Œ ì§œë†“ì€ ìƒíƒœ
* ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì¶”ê°€ì ìœ¼ë¡œ êµ¬í˜„ í›„ ë§ˆë¬´ë¦¬í•  ê³„íš

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* ë©˜í† ë§ ì§ˆë¬¸ ì¤€ë¹„
* [ì¶”ì²œì‹œìŠ¤í…œ ìë£Œ ê³µìœ ](https://root-decimal-c5d.notion.site/Recommender-System-KR-5b773a06e99145e6855bae391c94dc44)

## ğŸŒ¼ ë©˜í† ë§

### ì§ˆë¬¸ Time

* Q. Word2Vecì—ì„œ ê¼­ ì›í•«ë²¡í„°ë¥¼ ì‚¬ìš©í•´ì•¼í•˜ë‚˜ìš”?
  * ë‹¨ì–´ë“¤ì„ êµ¬ë¶„ë§Œ í•  ìˆ˜ ìˆë‹¤ë©´ ì›í•«ë²¡í„°ê°€ ì•„ë‹ˆì–´ë„ ê´œì°®ë‹¤.
  * ì˜ˆë¥¼ ë“¤ë©´ ì›í•«ë²¡í„°ê°€ ì•„ë‹ˆë¼ 0.1, 0.9 ì´ëŸ° ì‹ìœ¼ë¡œ ì‚¬ì‹¤ ë¼ë²¨ ìŠ¤ë¬´ì‹± í•  ìˆ˜ë„ ìˆë‹¤.
* Q. transformerì—ì„œë„ pack_padded_sequence / Bucketing ë¥¼ ì´ìš©í•´ì„œ ì—°ì‚°ëŸ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆì„ì§€
  * ì—°ì‚°ëŸ‰ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ëŠ” ìˆì§€ë§Œ, ì‚¬ì‹¤ sorting í•˜ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„° ë¶„í¬ì˜ ê¸¸ì´ë¥¼ ì „ì²´ì ìœ¼ë¡œ ì„ì–´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤.
* Q. Batch first ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìœ  / ìˆœì„œê°€ ë°”ë€ŒëŠ”ë° ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ (ì–´ë–¤ ê²ƒì´ ë” íš¨ìœ¨ì ì¸ì§€)
  * ì‚¬ì‹¤ ê·¸ëƒ¥ ì‚¬ìš©ì ë§ˆìŒì´ë‹¤. ìˆœì„œê°€ í—·ê°ˆë¦¬ëŠ” ë¶„ë“¤ì€ batch firstë¥¼ ì‚¬ìš©í•œë‹¤.

### í˜„ì—… ì¡°ì–¸ Time

* ì•„ë¬´ë˜ë„ ìš”ì¦˜ í”„ë¡œë•íŠ¸ ê°œë°œì— ê´€ì‹¬ìˆëŠ” ê¸°ì—…ë“¤ì´ ë§ì•„ì§€ë‹¤ë³´ë‹ˆ, ë…¼ë¬¸ìœ¼ë¡œ ì‹¤ì ë‚¸ë‹¤ê¸°ë³´ë‹¤ ì‹¤ì œ í”„ë¡œë•íŠ¸í™” ì‹œì¼œì„œ ê°€ì¹˜ì°½ì¶œí•˜ëŠ” ê²ƒì´ íŠ¸ë Œë“œì¸ ê²ƒ ê°™ë‹¤.
* ì—”ì§€ë‹ˆì–´ë‚˜ mlops ìª½ì€ ì„ì‚¬í•™ìœ„ê°€ ê¼­ ì—†ì–´ë„ ë˜ëŠ” ê²ƒ ê°™ë‹¤. ì„œë¹„ìŠ¤í™”, í”„ë¡œë•íŠ¸í™”, ì¸í”„ë¼ ê´€ë¦¬ ë“±ì— ê´€ì‹¬ì´ ë§ë‹¤ë©´ ì·¨ì—… ê¸°íšŒê°€ ë§ì„ ê²ƒì´ë‹¤.
* ìŠ¤íƒ€íŠ¸ì—…ì„ ê°€ë©´ í•˜ë‚˜ì˜ ê³¨ì— í¬ì»¤ìŠ¤í•´ì„œ ë‚˜ì•„ê°€ê³ , ê·¸ë£¹ì‚¬ë¡œ ê°€ê²Œ ë˜ë©´ ê³„ì—´ì‚¬ ë‹ˆì¦ˆë¥¼ íŒŒì•…í•´ì„œ ì„œí¬íŠ¸ í•˜ê¸°ë„ í•œë‹¤.

## ğŸ’ ë§ˆìŠ¤í„° í´ë˜ìŠ¤

### ìµœì‹  NLP íŠ¸ë Œë“œ

* [Open-Domain Question Answering System](https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html)
  * [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)
  * Open domain : ë‹¤ì–‘í•œ ì£¼ì œë¡œ ëŒ€í™” ê°€ëŠ¥ (ex. [Blender Bot 2.0](https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/))
  * Closed domain : íŠ¹ì • ì£¼ì œì— ëŒ€í•´ì„œ ëŒ€í™” ê°€ëŠ¥ (ex. ê¸ˆìœµ ë¹„ì„œ)
* [Unsupervised Neural Machine Translation](https://arxiv.org/abs/1710.11041)
* [Text Style Transfer](https://blog.diyaml.com/teampost/Text-Style-Transfer/)
  * [Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation](https://arxiv.org/abs/1905.05621)
* Quality Estimation
  * [DeepQuest: a framework for neural-based quality estimation](https://aclanthology.org/C18-1266.pdf)
  * [TransQuest: Translation Quality Estimation with Cross-lingual Transformers](https://arxiv.org/abs/2011.01536)
  * [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)
* Large-scale Language Models
  * [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
  * [oLMpics -- On what Language Model Pre-training Captures](https://arxiv.org/abs/1912.13283)
* Transfer Learning
  * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* In-Context Learning
  * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
* Prompt Tuning
  * [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
  * [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
* Language Models Trained on Code
  * [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
* Multi-Modal Models
  * [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092)
  * [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

## ğŸš€ í•™ìŠµ íšŒê³ 

* êµ¬í˜„ì„ í•˜ë ¤ê³  í•˜ë‹ˆê¹Œ í™•ì‹¤íˆ ì œëŒ€ë¡œ êµ¬ì¡°ë¥¼ ê³µë¶€í•˜ê²Œ ë˜ëŠ” ê²ƒ ê°™ì•„ì„œ ì¢‹ì€ ì‹œë„ì˜€ë˜ ê²ƒ ê°™ë‹¤.
* ë‹¤ìŒ ì£¼ì— ë°°ìš°ê²Œ ë  íŠ¸ëœìŠ¤í¬ë¨¸ë„ ì–´ë–»ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì„ì§€ êµ¬ì¡°ë¥¼ ì •í™•í•˜ê²Œ ì´í•´í•˜ë©´ì„œ ê³µë¶€í•´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.
