---
layout: post
title:  "[Boostcamp AI Tech] 7ì£¼ì°¨ - Huggingface ìŠ¤í„°ë””"
subtitle:   "Naver Boostcamp AI Tech Level2 U Stage"
categories: "Boostcamp-AI-Tech"
tags: [Level2-U-Stage]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 7ì£¼ì°¨

## ğŸ¤— [Huggingface Tutorial](https://huggingface.co/course/chapter1)

### [1] Transformer models

* Transformers
  * íŠ¹ì§•
    * ëª¨ë“  Transformer ëª¨ë¸ë“¤ì€ *language model* ë¡œ í•™ìŠµë¨
    * *transfer learning* ê³¼ì •ì„ í†µí•´ ì£¼ì–´ì§„ ì‘ì—…ì— ë§ê²Œ *fine-tuning* í•˜ê¸°
    * í¸ê²¬ê³¼ í•œê³„ : ì›ë³¸ ëª¨ë¸ì´ ì„±ì°¨ë³„, ì¸ì¢…ì°¨ë³„ ë˜ëŠ” ë™ì„±ì•  í˜ì˜¤ ì½˜í…ì¸ ë¥¼ ë§¤ìš° ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. fine-tuningì„ í•˜ë”ë¼ë„ ì´ëŸ¬í•œ ë‚´ì¬ì  í¸í–¥ì´ ì‚¬ë¼ì§€ì§€ ì•ŠëŠ”ë‹¤.
  * ì£¼ìš” êµ¬ì„±ìš”ì†Œ
    * Encoder : ëª¨ë¸ì´ ì…ë ¥ìœ¼ë¡œë¶€í„° ì´í•´ë¥¼ ì–»ë„ë¡ ìµœì í™”
    * Decoder : Encoderì˜ í‘œí˜„ì„ ë‹¤ë¥¸ ì…ë ¥ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ ìƒì„±ì— ìµœì í™”
  * **BERT-like** \| *auto-encoding* models \| Encoder-only models
    * ì¢…ë¥˜ : [ALBERT](https://huggingface.co/transformers/model_doc/albert.html) \| [BERT](https://huggingface.co/transformers/model_doc/bert.html) \| [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) \| [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html) \| [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
    * íŠ¹ì§• : ë¬¸ì¥ ë¶„ë¥˜(Sentence Classification) ë° ëª…ëª…ëœ ê°œì²´ ì¸ì‹(Named Entity Recognition), ì¶”ì¶œ ì§ˆë¬¸ ë‹µë³€(Extractive Question Answering)ê³¼ ê°™ì´ ì…ë ¥ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ì‘ì—…ì— ì í•© (bi-directional attention)
  * **GPT-like** \| *auto-regressive* models \| Decoder-only models
    * ì¢…ë¥˜ : [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html) \| [GPT](https://huggingface.co/transformers/model_doc/gpt.html) \| [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html) \| [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)
    * íŠ¹ì§• : í…ìŠ¤íŠ¸ ìƒì„±(Text Generation)ê³¼ ê°™ì€ ìƒì„± ì‘ì—…ì— ì í•©
  * **BART/T5-like** \| *sequence-to-sequence* models \| Encoder-Decoder models
    * ì¢…ë¥˜ : [BART](https://huggingface.co/transformers/model_doc/bart.html) \| [mBART](https://huggingface.co/transformers/model_doc/mbart.html) \| [Marian](https://huggingface.co/transformers/model_doc/marian.html) \| [T5](https://huggingface.co/transformers/model_doc/t5.html)
    * íŠ¹ì§• : ìš”ì•½(Summarization), ë²ˆì—­(Translation) ë˜ëŠ” ìƒì„±ì  ì§ˆë¬¸ ë‹µë³€(Generative Question Answering)ê³¼ ê°™ì´ ì£¼ì–´ì§„ ì…ë ¥ì— ë”°ë¼ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ì‘ì—…ì— ê°€ì¥ ì í•©

* High-Level API
  * [pipeline](https://huggingface.co/transformers/main_classes/pipelines.html) : ëª¨ë¸ì„ ì „ì²˜ë¦¬ë¶€í„° í›„ì²˜ë¦¬ê¹Œì§€ ì—°ê²°í•˜ì—¬ ê°„í¸í•˜ê²Œ ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŒ

    ```python
    from transformers import pipeline
    classifier = pipeline("sentiment-analysis")
    classifier("I've been waiting for a HuggingFace course my whole life.")
    ```

  * [Inference API](https://huggingface.co/inference-api) : ë¸Œë¼ìš°ì €ë¥¼ í†µí•´ ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

### [2] Using huggingface transformers

1. Preprocessing with a tokenizer
   * Tokenizer : í…ìŠ¤íŠ¸ ì…ë ¥ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•¨
     * ì…ë ¥ì„ *í† í°* ì´ë¼ê³  í•˜ëŠ” ë‹¨ì–´, í•˜ìœ„ ë‹¨ì–´ ë˜ëŠ” ê¸°í˜¸ë¡œ ë¶„í• 
     * ê° í† í°ì„ ì •ìˆ˜ë¡œ ë§¤í•‘
     * ëª¨ë¸ì— ìœ ìš©í•  ìˆ˜ ìˆëŠ” ì¶”ê°€ ì…ë ¥ ì¶”ê°€
     ```python
     from transformers import AutoTokenizer
     raw_inputs = [
         "I've been waiting for a HuggingFace course my whole life.", 
         "I hate this so much!",
     ]
     checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
     tokenizer = AutoTokenizer.from_pretrained(checkpoint)
     inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
     print(inputs) # 'input_ids', 'attention_mask'
     ```
   * ì¢…ë¥˜
     * Word-based
     * Character-based
     * Subword tokenization
   * ê³¼ì •
     * Encoding
     * Decoding

2. Going through the model
   * Model head : hidden statesì˜ ê³ ì°¨ì› ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹¤ë¥¸ ì°¨ì›ì— íˆ¬ì˜
     ![image](https://user-images.githubusercontent.com/35680202/134797641-198ee928-542a-4447-b923-471881e61972.png)
     ```python
     from transformers import AutoModel, AutoModelForSequenceClassification
     checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
     #model = AutoModel.from_pretrained(checkpoint)
     model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
     outputs = model(**inputs)
     ```
     * `*Model` : retrieve the hidden states
     * `*ForSequenceClassification` : model with a sequence classification head (to be able to classify the sentences as positive or negative)
   * Different loading methods
     ```python
     from transformers import BertConfig, BertModel
     # random initialized model
     config = BertConfig()
     model = BertModel(config)
     # pre-trained model
     model = BertModel.from_pretrained("bert-base-cased") # ~/.cache/huggingface/transformers ì— ë‹¤ìš´
     ```
   * Saving methods
     ```python
     model.save_pretrained(PATH) # config.json, pytorch_model.bin íŒŒì¼ì— ì €ì¥ë¨
     ```
     * `config.json` : attributes necessary to build the model architecture
     * `pytorch_model.bin` : *state dictionary*, modelâ€™s weights

3. Postprocessing the output
   * ëª¨ë¸ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ì–»ì€ ê°’ì´ ë°˜ë“œì‹œ ê·¸ ìì²´ë¡œ ì˜ë¯¸ê°€ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.
   * ì¼ë°˜ì ìœ¼ë¡œ SoftMax ë ˆì´ì–´ë¥¼ í†µê³¼í•œ í›„ êµì°¨ ì—”íŠ¸ë¡œí”¼ì™€ ê°™ì€ ì‹¤ì œ ì†ì‹¤ í•¨ìˆ˜ì— ë„£ëŠ”ë‹¤.
   * `model.config.id2label`ë¥¼ ì´ìš©í•´ì„œ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
   ```python
   import torch
   predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
   print(predictions)
   ```

## ğŸ¤— [Huggingface Notebooks](https://huggingface.co/transformers/notebooks.html)

1. How to fine-tune a model on text classification
2. How to fine-tune a model on language modeling
3. How to fine-tune a model on token classification
4. How to fine-tune a model on question answering
5. How to fine-tune a model on multiple choice
6. How to fine-tune a model on translation
7. How to fine-tune a model on summarization

* [huggingface datasets viewer](https://huggingface.co/datasets/viewer/)
