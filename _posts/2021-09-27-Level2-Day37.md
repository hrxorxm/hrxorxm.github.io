---
layout: post
title:  "[Boostcamp AI Tech] 37일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day37"
categories: "Boostcamp-AI-Tech"
tags: [9주차, Level2-P-Stage]
use_math: true
---

# 부스트캠프 37일차

## 📝 오늘 일정 정리

* 9/27(월)
  - [x] 타운홀 미팅 11:00~12:00
  - [x] KLUE 강의 수강
    - [x] (1강) 인공지능과 자연어 처리
    - [x] (2강) 자연어의 전처리
  - [x] 오피스 아워 9/27 (월) 18:00~19:30 AI Drug Discovery (시각화 - 김준태 멘토님)

## 📢 타운홀 미팅

1. V100 서버는 대회 시작 당일 오전 10시부터 할당받을 수 있습니다.
2. **KLUE부터 1주차부터 팀 단위 제출하는 방식으로 진행됩니다.** 대회 첫날(9/27 월요일) 오후 2시까지 팀원들과 함께 AI Stages에서 팀 결성을 해주세요.
3. 제출은 팀결성이 마무리된 이후(2시 이후)로 가능합니다. 팀 결성이 마무리되기 전, 개별적으로 제출하는 일이 없도록 유의해주세요.

## 📚 강의 내용 정리

### [1강] 인공지능과 자연어 처리

* 인공지능의 탄생과 자연어 처리
  * [ELIZA 챗봇](https://www.eclecticenergies.com/psyche/eliza) : 최초의 대화형(chitchat) 챗봇, 튜링 테스트를 적용할 수 있는 최초의 Human-Like AI
  * 컴퓨터의 자연어 처리
    * Encoder : 벡터 형태로 자연어를 인코딩
    * Decoder : 벡터를 자연어로 디코딩
  * 자연어 단어 임베딩
    * Word2Vec : 중심 단어의 주변 단어들을 이용해 중심 단어를 추론하는 방식으로 학습
      * 장점 : 벡터 연산을 통한 추론이 가능
      * 단점 : subword, OOV(Out of vocabulary) 에서 적용 불가능
    * [FastText](https://research.fb.com/fasttext/) : Word2Vec과 유사하지만, n-gram으로 나누어 학습
      * [[Paper Reivew] FastText: Enriching Word Vectors with Subword Information](https://www.youtube.com/watch?v=7UA21vg4kKE)]
      * 장점 : 오탈자, OOV, 등장 횟수가 적은 학습 단어에 강세
      * 단점 : 동형어, 다의어 문제나 문맥을 고려하지 못하는 것은 여전함
* 딥러닝 기반의 자연어 처리와 언어모델
  * 언어모델 : 자연어의 법칙을 컴퓨터로 모사한 모델, 주어진 단어들로부터 그 다음에 등장할 단어의 확률을 예측하는 방식으로 학습
    * 마코트 체인 모델(Markov Chain Model) : 통계와 단어의 n-gram을 기반으로 계산
    * RNN 기반의 언어모델 : 최종 출력된 context vector 를 이용하여 분류하는 등의 방식
  * [Seq2Seq](https://www.youtube.com/watch?v=4DzKM0vgG1Y)
    * RNN 기반의 Seq2Seq : RNN 구조인 Encoder를 통해 얻은 context vector 를 RNN 구조인 Decoder에 넣어 출력
  * [Seq2Seq + Attention](https://www.youtube.com/watch?v=WsQLdu2JMgI)
    * RNN 기반의 Seq2Seq with Attention : 긴 입력 시퀀스, 고정된 context vector 등 RNN 구조의 문제점을 해결하기 위해 등장
  * **Selt-Attention**
    * Transformer : 다양한 언어모델들이 Transformer를 기반으로 발전하고 있다.

* Further Questions
  * Embedding이 잘 되었는지, 안되었는지를 평가할 수 있는 방법은 무엇이 있을까요?
    - WordSim353, Spearman's correlation, Analogy test
  * Vanilar Transformer는 어떤 문제가 있고, 이걸 어떻게 극복할 수 있을까요?
    - Longformer, Linformer, Reformer

### [2강] 자연어의 전처리

* 통계학적 분석
  * Token 개수 파악 후 아웃라이어 제거
  * 빈도수 확인 후 사전(dictionary) 정의
* **전처리(Preprocessing)**
  * 기능
    * 학습에 사용될 데이터를 수집&가공하는 모든 프로세스
    * Task의 성능을 가장 확실하게 올릴 수 있는 방법
  * 종류
    * 개행문자, 공백 제거 + 띄어쓰기, 문장분리 보정
    * 특수문자, 이메일, 링크, 불용어, 조사, 제목, 중복 표현 제거
  * 문자열 함수
    * 대소문자 변환 : `upper()`, `lower()`, `capitalize()`, `title()`, `swapcase()`
    * 편집, 치환 : `strip()`, `rstrip()`, `lstrip()`, `replace(a, b)`
    * 분리, 결합 : `split()`, `''.join(list)`, `lines.splitlines()`
    * 구성 문자열 판별 : `isdigit()`, `isalpha()`, `isalnum()`, `islower()`, `isupper()`, `isspace()`, `startswith('hi')`, `endswith('hi')`
    * 검색 : `count('hi')`, `find('hi')`, `rfind('hi')`, `index('hi')`, `rindex('hi')`
* **토큰화(Tokenizing)**
  * 기능
    * 주어진 데이터를 토큰(Token) 단위로 나누는 작업
    * 어절, 단어, 형태소, 음절, 자소, WordPiece 등
  * 종류 예시
    * 문장 토큰화(Sentene Tokenizing) : 문장 분리
    * 단어 토큰화(Word Tokenizing) : 구두점 분리, 단어 분리
  * 한국어 토큰화
    * 영어와 달리 띄어쓰기만으로는 부족
    * 따라서 어절을 의미를 가지는 최소 단위인 형태소로 분리함

## 🔎 실습 내용 정리

### 한국어 전처리

* [Newspaper3k: Article scraping & curation](https://github.com/codelucas/newspaper) : url 정보를 입력해주면 텍스트를 추출해주는 라이브러리

  ```python
  from newspaper import Article
  article = Article(news_url, language='ko')
  article.download()
  article.parse()
  print('title:', article.title)
  print('context:', article.text)
  ```

* [Kss \| A Toolkit for Korean sentence segmentation](https://github.com/hyunwoongko/kss) : 한국어 문장분리기

  ```python
  import kss
  splited_sent = kss.split_sentences(sent)
  ```

* [soynlp](https://github.com/lovit/soynlp) : 한국어 자연어처리, 단어 추출/토크나이저/품사판별/전처리 기능 제공

  ```python
  from soynlp.normalizer import *
  print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))
  ```

* [PyKoSpacing](https://github.com/haven-jeon/PyKoSpacing) : Automatic Korean word spacing with Python

  ```python
  from pykospacing import Spacing
  spacing = Spacing()
  text = spacing(text) # 띄어쓰기 보정
  ```

* [py-hanspell](https://github.com/ssut/py-hanspell) : 네이버 맞춤법 검사기를 이용한 파이썬용 한글 맞춤법 검사 라이브러리

  ```python
  from hanspell import spell_checker
  spelled_sent = spell_checker.check(text)
  checked_sent = spelled_sent.checked # 교정된 문장
  ```

* [konlpy](https://konlpy.org/ko/latest/) : 한국어 형태소 분석기

  ```python
  from konlpy.tag import Mecab
  mecab = Mecab()
  morphs = mecab.pos("아버지가방에들어가신다.", join=False)
  print(morphs)
  ```

* 정규표현식을 통한 전처리

  * HTML 태그 제거

    ```python
    text = re.sub(r"<[^>]+>\s+(?=<)|<[^>]+>", "", text).strip()
    ```

  * 이메일 제거

    ```python
    text = re.sub(r"[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", "", text).strip()
    ```

  * 해시태그(\#) 제거

    ```python
    text = re.sub(r"#\S+", "", text).strip()
    ```

  * 멘션태그(@) 제거

    ```python
    text = re.sub(r"@\w+", "", text).strip()
    ```

  * URL 제거

    ```python
    text = re.sub(r"(http|https)?:\/\/\S+\b|www\.(\w+\.)+\S*", "", text).strip()
    text = re.sub(r"pic\.(\w+\.)+\S*", "", text).strip()
    ```

  * 두 개 이상의 연속된 공백을 하나로 치환

    ```python
    text = re.sub(r"\s+", " ", text).strip()
    ```

* 그 외 파이썬을 이용한 전처리

  * 중복된 문장 제거

    ```python
    from collections import OrderedDict
    texts = list(OrderedDict.fromkeys(texts))
    ```

  * 불용어 제거

    ```python
    sent = [w for w in sent.split(' ') if w not in stopwords]
    ```

  * [유니코드](https://jrgraphix.net/r/Unicode/0020-007F) 기반으로 필터링

    ```python
    # 문자 -> 숫자
    ord(w) # 문자 -> 10진수 변환
    hex(ord(w)) # 문자 -> 10진수 -> 16진수 변환
    # 숫자 -> 문자
    int('03FF',16) # 16진수 -> 10진수 변환
    chr(int('03FF',16)) # 10 진수 -> 16진수 -> 문자 변환
    ```

    * 주의사항
      * 영어, 독일어, 베트남어 일부는 같은 유니코드 체계를 이용해 표현된다.
      * 유니코드 범위에 공백을 표현하는 유니코드가 속해 있을 수 있어, 원하지 않게 띄어쓰기를 제거할 수 있다.

### 한국어 토크나이징

* 의미를 지닌 단위로 자연어를 분절

  * 어절 단위(띄어쓰기 단위)

    ```python
    tokenized_text = text.split(" ")
    ```

  * 형태소 단위

    ```python
    from konlpy.tag import Mecab
    mecab = Mecab()
    tokenized_text = [lemma[0] for lemma in mecab.pos(text)]
    ```

  * 음절 단위

    ```python
    tokenized_text = list(text)
    ```

  * 자소 단위

    ```python
    import hgtk
    tokenized_text = list(hgtk.text.decompose(text))
    ```

  * WordPiece 단위 - [Huggingface - Using a pretrained tokenizer](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#using-a-pretrained-tokenizer)

    ```python
    #!pip install transformers
    from tokenizers import BertWordPieceTokenizer
    tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
    tokenized_text = tokenizer.encode(text)
    ```

* Model의 학습 시, 동일한 size로 입력
  ```python
  tokenized_text += ["padding"] * (max_seq_length - len(tokenized_text)) # padding
  tokenized_text = tokenized_text[0:max_seq_length] # filtering
  ```

* Further Reading
  * [청와대 국민청원 데이터 전처리 (소개)](https://www.youtube.com/watch?v=9QW7QL8fvv0)
  * [청와대 국민청원 데이터 전처리 (실습)](https://www.youtube.com/watch?v=HIcXyyzefYQ)

* Reference
  * huggingface Tokenizer
    * [서브워드 구축하기](https://keep-steady.tistory.com/37)
    * [Huggingface Transformers의 attention mask 문서](https://huggingface.co/transformers/glossary.html#attention-mask)
    * [Huggingface Transformers의 attention mask 구현](https://github.com/huggingface/tokenizers/tree/2fbd6779f6bdeb55c0fb9cceb3716ec20fc92646/bindings/python/py_src/tokenizers/implementations)
  * KoNLPy
    * [한국어 전처리 위한 Huggingface + KoNLPy 실습](https://gist.github.com/lovit/259bc1d236d78a77f044d638d0df300c)

## 🔎 스페셜 미션

* Special Mission 1 - 한국어 전처리 연습하기

## 🌱 피어 세션 정리

* will be updated later...

## 💎 오피스 아워

* 신약개발 : 비용과 시간이 많이 든다.
* AI 공부도 중요하지만 사업성을 고려한 아이템도 고민해보면 좋을 것 같다.

## 🚀 학습 회고

* 늦은 학습 정리...
* 대회도 진행하면서 강의 내용을 정리하자니 꽤 시간이 많이 걸렸다.
* 그래도 강의 내용이 다 유익해서 시간을 투자한 보람이 있다!
