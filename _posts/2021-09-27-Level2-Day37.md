---
layout: post
title:  "[Boostcamp AI Tech] 37ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day37"
categories: "Boostcamp-AI-Tech"
tags: [9ì£¼ì°¨, Level2-P-Stage]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 37ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/27(ì›”)
  - [x] íƒ€ìš´í™€ ë¯¸íŒ… 11:00~12:00
  - [x] KLUE ê°•ì˜ ìˆ˜ê°•
    - [x] (1ê°•) ì¸ê³µì§€ëŠ¥ê³¼ ìì—°ì–´ ì²˜ë¦¬
    - [x] (2ê°•) ìì—°ì–´ì˜ ì „ì²˜ë¦¬
  - [x] ì˜¤í”¼ìŠ¤ ì•„ì›Œ 9/27 (ì›”) 18:00~19:30 AI Drug Discovery (ì‹œê°í™” - ê¹€ì¤€íƒœ ë©˜í† ë‹˜)

## ğŸ“¢ íƒ€ìš´í™€ ë¯¸íŒ…

1. V100 ì„œë²„ëŠ” ëŒ€íšŒ ì‹œì‘ ë‹¹ì¼ ì˜¤ì „ 10ì‹œë¶€í„° í• ë‹¹ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **KLUEë¶€í„° 1ì£¼ì°¨ë¶€í„° íŒ€ ë‹¨ìœ„ ì œì¶œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.** ëŒ€íšŒ ì²«ë‚ (9/27 ì›”ìš”ì¼) ì˜¤í›„ 2ì‹œê¹Œì§€ íŒ€ì›ë“¤ê³¼ í•¨ê»˜ AI Stagesì—ì„œ íŒ€ ê²°ì„±ì„ í•´ì£¼ì„¸ìš”.
3. ì œì¶œì€ íŒ€ê²°ì„±ì´ ë§ˆë¬´ë¦¬ëœ ì´í›„(2ì‹œ ì´í›„)ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŒ€ ê²°ì„±ì´ ë§ˆë¬´ë¦¬ë˜ê¸° ì „, ê°œë³„ì ìœ¼ë¡œ ì œì¶œí•˜ëŠ” ì¼ì´ ì—†ë„ë¡ ìœ ì˜í•´ì£¼ì„¸ìš”.

## ğŸ“š ê°•ì˜ ë‚´ìš© ì •ë¦¬

### [1ê°•] ì¸ê³µì§€ëŠ¥ê³¼ ìì—°ì–´ ì²˜ë¦¬

* ì¸ê³µì§€ëŠ¥ì˜ íƒ„ìƒê³¼ ìì—°ì–´ ì²˜ë¦¬
  * [ELIZA ì±—ë´‡](https://www.eclecticenergies.com/psyche/eliza) : ìµœì´ˆì˜ ëŒ€í™”í˜•(chitchat) ì±—ë´‡, íŠœë§ í…ŒìŠ¤íŠ¸ë¥¼ ì ìš©í•  ìˆ˜ ìˆëŠ” ìµœì´ˆì˜ Human-Like AI
  * ì»´í“¨í„°ì˜ ìì—°ì–´ ì²˜ë¦¬
    * Encoder : ë²¡í„° í˜•íƒœë¡œ ìì—°ì–´ë¥¼ ì¸ì½”ë”©
    * Decoder : ë²¡í„°ë¥¼ ìì—°ì–´ë¡œ ë””ì½”ë”©
  * ìì—°ì–´ ë‹¨ì–´ ì„ë² ë”©
    * Word2Vec : ì¤‘ì‹¬ ë‹¨ì–´ì˜ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì´ìš©í•´ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì¶”ë¡ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
      * ì¥ì  : ë²¡í„° ì—°ì‚°ì„ í†µí•œ ì¶”ë¡ ì´ ê°€ëŠ¥
      * ë‹¨ì  : subword, OOV(Out of vocabulary) ì—ì„œ ì ìš© ë¶ˆê°€ëŠ¥
    * [FastText](https://research.fb.com/fasttext/) : Word2Vecê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, n-gramìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ
      * [[Paper Reivew] FastText: Enriching Word Vectors with Subword Information](https://www.youtube.com/watch?v=7UA21vg4kKE)]
      * ì¥ì  : ì˜¤íƒˆì, OOV, ë“±ì¥ íšŸìˆ˜ê°€ ì ì€ í•™ìŠµ ë‹¨ì–´ì— ê°•ì„¸
      * ë‹¨ì  : ë™í˜•ì–´, ë‹¤ì˜ì–´ ë¬¸ì œë‚˜ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ëŠ” ê²ƒì€ ì—¬ì „í•¨
* ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ìì—°ì–´ ì²˜ë¦¬ì™€ ì–¸ì–´ëª¨ë¸
  * ì–¸ì–´ëª¨ë¸ : ìì—°ì–´ì˜ ë²•ì¹™ì„ ì»´í“¨í„°ë¡œ ëª¨ì‚¬í•œ ëª¨ë¸, ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ë¡œë¶€í„° ê·¸ ë‹¤ìŒì— ë“±ì¥í•  ë‹¨ì–´ì˜ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
    * ë§ˆì½”íŠ¸ ì²´ì¸ ëª¨ë¸(Markov Chain Model) : í†µê³„ì™€ ë‹¨ì–´ì˜ n-gramì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
    * RNN ê¸°ë°˜ì˜ ì–¸ì–´ëª¨ë¸ : ìµœì¢… ì¶œë ¥ëœ context vector ë¥¼ ì´ìš©í•˜ì—¬ ë¶„ë¥˜í•˜ëŠ” ë“±ì˜ ë°©ì‹
  * [Seq2Seq](https://www.youtube.com/watch?v=4DzKM0vgG1Y)
    * RNN ê¸°ë°˜ì˜ Seq2Seq : RNN êµ¬ì¡°ì¸ Encoderë¥¼ í†µí•´ ì–»ì€ context vector ë¥¼ RNN êµ¬ì¡°ì¸ Decoderì— ë„£ì–´ ì¶œë ¥
  * [Seq2Seq + Attention](https://www.youtube.com/watch?v=WsQLdu2JMgI)
    * RNN ê¸°ë°˜ì˜ Seq2Seq with Attention : ê¸´ ì…ë ¥ ì‹œí€€ìŠ¤, ê³ ì •ëœ context vector ë“± RNN êµ¬ì¡°ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥
  * **Selt-Attention**
    * Transformer : ë‹¤ì–‘í•œ ì–¸ì–´ëª¨ë¸ë“¤ì´ Transformerë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.

* Further Questions
  * Embeddingì´ ì˜ ë˜ì—ˆëŠ”ì§€, ì•ˆë˜ì—ˆëŠ”ì§€ë¥¼ í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”?
    - WordSim353, Spearman's correlation, Analogy test
  * Vanilar TransformerëŠ” ì–´ë–¤ ë¬¸ì œê°€ ìˆê³ , ì´ê±¸ ì–´ë–»ê²Œ ê·¹ë³µí•  ìˆ˜ ìˆì„ê¹Œìš”?
    - Longformer, Linformer, Reformer

### [2ê°•] ìì—°ì–´ì˜ ì „ì²˜ë¦¬

* í†µê³„í•™ì  ë¶„ì„
  * Token ê°œìˆ˜ íŒŒì•… í›„ ì•„ì›ƒë¼ì´ì–´ ì œê±°
  * ë¹ˆë„ìˆ˜ í™•ì¸ í›„ ì‚¬ì „(dictionary) ì •ì˜
* **ì „ì²˜ë¦¬(Preprocessing)**
  * ê¸°ëŠ¥
    * í•™ìŠµì— ì‚¬ìš©ë  ë°ì´í„°ë¥¼ ìˆ˜ì§‘&ê°€ê³µí•˜ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤
    * Taskì˜ ì„±ëŠ¥ì„ ê°€ì¥ í™•ì‹¤í•˜ê²Œ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•
  * ì¢…ë¥˜
    * ê°œí–‰ë¬¸ì, ê³µë°± ì œê±° + ë„ì–´ì“°ê¸°, ë¬¸ì¥ë¶„ë¦¬ ë³´ì •
    * íŠ¹ìˆ˜ë¬¸ì, ì´ë©”ì¼, ë§í¬, ë¶ˆìš©ì–´, ì¡°ì‚¬, ì œëª©, ì¤‘ë³µ í‘œí˜„ ì œê±°
  * ë¬¸ìì—´ í•¨ìˆ˜
    * ëŒ€ì†Œë¬¸ì ë³€í™˜ : `upper()`, `lower()`, `capitalize()`, `title()`, `swapcase()`
    * í¸ì§‘, ì¹˜í™˜ : `strip()`, `rstrip()`, `lstrip()`, `replace(a, b)`
    * ë¶„ë¦¬, ê²°í•© : `split()`, `''.join(list)`, `lines.splitlines()`
    * êµ¬ì„± ë¬¸ìì—´ íŒë³„ : `isdigit()`, `isalpha()`, `isalnum()`, `islower()`, `isupper()`, `isspace()`, `startswith('hi')`, `endswith('hi')`
    * ê²€ìƒ‰ : `count('hi')`, `find('hi')`, `rfind('hi')`, `index('hi')`, `rindex('hi')`
* **í† í°í™”(Tokenizing)**
  * ê¸°ëŠ¥
    * ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í† í°(Token) ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…
    * ì–´ì ˆ, ë‹¨ì–´, í˜•íƒœì†Œ, ìŒì ˆ, ìì†Œ, WordPiece ë“±
  * ì¢…ë¥˜ ì˜ˆì‹œ
    * ë¬¸ì¥ í† í°í™”(Sentene Tokenizing) : ë¬¸ì¥ ë¶„ë¦¬
    * ë‹¨ì–´ í† í°í™”(Word Tokenizing) : êµ¬ë‘ì  ë¶„ë¦¬, ë‹¨ì–´ ë¶„ë¦¬
  * í•œêµ­ì–´ í† í°í™”
    * ì˜ì–´ì™€ ë‹¬ë¦¬ ë„ì–´ì“°ê¸°ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±
    * ë”°ë¼ì„œ ì–´ì ˆì„ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ìµœì†Œ ë‹¨ìœ„ì¸ í˜•íƒœì†Œë¡œ ë¶„ë¦¬í•¨

## ğŸ” ì‹¤ìŠµ ë‚´ìš© ì •ë¦¬

### í•œêµ­ì–´ ì „ì²˜ë¦¬

* [Newspaper3k: Article scraping & curation](https://github.com/codelucas/newspaper) : url ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ë©´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

  ```python
  from newspaper import Article
  article = Article(news_url, language='ko')
  article.download()
  article.parse()
  print('title:', article.title)
  print('context:', article.text)
  ```

* [Kss \| A Toolkit for Korean sentence segmentation](https://github.com/hyunwoongko/kss) : í•œêµ­ì–´ ë¬¸ì¥ë¶„ë¦¬ê¸°

  ```python
  import kss
  splited_sent = kss.split_sentences(sent)
  ```

* [soynlp](https://github.com/lovit/soynlp) : í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬, ë‹¨ì–´ ì¶”ì¶œ/í† í¬ë‚˜ì´ì €/í’ˆì‚¬íŒë³„/ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì œê³µ

  ```python
  from soynlp.normalizer import *
  print(repeat_normalize('ì™€í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•«', num_repeats=2))
  ```

* [PyKoSpacing](https://github.com/haven-jeon/PyKoSpacing) : Automatic Korean word spacing with Python

  ```python
  from pykospacing import Spacing
  spacing = Spacing()
  text = spacing(text) # ë„ì–´ì“°ê¸° ë³´ì •
  ```

* [py-hanspell](https://github.com/ssut/py-hanspell) : ë„¤ì´ë²„ ë§ì¶¤ë²• ê²€ì‚¬ê¸°ë¥¼ ì´ìš©í•œ íŒŒì´ì¬ìš© í•œê¸€ ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

  ```python
  from hanspell import spell_checker
  spelled_sent = spell_checker.check(text)
  checked_sent = spelled_sent.checked # êµì •ëœ ë¬¸ì¥
  ```

* [konlpy](https://konlpy.org/ko/latest/) : í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°

  ```python
  from konlpy.tag import Mecab
  mecab = Mecab()
  morphs = mecab.pos("ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤.", join=False)
  print(morphs)
  ```

* ì •ê·œí‘œí˜„ì‹ì„ í†µí•œ ì „ì²˜ë¦¬

  * HTML íƒœê·¸ ì œê±°

    ```python
    text = re.sub(r"<[^>]+>\s+(?=<)|<[^>]+>", "", text).strip()
    ```

  * ì´ë©”ì¼ ì œê±°

    ```python
    text = re.sub(r"[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", "", text).strip()
    ```

  * í•´ì‹œíƒœê·¸(\#) ì œê±°

    ```python
    text = re.sub(r"#\S+", "", text).strip()
    ```

  * ë©˜ì…˜íƒœê·¸(@) ì œê±°

    ```python
    text = re.sub(r"@\w+", "", text).strip()
    ```

  * URL ì œê±°

    ```python
    text = re.sub(r"(http|https)?:\/\/\S+\b|www\.(\w+\.)+\S*", "", text).strip()
    text = re.sub(r"pic\.(\w+\.)+\S*", "", text).strip()
    ```

  * ë‘ ê°œ ì´ìƒì˜ ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¹˜í™˜

    ```python
    text = re.sub(r"\s+", " ", text).strip()
    ```

* ê·¸ ì™¸ íŒŒì´ì¬ì„ ì´ìš©í•œ ì „ì²˜ë¦¬

  * ì¤‘ë³µëœ ë¬¸ì¥ ì œê±°

    ```python
    from collections import OrderedDict
    texts = list(OrderedDict.fromkeys(texts))
    ```

  * ë¶ˆìš©ì–´ ì œê±°

    ```python
    sent = [w for w in sent.split(' ') if w not in stopwords]
    ```

  * [ìœ ë‹ˆì½”ë“œ](https://jrgraphix.net/r/Unicode/0020-007F) ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§

    ```python
    # ë¬¸ì -> ìˆ«ì
    ord(w) # ë¬¸ì -> 10ì§„ìˆ˜ ë³€í™˜
    hex(ord(w)) # ë¬¸ì -> 10ì§„ìˆ˜ -> 16ì§„ìˆ˜ ë³€í™˜
    # ìˆ«ì -> ë¬¸ì
    int('03FF',16) # 16ì§„ìˆ˜ -> 10ì§„ìˆ˜ ë³€í™˜
    chr(int('03FF',16)) # 10 ì§„ìˆ˜ -> 16ì§„ìˆ˜ -> ë¬¸ì ë³€í™˜
    ```

    * ì£¼ì˜ì‚¬í•­
      * ì˜ì–´, ë…ì¼ì–´, ë² íŠ¸ë‚¨ì–´ ì¼ë¶€ëŠ” ê°™ì€ ìœ ë‹ˆì½”ë“œ ì²´ê³„ë¥¼ ì´ìš©í•´ í‘œí˜„ëœë‹¤.
      * ìœ ë‹ˆì½”ë“œ ë²”ìœ„ì— ê³µë°±ì„ í‘œí˜„í•˜ëŠ” ìœ ë‹ˆì½”ë“œê°€ ì†í•´ ìˆì„ ìˆ˜ ìˆì–´, ì›í•˜ì§€ ì•Šê²Œ ë„ì–´ì“°ê¸°ë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤.

### í•œêµ­ì–´ í† í¬ë‚˜ì´ì§•

* ì˜ë¯¸ë¥¼ ì§€ë‹Œ ë‹¨ìœ„ë¡œ ìì—°ì–´ë¥¼ ë¶„ì ˆ

  * ì–´ì ˆ ë‹¨ìœ„(ë„ì–´ì“°ê¸° ë‹¨ìœ„)

    ```python
    tokenized_text = text.split(" ")
    ```

  * í˜•íƒœì†Œ ë‹¨ìœ„

    ```python
    from konlpy.tag import Mecab
    mecab = Mecab()
    tokenized_text = [lemma[0] for lemma in mecab.pos(text)]
    ```

  * ìŒì ˆ ë‹¨ìœ„

    ```python
    tokenized_text = list(text)
    ```

  * ìì†Œ ë‹¨ìœ„

    ```python
    import hgtk
    tokenized_text = list(hgtk.text.decompose(text))
    ```

  * WordPiece ë‹¨ìœ„ - [Huggingface - Using a pretrained tokenizer](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#using-a-pretrained-tokenizer)

    ```python
    #!pip install transformers
    from tokenizers import BertWordPieceTokenizer
    tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
    tokenized_text = tokenizer.encode(text)
    ```

* Modelì˜ í•™ìŠµ ì‹œ, ë™ì¼í•œ sizeë¡œ ì…ë ¥
  ```python
  tokenized_text += ["padding"] * (max_seq_length - len(tokenized_text)) # padding
  tokenized_text = tokenized_text[0:max_seq_length] # filtering
  ```

* Further Reading
  * [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì†Œê°œ)](https://www.youtube.com/watch?v=9QW7QL8fvv0)
  * [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì‹¤ìŠµ)](https://www.youtube.com/watch?v=HIcXyyzefYQ)

* Reference
  * huggingface Tokenizer
    * [ì„œë¸Œì›Œë“œ êµ¬ì¶•í•˜ê¸°](https://keep-steady.tistory.com/37)
    * [Huggingface Transformersì˜ attention mask ë¬¸ì„œ](https://huggingface.co/transformers/glossary.html#attention-mask)
    * [Huggingface Transformersì˜ attention mask êµ¬í˜„](https://github.com/huggingface/tokenizers/tree/2fbd6779f6bdeb55c0fb9cceb3716ec20fc92646/bindings/python/py_src/tokenizers/implementations)
  * KoNLPy
    * [í•œêµ­ì–´ ì „ì²˜ë¦¬ ìœ„í•œ Huggingface + KoNLPy ì‹¤ìŠµ](https://gist.github.com/lovit/259bc1d236d78a77f044d638d0df300c)

## ğŸ” ìŠ¤í˜ì…œ ë¯¸ì…˜

* Special Mission 1 - í•œêµ­ì–´ ì „ì²˜ë¦¬ ì—°ìŠµí•˜ê¸°

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* will be updated later...

## ğŸ’ ì˜¤í”¼ìŠ¤ ì•„ì›Œ

* ì‹ ì•½ê°œë°œ : ë¹„ìš©ê³¼ ì‹œê°„ì´ ë§ì´ ë“ ë‹¤.
* AI ê³µë¶€ë„ ì¤‘ìš”í•˜ì§€ë§Œ ì‚¬ì—…ì„±ì„ ê³ ë ¤í•œ ì•„ì´í…œë„ ê³ ë¯¼í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.

## ğŸš€ í•™ìŠµ íšŒê³ 

* ëŠ¦ì€ í•™ìŠµ ì •ë¦¬...
* ëŒ€íšŒë„ ì§„í–‰í•˜ë©´ì„œ ê°•ì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ìë‹ˆ ê½¤ ì‹œê°„ì´ ë§ì´ ê±¸ë ¸ë‹¤.
* ê·¸ë˜ë„ ê°•ì˜ ë‚´ìš©ì´ ë‹¤ ìœ ìµí•´ì„œ ì‹œê°„ì„ íˆ¬ìí•œ ë³´ëŒì´ ìˆë‹¤!
