---
layout: post
title:  "[Boostcamp AI Tech] 3ì£¼ì°¨ - PyTorch"
subtitle:   "Naver Boostcamp AI Tech Level1 U Stage"
categories: "Boostcamp-AI-Tech"
tags: [Level1-U-Stage]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 3ì£¼ì°¨

## [1ê°•] Introduction to PyTorch

* í”„ë ˆì„ì›Œí¬ë¥¼ ê³µë¶€í•˜ëŠ” ê²ƒì´ ê³§ ë”¥ëŸ¬ë‹ì„ ê³µë¶€í•˜ëŠ” ê²ƒì´ë‹¤.
* ì¢…ë¥˜
  * PyTorch(facebook)
    * Define by Run (Dynamic Computation Graph) : ì‹¤í–‰ì„ í•˜ë©´ì„œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹
    * ê°œë°œê³¼ì •ì—ì„œ ë””ë²„ê¹…ì´ ì‰½ë‹¤. (pythonic code)
  * TensorFlow(Google)
    * Define and Run : ê·¸ë˜í”„ë¥¼ ë¨¼ì € ì •ì˜í•œ í›„ ì‹¤í–‰ì‹œì ì— ë°ì´í„°ë¥¼ í˜ë ¤ë³´ëƒ„(feed)
    * Production, Scalability, Cloud, Multi-GPU ì—ì„œ ì¥ì ì„ ê°€ì§„ë‹¤.
    * Keras ëŠ” Wrapper ë‹¤. (high-level API)
* ìš”ì•½
  * PyTorch = Numpy + AutoGrad + Function

## [2ê°•] PyTorch Basics

* Tensor : ë‹¤ì°¨ì› Arraysë¥¼ í‘œí˜„í•˜ëŠ” í´ë˜ìŠ¤ (numpyì˜ ndarrayì™€ ë™ì¼)
  * list to tensor, ndarray to tensor ëª¨ë‘ ê°€ëŠ¥
  * tensorëŠ” GPUì— ì˜¬ë ¤ì„œ ì‚¬ìš©ê°€ëŠ¥
* Operations : numpy ì‚¬ìš©ë²•ì´ ê±°ì˜ ê·¸ëŒ€ë¡œ ì ìš©ëœë‹¤.
  * reshape() ëŒ€ì‹  view() í•¨ìˆ˜ ì‚¬ìš© ê¶Œì¥
  * squeeze(), unsqueeze() ì°¨ì´ì™€ ì‚¬ìš©ë²• ìµíˆê¸°
  * í–‰ë ¬ê³±ì€ mm(),matmul() ì‚¬ìš© (matmulì€ broadcasting ì§€ì›)
  * ë‚´ì ì€ dot() ì‚¬ìš©
  * `nn.functional` ì—ì„œ ë‹¤ì–‘í•œ ìˆ˜ì‹ ë³€í™˜ ì§€ì›
* AutoGrad : ìë™ ë¯¸ë¶„ ì§€ì›
  * tensor(requires_grad=True)ë¡œ ì„ ì–¸í•œ í›„ backward() í•¨ìˆ˜ ì‚¬ìš©
  * [A GENTLE INTRODUCTION TO `TORCH.AUTOGRAD`](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
  * [PYTORCH: TENSORS AND AUTOGRAD](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html)

## [3ê°•] PyTorch í”„ë¡œì íŠ¸ êµ¬ì¡° ì´í•´í•˜ê¸°

* ëª©í‘œ
  * ì´ˆê¸° ë‹¨ê³„ : í•™ìŠµê³¼ì • í™•ì¸, ë””ë²„ê¹…
  * ë°°í¬ ë° ê³µìœ  ë‹¨ê³„ : ì‰¬ìš´ ì¬í˜„, ê°œë°œ ìš©ì´ì„±, ìœ ì§€ë³´ìˆ˜ í–¥ìƒ ë“±
* ë°©ë²•
  * OOP + ëª¨ë“ˆ => í”„ë¡œì íŠ¸
  * ì‹¤í–‰, ë°ì´í„°, ëª¨ë¸, ì„¤ì •, ë¡œê¹…, ì§€í‘œ, ìœ í‹¸ë¦¬í‹° ë“±ì„ ë¶„ë¦¬í•˜ì—¬ í”„ë¡œì íŠ¸ í…œí”Œë¦¿í™”
* í…œí”Œë¦¿ ì¶”ì²œ
  * [Pytorch Template](https://github.com/victoresque/pytorch-template) <- ì‹¤ìŠµ ì§„í–‰ğŸ“Œ
  * [Pytorch Template 2](https://github.com/FrancescoSaverioZuppichini/PyTorch-Deep-Learning-Template)
  * [Pytorch Lightning Template](https://github.com/PyTorchLightning/deep-learning-project-template)
  * [Pytorch Lightning](https://www.pytorchlightning.ai/)
  * [Pytoch Lightning + NNI Boilerplate](https://github.com/davinnovation/pytorch-boilerplate)
* êµ¬ê¸€ ì½”ë©ê³¼ vscode ì—°ê²°í•˜ê¸°
  * colab
    * [ngrok](https://ngrok.com/) ê°€ì…í•˜ê¸°
    * `colab-ssh` ì„¤ì¹˜í•˜ê¸°
    * í† í° ë„£ì–´ì„œ `launch_ssh` ì‹¤í–‰í•´ì„œ ì—°ê²°ì •ë³´ í™•ì¸í•˜ê¸°
  * vscode
    * extension : Remote - SSH install í•˜ê¸°
    * Remote-SSH: Add New SSH Host ì—ì„œ `sshÂ root@[HostName]Â -pÂ [Port]` ì…ë ¥
    * Remote-SSH: Connect to Host ì—ì„œ ìœ„ì—ì„œ ë“±ë¡í•œ host ì—°ê²°í•˜ê¸°
    * `cdÂ /content`ë¡œ ê°€ë©´ ì½”ë©ì—ì„œ ë‹¤ìš´ë°›ì•˜ë˜ íŒŒì¼ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤.
    * `/content/pytorch-template/MNIST-example`ì—ì„œ `python3 train.py -cÂ config.json` ìœ¼ë¡œ ì˜ˆì œë¥¼ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆë‹¤.

## [4ê°•] AutoGrad & Optimizer

* ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì³ : ë¸”ë¡ ë°˜ë³µì˜ ì—°ì†
  * Layer = Block
* `nn.Module` : Layerì˜ base class
  * Input, Output, Forward, Backward, parameter ì •ì˜
* `nn.Parameter` : Tensor ê°ì²´ì˜ ìƒì† ê°ì²´
  * `nn.Module` ë‚´ì—ì„œ parameterë¡œ ì •ì˜ë  ë•Œ, `required_grad=True` ì§€ì •í•˜ê¸°
  * `layer.parameter()`ì—ëŠ” `required_grad=True` ë¡œ ì§€ì •ëœ ë³€ìˆ˜ë“¤ë§Œ í¬í•¨ëœë‹¤.
  * ëŒ€ë¶€ë¶„ì˜ layerì— weights ê°’ë“¤ì´ ì§€ì •ë˜ì–´ ìˆì–´ì„œ ì§ì ‘ ì§€ì •í•  ì¼ì€ ê±°ì˜ ì—†ê¸´ í•¨
* Backward from the scratch
  * `nn.Module`ì—ì„œ `backward`ì™€ `optimizer` ì˜¤ë²„ë¼ì´ë”©í•˜ë©´ ëœë‹¤.
* ì¶”ê°€ìë£Œ
  * [Pytorchë¡œ Linear Regressioní•˜ê¸°](https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817)
  * [Pytorchë¡œ Logistic Regessioní•˜ê¸°](https://medium.com/dair-ai/implementing-a-logistic-regression-model-from-scratch-with-pytorch-24ea062cd856)

## [5ê°•] Dataset & Dataloader

* `Dataset(Data, transforms)` : ë°ì´í„°ë¥¼ ì…ë ¥í•˜ëŠ” ë°©ì‹ì˜ í‘œì¤€í™”
  * `__init__()` : ì´ˆê¸° ë°ì´í„° ìƒì„± ë°©ë²• ì§€ì •
  * `__len__()` : ë°ì´í„°ì˜ ì „ì²´ ê¸¸ì´
  * `__getitem__()` : indexê°’ì„ ì£¼ì—ˆì„ ë•Œ ë°˜í™˜ë˜ëŠ” ë°ì´í„°ì˜ í˜•íƒœ
* `DataLoader(Dataset, batch, shuffle, ...)` : ë°ì´í„°ì˜ batchë¥¼ ìƒì„±í•´ì£¼ëŠ” í´ë˜ìŠ¤
* [TORCHVISION.DATASETS](https://pytorch.org/vision/stable/datasets.html)
  * [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) : ìœ„ì˜ ì†ŒìŠ¤ì½”ë“œ ì°¸ê³ í•´ì„œ ë°ì´í„°ì…‹ ë§Œë“œëŠ” ì—°ìŠµí•´ë³´ê¸°
* ì¶”ê°€ìë£Œ
  * [Pytorch Dataset, Dataloader íŠœí† ë¦¬ì–¼](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)

## [6ê°•] ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

* ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì €ì¥ ë° ë¡œë“œ
  * `model.state_dict()` : ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° í‘œì‹œ
  ```python
  torch.save(model.state_dict(),"model.pt") # ì €ì¥
  new_model = ModelClass()
  new_model.load_state_dict(torch.load("model.pt")) # ë¡œë“œ
  ```
* ëª¨ë¸ í˜•íƒœ(architecture)ì™€ íŒŒë¼ë¯¸í„° ì €ì¥ ë° ë¡œë“œ
  ```python
  torch.save(model, "model.pt") # ì €ì¥
  new_modelÂ =Â torch.load("model.pt") # ë¡œë“œ
  ```
* ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
  ```python
  from torchsummary import summary
  summary(model, (3, 224, 224))
  ```
  ```python
  for name layer in model.named_modules():
      print(name, layer)
  ```

* checkpoints
  * í•™ìŠµì˜ ì¤‘ê°„ ê²°ê³¼ ì €ì¥, ì¼ë°˜ì ìœ¼ë¡œ epoch, loss, merticì„ í•¨ê»˜ ì €ì¥
    ```python
    torch.save({
        'epoch':e, 'loss': epoch_loss, 'optimizer_state_dict': optimizer.state_dict(),
        'model_statae_dict': model.state_dict()
    }, PATH) # ì €ì¥
    checkpoint = torch.load(PATH) # ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    ```

* pretrained model transfer learning
  * ë‹¤ë¥¸ ë°ì´í„°ì…‹(ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹)ìœ¼ë¡œ ë§Œë“  ëª¨ë¸ì„ í˜„ì¬ ë°ì´í„°ì— ì ìš©
    * ëª¨ë¸ ì¶”ì²œ : [CV models](https://github.com/rwightman/pytorch-image-models) & [NLP models](https://huggingface.co/models)
  * ë§ˆì§€ë§‰ ë ˆì´ì–´ ìˆ˜ì •í•˜ê¸°
    * `vgg.fc =Â torch.nn.Linear(1000, 1)` : ë§¨ ë§ˆì§€ë§‰ì— fc ë ˆì´ì–´ ì¶”ê°€í•˜ê¸° (ì¶”ì²œ)
    * `vgg.classifier._modules['6']Â = torch.nn.Linear(4096, 1)` : ë§¨ ë§ˆì§€ë§‰ ë ˆì´ì–´ êµì²´í•˜ê¸°
  * Freezing : pretrained model í™œìš© ì‹œ ëª¨ë¸ì˜ ì¼ë¶€ë¶„ì„ frozen ì‹œí‚¨ë‹¤.
    ```python
    for param in mymodel.parameters():
        param.requires_grad = False # frozen
    for param in mymodel.linear_layers.parameters():
        param.requires_grad = True # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚´ë¦¬ê¸°
    ```

## [7ê°•] Monitoring tools for PyTorch

* ëª©í‘œ : printë¬¸ì€ ì´ì œ ê·¸ë§Œ!
* âœ¨[Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)âœ¨ : TensorFlowì˜ í”„ë¡œì íŠ¸ë¡œ ë§Œë“¤ì–´ì§„ ì‹œê°í™” ë„êµ¬, PyTorchë„ ì—°ê²° ê°€ëŠ¥
  * ì¢…ë¥˜
    * scalar : metric ë“± í‘œì‹œ
    * graph : ê³„ì‚° ê·¸ë˜í”„ í‘œì‹œ
    * histogram : ê°€ì¤‘ì¹˜ ë“± ê°’ì˜ ë¶„í¬ í‘œì‹œ
    * image / text : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ
    * mesh : 3d í˜•íƒœë¡œ ë°ì´í„° í‘œí˜• (ìœ„ì— ë¹„í•´ ìì£¼ ì“°ì§„ ì•ŠìŒ)
  * ë°©ë²•
    * ê¸°ë¡ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„± : `logs/[ì‹¤í—˜í´ë”]` ë¡œ í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
    * ê¸°ë¡ ìƒì„± ê°ì²´ `SummaryWriter` ìƒì„±
    * `writer.add_scalar()` ë“±ìœ¼ë¡œ ê°’ë“¤ì„ ê¸°ë¡
    * `writer.flush()` : diskì— ì“°ê¸°
    * `%load_extÂ tensorboard` : í…ì„œë³´ë“œ ë¶€ë¥´ê¸°
    * `%tensorboard --logdirÂ {logs_base_dir}` : 6006í¬íŠ¸ì— ìë™ìœ¼ë¡œ í…ì„œë³´ë“œ ìƒì„±
* âœ¨[Weight & Biases(WandB)](https://wandb.ai/site)âœ¨ : ë¨¸ì‹ ëŸ¬ë‹ ì‹¤í—˜ ì§€ì›, MLOpsì˜ ëŒ€í‘œì ì¸ íˆ´ì´ ë˜ê³  ìˆë‹¤.
  * ê¸°ëŠ¥
    * í˜‘ì—… / code versioning / ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡ ë“±
    * ìœ ë£Œì§€ë§Œ, ë¬´ë£Œ ê¸°ëŠ¥ë„ ìˆë‹¤.
  * ë°©ë²•
    * í™ˆí˜ì´ì§€ : íšŒì›ê°€ì… -> API í‚¤ í™•ì¸ -> ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
    * `wandb.init(project, entity)` : ì—¬ê¸°ì„œ API ì…ë ¥í•´ì„œ ì ‘ì†
    * `wandb.init(project, config)` : config ì„¤ì •
    * `wandb.log()` : ê¸°ë¡
* [Pytorch Lightning Logger ëª©ë¡](https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html)

## [8ê°•] Multi-GPU í•™ìŠµ

* ì •ì˜
  * Multi-GPU : GPUë¥¼ 2ê°œ ì´ìƒ ì“¸ ë•Œ
  * Single Node Multi GPU : í•œ ëŒ€ì˜ ì»´í“¨í„°ì— ì—¬ëŸ¬ ê°œì˜ GPU
* ë°©ë²•
  * ëª¨ë¸ ë³‘ë ¬í™”(Model parallel)
    * ëª¨ë¸ì„ ë‚˜ëˆ„ê¸° (ex. AlexNet)
    * ëª¨ë¸ì˜ ë³‘ëª©, íŒŒì´í”„ë¼ì¸ ì–´ë ¤ì›€ ë“±ì˜ ë¬¸ì œ
      ```python
      # __init__()
      self.seq1 = nn.Sequential(~~).to('cuda:0') # ì²«ë²ˆì§¸ ëª¨ë¸ì„ cuda 0ì— í• ë‹¹
      self.seq2 = nn.Sequential(~~).to('cuda:1') # ë‘ë²ˆì§¸ ëª¨ë¸ì„ cuda 1ì— í• ë‹¹
      # forward()
      x = self.seq2(self.seq1(x).to('cuda:1')) # ë‘ ëª¨ë¸ ì—°ê²°í•˜ê¸°
      ```
  * ë°ì´í„° ë³‘ë ¬í™”(Data parallel)
    * ë°ì´í„°ë¥¼ ë‚˜ëˆ  GPUì— í• ë‹¹í•œ í›„, ê²°ê³¼ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” ë°©ë²•
    * `DataParallel`
      * íŠ¹ì§• : ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ë¶„ë°°í•œ í›„ í‰ê·  (ì¤‘ì•™ ì½”ë””ë„¤ì´í„° í•„ìš”)
      * ë¬¸ì œ : GPU ì‚¬ìš© ë¶ˆê· í˜•, Batch ì‚¬ì´ì¦ˆ ê°ì†Œ, GIL(Global Interpreter Lock)
      * `parallel_model =Â torch.nn.DataParallel(model)`
    * `DistributedDataParallel` : ê° CPUë§ˆë‹¤ process ìƒì„±í•˜ì—¬ ê°œë³„ GPUì— í• ë‹¹
      * íŠ¹ì§• : ê°œë³„ì ìœ¼ë¡œ ì—°ì‚°ì˜ í‰ê· ì„ ëƒ„ (ì¤‘ì•™ ì½”ë””ë„¤ì´í„° ë¶ˆí•„ìš”, ê°ê°ì´ ì½”ë””ë„¤ì´í„° ì—­í•  ìˆ˜í–‰)
      * ë°©ë²• : ê° CPUë§ˆë‹¤ process ìƒì„±í•˜ì—¬ ê°œë³„ GPUì— í• ë‹¹ (CPUë„ GPU ê°œìˆ˜ë§Œí¼ í• ë‹¹)
        ```python
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)
        trainloader = torch.utils.data.DataLoader(
            train_data, batch_size=batch_size, 
            shuffle=False, pin_memory=True, num_workers=[GPUê°œìˆ˜x4],
            sampler=train_sampler # sampler ì‚¬ìš©
        )
        ...
        # Distributed dataparallel ì •ì˜
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])
        ```
      * ë” ìì„¸í•œ ì½”ë“œëŠ” [ì—¬ê¸°](https://blog.si-analytics.ai/12)
* ì¶”ê°€ìë£Œ
  * [PyTorch Lightning - MULTI-GPU TRAINING](https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html)
  * [PyTorch - GETTING STARTED WITH DISTRIBUTED DATA PARALLEL](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
  * [ê´€ë ¨ ì§‘í˜„ì „ ì˜ìƒ](https://youtu.be/w4a-ARCEiqU?t=1978)
  * TensorRT : NVIDIAê°€ ì œê³µí•˜ëŠ” ë„êµ¬

## [9ê°•] Hyperparameter Tuning

* ëª©í‘œ : ë§ˆì§€ë§‰ 0.01ì˜ ì„±ëŠ¥ì´ë¼ë„ ë†’ì—¬ì•¼ í•  ë•Œ!
* ê¸°ë²• : Grid Search / Random Search / ë² ì´ì§€ì•ˆ ê¸°ë²•(BOHB ë“±)
* âœ¨[Ray](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)âœ¨ : multi-node multi processing ì§€ì› ëª¨ë“ˆ, Hyperparameter Searchë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë“ˆ ì œê³µ
  * ë°©ë²•
    * `fromÂ rayÂ importÂ tune`
    * configì— search space ì§€ì •
      ```python
      config = {
              "l1": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),
              "l2": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),
              "lr": tune.loguniform(1e-4, 1e-1),
              "batch_size": tune.choice([2, 4, 8, 16])}
      ```
    * í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ ì§€ì •
      ```python
      scheduler = ASHAScheduler(
              metric="loss",
              mode="min",
              max_t=max_num_epochs,
              grace_period=1,
              reduction_factor=2)
      ```
    * ê²°ê³¼ ì¶œë ¥ ì–‘ì‹ ì§€ì •
      ```python
      reporter = CLIReporter(
              # parameter_columns=["l1", "l2", "lr", "batch_size"],
              metric_columns=["loss", "accuracy", "training_iteration"])
      ```
    * ë³‘ë ¬ ì²˜ë¦¬ ì–‘ì‹ìœ¼ë¡œ í•™ìŠµ ì‹¤í–‰
      ```python
      result = tune.run(
              partial(train_cifar, data_dir=data_dir), # train_cifar : full training function
              resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
              config=config,
              num_samples=num_samples,
              scheduler=scheduler,
              progress_reporter=reporter)
      ```
    * í•™ìŠµ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
      * `best_trial = result.get_best_trial("loss", "min", "last")`

## [10ê°•] Pytorch Troubleshooting

* OOM(Out Of Memory) : ì™œ, ì–´ë””ì„œ ë°œìƒí–ˆëŠ”ì§€ ì•Œê¸° ì–´ë ¤ì›€
* ì‰¬ìš´ ë°©ë²• : Batch size ì¤„ì´ê³  GPU clean í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰
* ë‹¤ë¥¸ ë°©ë²•
  * `GPUtil.showUtilization()` : GPUì˜ ìƒíƒœë¥¼ ë³´ì—¬ì¤€ë‹¤.
  * `torch.cuda.empty_cache()` : ì‚¬ìš©ë˜ì§€ ì•Šì€ GPUìƒ cache ì •ë¦¬ (í•™ìŠµ loop ì „ì— ì‹¤í–‰í•˜ë©´ ì¢‹ë‹¤)
  * `total_lossÂ +=Â loss.item` : `total_loss += loss` ì—ì„œëŠ” ê³„ì‚° ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì—,  python ê¸°ë³¸ ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë”í•´ì¤€ë‹¤.
  * `delÂ ë³€ìˆ˜` : í•„ìš”ê°€ ì—†ì–´ì§„ ë³€ìˆ˜ëŠ” ì ì ˆíˆ ì‚­ì œí•˜ê¸° (ì •í™•íˆëŠ” ë³€ìˆ˜ì™€ ë©”ëª¨ë¦¬ ê´€ê³„ ëŠê¸°)
  * try-exceptë¬¸ì„ ì´ìš©í•´ì„œ ê°€ëŠ¥í•œ batch size ì‹¤í—˜í•´ë³´ê¸°
  * `with torch.no_grad()` : Inference ì‹œì ì— ì‚¬ìš©
  * tensorì˜ float precisionì„ 16bitë¡œ ì¤„ì¼ ìˆ˜ë„ ìˆë‹¤. (ë§ì´ ì“°ì´ì§„ ì•ŠìŒ)
* [ì´ ì™¸ GPU ì—ëŸ¬ ì •ë¦¬ ë¸”ë¡œê·¸](https://brstar96.github.io/shoveling/device_error_summary/)
