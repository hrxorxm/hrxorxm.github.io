---
layout: post
title:  "[Boostcamp AI Tech] 34ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day34"
categories: "Boostcamp-AI-Tech"
tags: [7ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 34ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/17(ê¸ˆ)
  - [x] ë…¼ë¬¸ ìŠ¤í„°ë””(BART) 13:00~14:00
  - [x] í”¼ì–´ì„¸ì…˜ ì£¼ì œ : huggingface notebooks ì‹¤ìŠµ í›„ ê²½í—˜ ê³µìœ 
  - [x] ë§ˆìŠ¤í„°í´ë˜ìŠ¤ 9/17 (ê¸ˆ) 18:00~19:00 ì£¼ì¬ê±¸ ë§ˆìŠ¤í„°ë‹˜

## ğŸ“– ë…¼ë¬¸ ìŠ¤í„°ë””

### [BART](https://arxiv.org/abs/1910.13461)

* [ë”¥ëŸ¬ë‹ ë…¼ë¬¸ì½ê¸°ëª¨ì„ ë°œí‘œ](https://youtu.be/VmYMnpDLPEo)
* [DSBA ì—°êµ¬ì‹¤ ë°œí‘œë…¼ë¬¸](https://youtu.be/v7diENO2mEA)
* í•µì‹¬
  * Bidirectional Encoder + Autoregressive Decoder
  * Sequence-to-Sequence trained with **denoising** as pretraining objective
* ê¸°ì¡´ ì—°êµ¬ (Cloze Tasksì—ì„œ ì˜ê°ë°›ì€ MLMì˜ ë³€í˜•)
  * XLNet : Masked Tokenì´ ì˜ˆì¸¡ë  ìˆœì„œë¥¼ ê°œì„  (Permmutation Operation)
  * **SpanBERT** : Masked Tokenì˜ ë¶„í¬ë¥¼ ê°œì„  â†’ token ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ span ë‹¨ìœ„ë¡œ ë§ˆìŠ¤í‚¹ í›„ ì˜ˆì¸¡ â†’ text span ê°„ ê´€ê³„ ì¶”ë¡ ì´ í•„ìš”
  * UniLM : Masked Tokenì„ ëŒ€ì²´í•œ Contextë¥¼ ê°œì„ 
* **denoising** as pretraining objective : ì…ë ¥ì— ë…¸ì´ì¦ˆê°€ ê¼ˆì„ ë•Œ ë³µì›í•˜ëŠ” task
  1. Token Masking : BERTì™€ ë™ì¼í•œ MLM
  2. Token Deletion : ì„ì˜ì˜ tokenë“¤ì„ ì œê±°
  3. **Text Infilling** : Text Spanì„ ìƒ˜í”Œë§í•˜ê³  ë‹¨ì¼ `[MASK]` tokenìœ¼ë¡œ ëŒ€ì²´í•œë‹¤. ì–¼ë§ˆë‚˜ ë§ì€ tokenë“¤ì´ ìœ ì‹¤ë˜ì—ˆëŠ”ì§€ë„ ì˜ˆì¸¡í•´ì•¼í•¨
  4. Sentence Permutation : Documentë¥¼ Sentencesë¡œ ë‚˜ëˆ„ê³  ìˆœì„œë¥¼ ì„ì˜ë¡œ ì„ëŠ”ë‹¤.
  5. Document Rotation : ì„ì˜ë¡œ tokenì„ ì„ íƒí•˜ê³  documentë¥¼ í•´ë‹¹ tokenìœ¼ë¡œ ì‹œì‘í•˜ë„ë¡ rotate. ë¬¸ì¥ì˜ ì‹œì‘ì´ ì–´ë””ì¸ì§€ í•™ìŠµ
* ê²°ê³¼ : Translationì€ ì˜ ëª»í•˜ì§€ë§Œ Summarization ì„ ì •ë§ ì˜í•œë‹¤.
* ì°¸ê³ í•˜ë©´ ì¢‹ì€ ì˜ìƒ : [Machine Translation Survey](https://youtu.be/18iH6VX-IU4)

## ğŸ¤— [Huggingface Notebooks](https://huggingface.co/transformers/notebooks.html)

* [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/master/examples/summarization.ipynb)

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* ì½”ë“œ ì‹¤ìŠµ ë‚´ìš© ê³µìœ 
  1. How to fine-tune a model on text classification
  2. How to fine-tune a model on language modeling
  3. How to fine-tune a model on token classification
  4. How to fine-tune a model on question answering
  5. How to fine-tune a model on multiple choice
  6. How to fine-tune a model on translation ğŸ™‹â€â™€ï¸
  7. How to fine-tune a model on summarization

* [huggingface datasets viewer](https://huggingface.co/datasets/viewer/)

## ğŸ’ ë§ˆìŠ¤í„° í´ë˜ìŠ¤

* ê³ ë¯¼ í•´ê²° & ì§ˆì˜ì‘ë‹µ
  * [Tracking Progress in Natural Language Processing](https://github.com/sebastianruder/NLP-progress)
  * [Natural Language Processing Tasks and Selected References](https://github.com/Kyubyong/nlp_tasks)
  * CS, ìˆ˜í•™ ë“± ê¸°ë³¸ì ì¸ ê³µë¶€ ë“±í•œì‹œ í•˜ì§€ ì•Šê¸°
  * ë‹¤ë¥¸ ì‚¬ëŒí•œí…Œ ì„¤ëª…í•  ìˆ˜ ìˆê³ , ê·¸ ì‚¬ëŒì´ ì§ˆë¬¸í–ˆì„ ë•Œ ëª¨ë¥´ì§€ ì•Šì•„ì•¼ ì œëŒ€ë¡œ ê³µë¶€í–ˆë‹¤ê³  ë§í•  ìˆ˜ ìˆë‹¤.
  * ê¶ê¸ˆì¦ì„ ê°€ì§€ê³  ê³„ì† ì§ˆë¬¸ì„ í•˜ê³  ë‹µì„ ì°¾ìœ¼ë©´ì„œ ê³µë¶€í•˜ê¸°!

## ğŸš€ í•™ìŠµ íšŒê³ 

* ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ì´ì œ ì–´ëŠì •ë„ ê°ì´ ì¡íˆëŠ”ë°, ì „ì²˜ë¦¬ ë¶€ë¶„(íŠ¹íˆ í•œêµ­ì–´ ì „ì²˜ë¦¬)ì— ëŒ€í•œ ê³µë¶€ë¥¼ ë” í•´ì•¼í•  ê²ƒ ê°™ë‹¤.
* ì¶”ì„ ì—°íœ´ê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ê¸°ë³¸ ê°œë…ë“¤ì„ ë‹¤ì§ˆ ìˆ˜ ìˆëŠ” ë§ˆì§€ë§‰ ì‹œê¸°ì¸ ê²ƒ ê°™ì•„ì„œ ì˜ í™œìš©í•˜ë ¤ê³  í•œë‹¤.
