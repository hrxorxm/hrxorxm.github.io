---
layout: post
title:  "[Boostcamp AI Tech] 34일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day34"
categories: "Boostcamp-AI-Tech"
tags: [7주차]
use_math: true
---

# 부스트캠프 34일차

## 📝 오늘 일정 정리

* 9/17(금)
  - [x] 논문 스터디(BART) 13:00~14:00
  - [x] 피어세션 주제 : huggingface notebooks 실습 후 경험 공유
  - [x] 마스터클래스 9/17 (금) 18:00~19:00 주재걸 마스터님

## 📖 논문 스터디

### [BART](https://arxiv.org/abs/1910.13461)

* [딥러닝 논문읽기모임 발표](https://youtu.be/VmYMnpDLPEo)
* [DSBA 연구실 발표논문](https://youtu.be/v7diENO2mEA)
* 핵심
  * Bidirectional Encoder + Autoregressive Decoder
  * Sequence-to-Sequence trained with **denoising** as pretraining objective
* 기존 연구 (Cloze Tasks에서 영감받은 MLM의 변형)
  * XLNet : Masked Token이 예측될 순서를 개선 (Permmutation Operation)
  * **SpanBERT** : Masked Token의 분포를 개선 → token 단위가 아니라 span 단위로 마스킹 후 예측 → text span 간 관계 추론이 필요
  * UniLM : Masked Token을 대체한 Context를 개선
* **denoising** as pretraining objective : 입력에 노이즈가 꼈을 때 복원하는 task
  1. Token Masking : BERT와 동일한 MLM
  2. Token Deletion : 임의의 token들을 제거
  3. **Text Infilling** : Text Span을 샘플링하고 단일 `[MASK]` token으로 대체한다. 얼마나 많은 token들이 유실되었는지도 예측해야함
  4. Sentence Permutation : Document를 Sentences로 나누고 순서를 임의로 섞는다.
  5. Document Rotation : 임의로 token을 선택하고 document를 해당 token으로 시작하도록 rotate. 문장의 시작이 어디인지 학습
* 결과 : Translation은 잘 못하지만 Summarization 을 정말 잘한다.
* 참고하면 좋은 영상 : [Machine Translation Survey](https://youtu.be/18iH6VX-IU4)

## 🤗 [Huggingface Notebooks](https://huggingface.co/transformers/notebooks.html)

* [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/master/examples/summarization.ipynb)

## 🌱 피어 세션 정리

* 코드 실습 내용 공유
  1. How to fine-tune a model on text classification
  2. How to fine-tune a model on language modeling
  3. How to fine-tune a model on token classification
  4. How to fine-tune a model on question answering
  5. How to fine-tune a model on multiple choice
  6. How to fine-tune a model on translation 🙋‍♀️
  7. How to fine-tune a model on summarization

* [huggingface datasets viewer](https://huggingface.co/datasets/viewer/)

## 💎 마스터 클래스

* 고민 해결 & 질의응답
  * [Tracking Progress in Natural Language Processing](https://github.com/sebastianruder/NLP-progress)
  * [Natural Language Processing Tasks and Selected References](https://github.com/Kyubyong/nlp_tasks)
  * CS, 수학 등 기본적인 공부 등한시 하지 않기
  * 다른 사람한테 설명할 수 있고, 그 사람이 질문했을 때 모르지 않아야 제대로 공부했다고 말할 수 있다.
  * 궁금증을 가지고 계속 질문을 하고 답을 찾으면서 공부하기!

## 🚀 학습 회고

* 모델에 대해서는 이제 어느정도 감이 잡히는데, 전처리 부분(특히 한국어 전처리)에 대한 공부를 더 해야할 것 같다.
* 추석 연휴가 마지막으로 기본 개념들을 다질 수 있는 마지막 시기인 것 같아서 잘 활용하려고 한다.
