---
layout: post
title:  "[Boostcamp AI Tech] 38일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day38"
categories: "Boostcamp-AI-Tech"
tags: [9주차, Level2-P-Stage]
use_math: true
---

# 부스트캠프 38일차

## 📝 오늘 일정 정리

* 9/28(화)
  - [x] KLUE 강의 수강
    - [x] (3강) BERT 언어모델 소개
    - [x] (4강) 한국어 BERT 언어 모델 학습

## 📚 강의 내용 정리

### [3강] BERT 언어모델 소개

* BERT 언어모델
  * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
  * [BERT 톺아보기](http://docs.likejazz.com/bert/)
  * 모델 구조 : Transformer Encoder, All-to-all network
  * 사전학습 태스크 : Masked Language Model, Next Sentence Prediction
* BERT 모델 응용 (`BERT Multi-lingual pretrained model` 사용)
  * 감성 분석 데이터셋 : [네이버 영화 리뷰 코퍼스](https://github.com/e9t/nsmc)
  * 관계 추출 데이터셋 : KAIST가 구축한 Silver data
  * 의미 비교 데이터셋 : 디지털 동반자 패러프레이징 질의 문장 데이터를 이용하여 질문-질문 데이터 생성 및 학습
  * 개체명 분석 : ETRI 개체명 인식 데이터
  * 기계 독해 : LG CNS가 공개한 한국어 AQ 데이터셋, [KorQuAD](https://korquad.github.io/)
* 한국어 BERT 모델
  * [한국어 tokenizing에 따른 성능 비교](https://arxiv.org/abs/2010.02534)
  * Advanced BERT model : KBQA에서 주요 entity 추출, entity tag 부착, entity embedding layer 추가를 통해 개선 가능

### [4강] 한국어 BERT 언어 모델 학습

* BERT 모델 학습 : 도메인 특화 Task의 경우, 도메인 특화 학습 데이터를 사용하여 새로 학습하는 것이 좋다.
* BERT 모델 학습 단계
  1. Tokenizer 만들기
  2. 데이터셋 확보
  3. Next Sentence Prediction (NSP)
  4. Masked Language Model (MLM)
* 학습을 위한 데이터
  * `input_ids` : for Token Embeddings
  * `token_type_ids` : Segment Embeddings
* [BERT 추가 설명](https://jiho-ml.com/weekly-nlp-28/)

## 🔎 스페셜 미션

* Special Mission 2 - Huggingface hub에 BERT 모델 공유하기

## 🌱 피어 세션 정리

* will be updated later...

## 🚀 학습 회고

* 대회 데이터셋 EDA 하면서 토론 게시판에 텍스트 게시물도 올려봤다.
* 더 좋은 시각화를 해보고 싶은 욕심이 생겼다.
