---
layout: post
title:  "[Boostcamp AI Tech] 32ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day32"
categories: "Boostcamp-AI-Tech"
tags: [7ì£¼ì°¨, Level2-U-Stage, NLP]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 32ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/15(ìˆ˜)
  - [x] NLP ì½”ë“œ ì‹¤ìŠµ
    - [x] [ì‹¤ìŠµ] Multi-head Attention
    - [x] [ì‹¤ìŠµ] Masked Multi-head Attention
    - [x] [ì‹¤ìŠµ] Transformers Library 1 - BERT base
    - [x] [ì‹¤ìŠµ] Transformers Library 2 - GPT-2
  - [ ] NLP ì´ë¡  ê°•ì˜ ì„ íƒ ê³¼ì œ
    - [x] [ì„ íƒ ê³¼ì œ 1] BERT Fine-tuning
    - [x] [ì„ íƒ ê³¼ì œ 2] NMT training with Fairseq Assignment
    - [ ] [ì„ íƒ ê³¼ì œ 3] Byte Pair Encoding Assignment
  - [x] ë…¼ë¬¸ ìŠ¤í„°ë””(ELMO, GPT-1) 13:00~14:00

## ğŸ” ì‹¤ìŠµ ë‚´ìš© ì •ë¦¬

### Multi-head Attention

* Batch data
  * B : batch size = 10
  * L : maximum sequence length = 20
  
  ```python
  batch_data # (B, L)
  ```

* Embedding
  * V : vocab size = 100
  * d_model : model hidden size = 512
  
  ```python
  embedding = nn.Embedding(V, d_model)
  batch_emb = embedding(batch_data) # (B, L, d_model)
  ```

* Calculate Attention
  * H : head ê°œìˆ˜ = 8
  * d_k = d_model // num_heads = 64
  
  ```python
  # linear transformation matrixs
  w_q = nn.Linear(d_model, d_model)
  w_k = nn.Linear(d_model, d_model)
  w_v = nn.Linear(d_model, d_model)
  # (B, L, d_model) -> (B, L, d_model) -> (B, L, H, d_k) -> (B, H, L, d_k)
  q = w_q(batch_emb).view(B, -1, H, d_k).transpose(1, 2)
  k = w_k(batch_emb).view(B, -1, H, d_k).transpose(1, 2)
  v = w_v(batch_emb).view(B, -1, H, d_k).transpose(1, 2)
  ```

* Scaled Dot-Product Self-Attention
  ```python
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, H, L, d_k) x (B, H, d_k, L) = (B, H, L, L)
  attn_dists = F.softmax(attn_scores, dim=-1)  # (B, H, L, L)
  attn_values = torch.matmul(attn_dists, v)  # (B, H, L, L) x (B, H, L, d_k) = (B, H, L, d_k)
  ```

* Concat & Linear transformation
  * [``contiguous()``](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html) : ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ê³µê°„ì— ë°ì´í„° ë³µì‚¬
  
  ```python
  attn_values = attn_values.transpose(1, 2)  # (B, L, H, d_k)
  attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)
  w_0 = nn.Linear(d_model, d_model)
  outputs = w_0(attn_values) # (B, L, d_model)
  ```

### Masked Multi-head Attention

* Mask
  * padding_mask : padding ëœ ë¶€ë¶„ì„ False ì²˜ë¦¬
  * nopeak_mask : ì¹˜íŒ…í•˜ë©´ ì•ˆë˜ëŠ” ë¶€ë¶„ì„ False ì²˜ë¦¬
    * [`torch.tril`](https://pytorch.org/docs/stable/generated/torch.tril.html) : ì•„ë˜ ì‚¼ê°í˜• ë°˜í™˜ (ë‚˜ë¨¸ì§€ ìš”ì†ŒëŠ” 0)
  * mask : ìœ„ ë‘˜ì„ bitwise and ì—°ì‚°
  
  ```python
  padding_mask = (batch_data != pad_id).unsqueeze(1)  # (B, 1, L)
  nopeak_mask = torch.ones([1, L, L], dtype=torch.bool)  # (1, L, L)
  nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)
  mask = padding_mask & nopeak_mask  # (B, L, L)
  ```

* Masked Multi-head Attention
  ```python
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, H, L, L)
  inf = 1e12
  masks = mask.unsqueeze(1)  # (B, 1, L, L)
  masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, H, L, L)
  attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, H, L, L)
  attn_values = torch.matmul(attn_dists, v)  # (B, H, L, d_k)
  ```

* Encoder-Decoder Attention
  * src_emb : encoderì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ (S_L : source maximum sequence length)
  * trg_emb : masked multi-head attention í›„ ê²°ê³¼ (T_L : target maximum sequence length)
  
  ```python
  src_emb # (B, S_L, d_model)
  trg_emb # (B, T_L, d_model)
  q = w_q(trg_emb)  # (B, T_L, d_model)
  k = w_k(src_emb)  # (B, S_L, d_model)
  v = w_v(src_emb)  # (B, S_L, d_model)
  ```

### Transformers Library 1 - BERT base

* Huggingfaceì—ì„œ ì œê³µí•˜ëŠ” Transformers
  * [Docs](https://huggingface.co/transformers/index.html) \| [GitHub](https://github.com/huggingface/transformers) \| [Models](https://huggingface.co/models)

* ì„¤ì¹˜ ë° ì„í¬íŠ¸
  ```python
  # !pip install transformers
  from transformers import BertConfig, BertTokenizer, BertModel
  ```

* Pre-trainëœ BERT ë¶ˆëŸ¬ì˜¤ê¸°
  ```python
  bert_name = 'bert-base-uncased'
  config = BertConfig.from_pretrained(bert_name)
  tokenizer = BertTokenizer.from_pretrained(bert_name)
  model = BertModel.from_pretrained(bert_name)
  ```

* Tokenizer ì‚¬ìš©
  ```python
  vocab = tokenizer.get_vocab()
  # encode ë°©ë²• : ìë™ìœ¼ë¡œ [CLS], [SEP]ë„ ì¶”ê°€
  token_ids = tokenizer.encode(sentence) # í˜¹ì€ tokenizer(sentence)
  # decode ë°©ë²•
  tokens = tokenizer.convert_ids_to_tokens(token_ids)
  sentence = tokenizer.convert_tokens_to_string(tokens)
  ```

* ë°ì´í„° ì „ì²˜ë¦¬
  1. encode : `tokenizer.encode`
  2. padding : `pad_id = tokenizer._convert_token_to_id('[PAD]')`
  3. attention mask : `batch_mask =Â (batch != pad_id).float()`

* BERT ì‚¬ìš©
  * d_h : hidden size (ìœ„ì˜ d_modelê³¼ ê°™ì€ ì—­í• )
  * last_hidden_state : ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ ì¶œë ¥ì—ì„œ hidden states ì‹œí€€ìŠ¤
  * pooler_output : NSP ë ˆì´ì–´ì˜ hidden state
  
  ```python
  batch # (B, L)
  outputs = model(input_ids=batch, attention_mask=batch_mask)
  last_hidden_states = outputs[0]  # (B, L, d_h)
  pooler_output = outputs[1] # (B, d_h)
  ```

* Sentence-level classification : `[CLS]` í† í° ì´ìš©
  ```python
  num_classes = 10
  sent_linear = nn.Linear(config.hidden_size, num_classes)
  cls_output = last_hidden_states[:, 0, :] # (B, d_h)
  sent_output = sent_linear(cls_output) # (B, num_classes)
  ```

* Token-level classification : ì „ì²´ sequenceì˜ hidden state í™œìš©
  ```python
  num_classes = 50
  token_linear = nn.Linear(config.hidden_size, num_classes) # config.hidden_size = d_h
  token_output = token_linear(last_hidden_states) # (B, L, num_classes)
  ```

* [ë‹¤ì–‘í•œ headë¥¼ ì¶”ê°€í•œ BERT](https://huggingface.co/transformers/model_doc/bert.html)
  * [`BertForMaskedLM`](https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm), [`BertForSequenceClassification`](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification), ... ë“±

### Transformers Library 2 - GPT-2

* Pre-trainëœ GPT-2 ë¶ˆëŸ¬ì˜¤ê¸°
  ```python
  from transformers import GPT2Config, GPT2Tokenizer, GPT2Model
  gpt_name = 'gpt2'
  config = GPT2Config.from_pretrained(gpt_name)
  tokenizer = GPT2Tokenizer.from_pretrained(gpt_name)
  model = GPT2Model.from_pretrained(gpt_name)
  ```

* GPT-2 ì‚¬ìš©
  ```python
  batch # (B, L)
  outputs = model(input_ids=batch, attention_mask=batch_mask)
  last_hidden_states = outputs[0]  # (B, L, d_h)
  # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡
  lm_linear = nn.Linear(config.hidden_size, config.vocab_size)
  lm_output = lm_linear(last_hidden_states)  # (B, L, V)
  ```

* [ë‹¤ì–‘í•œ headë¥¼ ì¶”ê°€í•œ GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
  * [`GPT2LMHeadModel`](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel) : `input_ids`ì™€ `labels`ë¥¼ í•¨ê»˜ ì¤„ ê²½ìš° ìë™ìœ¼ë¡œ cross entropy lossê¹Œì§€ ê³„ì‚°, `labels`ë¥¼ ì£¼ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ê³¼ ë™ì¼í•œ ê²°ê³¼
    ```python
    outputs = lm_model(input_ids=batch, attention_mask=batch_mask, labels=batch)
    loss = outputs[0] # scalar
    logits = outputs[1] # (B, L, V)
    ```

* Special token ì¶”ê°€í•˜ê¸°
  ```python
  # Special token ì¶”ê°€
  special_tokens = {
      'bos_token': '[BOS]',
      'eos_token': '[EOS]',
      'pad_token': '[PAD]',
      'additional_special_tokens': ['[SP1]', '[SP2]']
  }
  num_new_tokens = tokenizer.add_special_tokens(special_tokens)
  # ëª¨ë¸ì˜ embedding layerì˜ input size ë°”ê¿”ì£¼ê¸°
  vocab = tokenizer.get_vocab()
  model.resize_token_embeddings(len(vocab))
  ```

## ğŸ” ê³¼ì œ ìˆ˜í–‰ ê³¼ì •

### [ì„ íƒ ê³¼ì œ 1] BERT Fine-tuning

* ì£¼ì œ : imdb ì˜í™” ë¦¬ë·° ë°ì´í„°ì— ëŒ€í•´ pretrain ëª¨ë¸ì„ finetuningí•˜ëŠ” ê³¼ì œ
* ëª©í‘œ : ëª¨ë¸, íŒŒë¼ë¯¸í„°, ë“±ë“± ì—¬ëŸ¬ê°€ì§€ ê°’ë“¤ì„ ë°”ê¾¸ì–´ì„œ finetuningí•˜ì—¬, Test Accuracy 92% ì´ìƒì„ ë„˜ê¸°ê¸°
* ì½”ë“œ ë¶„ì„
  * ë°ì´í„°ì…‹ ë§Œë“¤ê¸°
    ```python
    class IMDbDataset(torch.utils.data.Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = labels
        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['labels'] = torch.tensor(self.labels[idx])
            # (ex) item = {'input_ids': [101, 2004, ...], 'attention_mask': [1, 1, ...], 'labels': 1}
            return item
        def __len__(self):
            return len(self.labels)
    
    # encoding
    from transformers import DistilBertTokenizerFast
    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
    train_encodings = tokenizer(train_texts, truncation=True, padding=True)
    val_encodings = tokenizer(val_texts, truncation=True, padding=True)
    test_encodings = tokenizer(test_texts, truncation=True, padding=True)
    # dataset
    train_dataset = IMDbDataset(train_encodings, train_labels)
    val_dataset = IMDbDataset(val_encodings, val_labels)
    test_dataset = IMDbDataset(test_encodings, test_labels)
    ```

  * í•™ìŠµ
    ```python
    from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments
    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        num_train_epochs=1,              # total number of training epochs
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=64,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=100,
    )
    model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)
    trainer = Trainer(
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=val_dataset             # evaluation dataset
    )
    trainer.train()
    ```

  * í…ŒìŠ¤íŠ¸
    ```python
    from datasets import load_metric
    from torch.utils.data import DataLoader
    metric= load_metric("accuracy")
    test_dataloader = DataLoader(test_dataset, batch_size=128)
    model.eval()
    for batch in tqdm(test_dataloader):
        batch = {k: v.to("cuda") for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])
    metric.compute()
    ```

* ì°¸ê³  ìë£Œ
  * [huggingface transformers](https://huggingface.co/transformers/)
  * [Text Classification on IMDb](https://paperswithcode.com/sota/text-classification-on-imdb)

### [ì„ íƒ ê³¼ì œ 2] NMT training with Fairseq Assignment

* ì£¼ì œ : pytorchë¥¼ ê°œë°œí•˜ê³  ìˆëŠ” facebookì—ì„œ ì‘ì—… ì¤‘ì¸ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì¸ Fairseqì„ ì´ìš©í•´ ë²ˆì—­ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸°
  * [Docs](https://fairseq.readthedocs.io/en/latest/) \| [GitHub](https://github.com/pytorch/fairseq) \| [Command-line Tools](https://fairseq.readthedocs.io/en/latest/command_line_tools.html)
* ëª©í‘œ : BLEU score 25 ì´ìƒ ë‹¬ì„±í•´ë³´ê¸°
* ì„¤ì¹˜ : `pip install fastBPE sacremoses subword_nmt hydra-core omegaconf fairseq`
* ì „ì²˜ë¦¬
  ````bash
  fairseq-preprocess \
  		--source-lang de \
  		--target-lang en \
  		--trainpref ./iwslt14.tokenized.de-en/train \
  		--validpref ./iwslt14.tokenized.de-en/valid \
  		--testpref ./iwslt14.tokenized.de-en/test \
  		--destdir ./iwslt14.tokenized.de-en/
  ````
* ëª¨ë¸ í•™ìŠµ
  ```bash
  fairseq-train ./iwslt14.tokenized.de-en/ \
  		--arch transformer_iwslt_de_en \
  		--optimizer adam \
  		--clip-norm 0.0 \
  		--lr 5e-4 \
  		--lr-scheduler inverse_sqrt \
  		--criterion label_smoothed_cross_entropy \
  		--max-tokens 4096 \
  		--max-epoch 15
  ```
* ì˜ˆì¸¡ ë¬¸ì¥ ìƒì„± ë° í‰ê°€
  ```bash
  fairseq-generate ./iwslt14.tokenized.de-en \
  		--path ./checkpoints/checkpoint_best.pt \
  		--beam 5 \
  		--remove-bpe
  ```

### [ì„ íƒ ê³¼ì œ 3] Byte Pair Encoding Assignment

* ì°¸ê³  ë…¼ë¬¸ : [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)

  ![image](https://user-images.githubusercontent.com/35680202/133384471-7e66acc8-f220-467b-a1b2-d52404102ec2.png)

## ğŸ“– ë…¼ë¬¸ ìŠ¤í„°ë””

### ELMO

* ì£¼ì œ : Deep Contextualized Word Representations
* íŠ¹ì§•
  * ë‹¨ì–´ì˜ ë³µì¡í•œ íŠ¹ì§• ëª¨ë¸ë§ (syntax, semantics)
  * (ë™ìŒì´ì˜ì–´ ë“±) ë‹¤ì–‘í•œ ì–¸ì–´ì  ë§¥ë½ ìƒì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ í•™ìŠµ (polysemy)
* Related Work
  * ê¸°ì¡´ : context-independent / ELMo : **context-dependent** => ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ
  * **subword** & multi-sense information : ê¸°ì¡´ ì—°êµ¬ë“¤ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ì•˜ì–´ì„œ ELMoë„ ì‚¬ìš©í•œë‹¤. (ELMoê°€ ì—¬ëŸ¬ ì—°êµ¬ë“¤ì˜ ê²°ê³¼ë¥¼ í•œ ê³³ì— ëª¨ì•„ì£¼ëŠ” ìœ„ì¹˜ì— ìˆìŒ)
  * **Deep contextual representation** : pre-ELMoëŠ” labeled ë°ì´í„°ë¡œ í•™ìŠµ / ELMoëŠ” unlabeled ë°ì´í„°ë¡œ í•™ìŠµ ê°€ëŠ¥!
* êµ¬ì¡°
  * Char-CNN : n-gramì„ ëª¨ë¸ë§í•˜ëŠ” ì—­í•  (ì•„ì§ê¹Œì§„ context-independent : ë¬¸ë²•ì  í‘œí˜„ í•™ìŠµ)
  * biLM : forward LM + backward LM (context-dependent! : ì˜ë¯¸ë¡ ì  í‘œí˜„ í•™ìŠµ)
    * êµ¬í˜„ : `nn.LSTM(E, H, num_layers=2, bidirectioanl=True)` (ì •ë§ ê°„ë‹¨)
  * Feature-based pre-training (not fine-tuning)
    * pre-trainedëœ ë¶€ë¶„ì„ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤.
* ì°¸ê³  ìë£Œ
  * [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)

### GPT-1

* ì£¼ì œ : Generative Pre-Training
* Introduction
  * Unlabeled Data ì‚¬ìš©
  * ë¬¸ì œ : ì–´ë–»ê²Œ í•™ìŠµ? + ì–´ë–»ê²Œ ì „ì´?
  * í•´ê²° : task-specific input adaptation
* Stage1 : Unsupervised pre-training
  * Transformer decoder ë¡œ ëª¨ë¸ë§ (Cross-Attention ì œê±°í•˜ê³  ì‚¬ìš©)
* Stage2 : Supervised fine-tuning
  * Task-specific input transformations
* Final Loss : Sum Over
  * LM Loss ì‚¬ìš©

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* ì„ íƒê³¼ì œ2
  * architecture ë³€ê²½ -> [lightconv](https://github.com/pytorch/fairseq/blob/master/fairseq/models/lightconv.py)
  * cliping norm
* ìë£Œ ê³µìœ 
  * [huggingface tutorial](https://huggingface.co/course/chapter1)
    * ì§ì ‘ pretrain í•´ë³´ê³  ì‹¶ë‹¤ë©´ : `transformers.BertForPreTraining`
  * [KcBERT](https://github.com/Beomi/KcBERT) & [KcBERT Pre-Training Corpus (Korean News Comments)](https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments)

## ğŸš€ í•™ìŠµ íšŒê³ 

* ì˜¤ëŠ˜ì€ ì½”ì–´íƒ€ì„ ëë‚˜ê³  (ì •ë§ ì˜¤ëœë§Œì—) ê³µì› ì‚°ì±…ì„ ë‹¤ë…€ì™”ëŠ”ë° ê¸°ë¶„ì „í™˜ì´ ë˜ì–´ì„œ ì¢‹ì•˜ë‹¤.
* ì•ìœ¼ë¡œ ìš´ë™ê¹Œì§€ëŠ” ì•„ë‹ˆë”ë¼ë„ ë§¤ì¼ ì‚°ì±…ì„ í•´ë„ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.
* ì¦ê²ê²Œ ê±´ê°•í•˜ê²Œ 12ì›”ê¹Œì§€ ë‹¬ë ¤ë³´ì!!!ğŸ¤¸â€â™€ï¸
