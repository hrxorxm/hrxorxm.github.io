---
layout: post
title:  "[Boostcamp AI Tech] 32일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day32"
categories: "Boostcamp-AI-Tech"
tags: [7주차, Level2-U-Stage, NLP]
use_math: true
---

# 부스트캠프 32일차

## 📝 오늘 일정 정리

* 9/15(수)
  - [x] NLP 코드 실습
    - [x] [실습] Multi-head Attention
    - [x] [실습] Masked Multi-head Attention
    - [x] [실습] Transformers Library 1 - BERT base
    - [x] [실습] Transformers Library 2 - GPT-2
  - [ ] NLP 이론 강의 선택 과제
    - [x] [선택 과제 1] BERT Fine-tuning
    - [x] [선택 과제 2] NMT training with Fairseq Assignment
    - [ ] [선택 과제 3] Byte Pair Encoding Assignment
  - [x] 논문 스터디(ELMO, GPT-1) 13:00~14:00

## 🔎 실습 내용 정리

### Multi-head Attention

* Batch data
  * B : batch size = 10
  * L : maximum sequence length = 20
  
  ```python
  batch_data # (B, L)
  ```

* Embedding
  * V : vocab size = 100
  * d_model : model hidden size = 512
  
  ```python
  embedding = nn.Embedding(V, d_model)
  batch_emb = embedding(batch_data) # (B, L, d_model)
  ```

* Calculate Attention
  * H : head 개수 = 8
  * d_k = d_model // num_heads = 64
  
  ```python
  # linear transformation matrixs
  w_q = nn.Linear(d_model, d_model)
  w_k = nn.Linear(d_model, d_model)
  w_v = nn.Linear(d_model, d_model)
  # (B, L, d_model) -> (B, L, d_model) -> (B, L, H, d_k) -> (B, H, L, d_k)
  q = w_q(batch_emb).view(B, -1, H, d_k).transpose(1, 2)
  k = w_k(batch_emb).view(B, -1, H, d_k).transpose(1, 2)
  v = w_v(batch_emb).view(B, -1, H, d_k).transpose(1, 2)
  ```

* Scaled Dot-Product Self-Attention
  ```python
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, H, L, d_k) x (B, H, d_k, L) = (B, H, L, L)
  attn_dists = F.softmax(attn_scores, dim=-1)  # (B, H, L, L)
  attn_values = torch.matmul(attn_dists, v)  # (B, H, L, L) x (B, H, L, d_k) = (B, H, L, d_k)
  ```

* Concat & Linear transformation
  * [``contiguous()``](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html) : 새로운 메모리 공간에 데이터 복사
  
  ```python
  attn_values = attn_values.transpose(1, 2)  # (B, L, H, d_k)
  attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)
  w_0 = nn.Linear(d_model, d_model)
  outputs = w_0(attn_values) # (B, L, d_model)
  ```

### Masked Multi-head Attention

* Mask
  * padding_mask : padding 된 부분을 False 처리
  * nopeak_mask : 치팅하면 안되는 부분을 False 처리
    * [`torch.tril`](https://pytorch.org/docs/stable/generated/torch.tril.html) : 아래 삼각형 반환 (나머지 요소는 0)
  * mask : 위 둘을 bitwise and 연산
  
  ```python
  padding_mask = (batch_data != pad_id).unsqueeze(1)  # (B, 1, L)
  nopeak_mask = torch.ones([1, L, L], dtype=torch.bool)  # (1, L, L)
  nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)
  mask = padding_mask & nopeak_mask  # (B, L, L)
  ```

* Masked Multi-head Attention
  ```python
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, H, L, L)
  inf = 1e12
  masks = mask.unsqueeze(1)  # (B, 1, L, L)
  masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, H, L, L)
  attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, H, L, L)
  attn_values = torch.matmul(attn_dists, v)  # (B, H, L, d_k)
  ```

* Encoder-Decoder Attention
  * src_emb : encoder에서 나온 결과 (S_L : source maximum sequence length)
  * trg_emb : masked multi-head attention 후 결과 (T_L : target maximum sequence length)
  
  ```python
  src_emb # (B, S_L, d_model)
  trg_emb # (B, T_L, d_model)
  q = w_q(trg_emb)  # (B, T_L, d_model)
  k = w_k(src_emb)  # (B, S_L, d_model)
  v = w_v(src_emb)  # (B, S_L, d_model)
  ```

### Transformers Library 1 - BERT base

* Huggingface에서 제공하는 Transformers
  * [Docs](https://huggingface.co/transformers/index.html) \| [GitHub](https://github.com/huggingface/transformers) \| [Models](https://huggingface.co/models)

* 설치 및 임포트
  ```python
  # !pip install transformers
  from transformers import BertConfig, BertTokenizer, BertModel
  ```

* Pre-train된 BERT 불러오기
  ```python
  bert_name = 'bert-base-uncased'
  config = BertConfig.from_pretrained(bert_name)
  tokenizer = BertTokenizer.from_pretrained(bert_name)
  model = BertModel.from_pretrained(bert_name)
  ```

* Tokenizer 사용
  ```python
  vocab = tokenizer.get_vocab()
  # encode 방법 : 자동으로 [CLS], [SEP]도 추가
  token_ids = tokenizer.encode(sentence) # 혹은 tokenizer(sentence)
  # decode 방법
  tokens = tokenizer.convert_ids_to_tokens(token_ids)
  sentence = tokenizer.convert_tokens_to_string(tokens)
  ```

* 데이터 전처리
  1. encode : `tokenizer.encode`
  2. padding : `pad_id = tokenizer._convert_token_to_id('[PAD]')`
  3. attention mask : `batch_mask = (batch != pad_id).float()`

* BERT 사용
  * d_h : hidden size (위의 d_model과 같은 역할)
  * last_hidden_state : 모델의 마지막 레이어 출력에서 hidden states 시퀀스
  * pooler_output : NSP 레이어의 hidden state
  
  ```python
  batch # (B, L)
  outputs = model(input_ids=batch, attention_mask=batch_mask)
  last_hidden_states = outputs[0]  # (B, L, d_h)
  pooler_output = outputs[1] # (B, d_h)
  ```

* Sentence-level classification : `[CLS]` 토큰 이용
  ```python
  num_classes = 10
  sent_linear = nn.Linear(config.hidden_size, num_classes)
  cls_output = last_hidden_states[:, 0, :] # (B, d_h)
  sent_output = sent_linear(cls_output) # (B, num_classes)
  ```

* Token-level classification : 전체 sequence의 hidden state 활용
  ```python
  num_classes = 50
  token_linear = nn.Linear(config.hidden_size, num_classes) # config.hidden_size = d_h
  token_output = token_linear(last_hidden_states) # (B, L, num_classes)
  ```

* [다양한 head를 추가한 BERT](https://huggingface.co/transformers/model_doc/bert.html)
  * [`BertForMaskedLM`](https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm), [`BertForSequenceClassification`](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification), ... 등

### Transformers Library 2 - GPT-2

* Pre-train된 GPT-2 불러오기
  ```python
  from transformers import GPT2Config, GPT2Tokenizer, GPT2Model
  gpt_name = 'gpt2'
  config = GPT2Config.from_pretrained(gpt_name)
  tokenizer = GPT2Tokenizer.from_pretrained(gpt_name)
  model = GPT2Model.from_pretrained(gpt_name)
  ```

* GPT-2 사용
  ```python
  batch # (B, L)
  outputs = model(input_ids=batch, attention_mask=batch_mask)
  last_hidden_states = outputs[0]  # (B, L, d_h)
  # 다음 단어 예측
  lm_linear = nn.Linear(config.hidden_size, config.vocab_size)
  lm_output = lm_linear(last_hidden_states)  # (B, L, V)
  ```

* [다양한 head를 추가한 GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
  * [`GPT2LMHeadModel`](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel) : `input_ids`와 `labels`를 함께 줄 경우 자동으로 cross entropy loss까지 계산, `labels`를 주지 않으면 기존과 동일한 결과
    ```python
    outputs = lm_model(input_ids=batch, attention_mask=batch_mask, labels=batch)
    loss = outputs[0] # scalar
    logits = outputs[1] # (B, L, V)
    ```

* Special token 추가하기
  ```python
  # Special token 추가
  special_tokens = {
      'bos_token': '[BOS]',
      'eos_token': '[EOS]',
      'pad_token': '[PAD]',
      'additional_special_tokens': ['[SP1]', '[SP2]']
  }
  num_new_tokens = tokenizer.add_special_tokens(special_tokens)
  # 모델의 embedding layer의 input size 바꿔주기
  vocab = tokenizer.get_vocab()
  model.resize_token_embeddings(len(vocab))
  ```

## 🔎 과제 수행 과정

### [선택 과제 1] BERT Fine-tuning

* 주제 : imdb 영화 리뷰 데이터에 대해 pretrain 모델을 finetuning하는 과제
* 목표 : 모델, 파라미터, 등등 여러가지 값들을 바꾸어서 finetuning하여, Test Accuracy 92% 이상을 넘기기
* 코드 분석
  * 데이터셋 만들기
    ```python
    class IMDbDataset(torch.utils.data.Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = labels
        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['labels'] = torch.tensor(self.labels[idx])
            # (ex) item = {'input_ids': [101, 2004, ...], 'attention_mask': [1, 1, ...], 'labels': 1}
            return item
        def __len__(self):
            return len(self.labels)
    
    # encoding
    from transformers import DistilBertTokenizerFast
    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
    train_encodings = tokenizer(train_texts, truncation=True, padding=True)
    val_encodings = tokenizer(val_texts, truncation=True, padding=True)
    test_encodings = tokenizer(test_texts, truncation=True, padding=True)
    # dataset
    train_dataset = IMDbDataset(train_encodings, train_labels)
    val_dataset = IMDbDataset(val_encodings, val_labels)
    test_dataset = IMDbDataset(test_encodings, test_labels)
    ```

  * 학습
    ```python
    from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments
    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        num_train_epochs=1,              # total number of training epochs
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=64,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=100,
    )
    model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",config=config)
    trainer = Trainer(
        model=model,                         # the instantiated 🤗 Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=val_dataset             # evaluation dataset
    )
    trainer.train()
    ```

  * 테스트
    ```python
    from datasets import load_metric
    from torch.utils.data import DataLoader
    metric= load_metric("accuracy")
    test_dataloader = DataLoader(test_dataset, batch_size=128)
    model.eval()
    for batch in tqdm(test_dataloader):
        batch = {k: v.to("cuda") for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])
    metric.compute()
    ```

* 참고 자료
  * [huggingface transformers](https://huggingface.co/transformers/)
  * [Text Classification on IMDb](https://paperswithcode.com/sota/text-classification-on-imdb)

### [선택 과제 2] NMT training with Fairseq Assignment

* 주제 : pytorch를 개발하고 있는 facebook에서 작업 중인 오픈소스 프로젝트인 Fairseq을 이용해 번역 모델을 학습하기
  * [Docs](https://fairseq.readthedocs.io/en/latest/) \| [GitHub](https://github.com/pytorch/fairseq) \| [Command-line Tools](https://fairseq.readthedocs.io/en/latest/command_line_tools.html)
* 목표 : BLEU score 25 이상 달성해보기
* 설치 : `pip install fastBPE sacremoses subword_nmt hydra-core omegaconf fairseq`
* 전처리
  ````bash
  fairseq-preprocess \
  		--source-lang de \
  		--target-lang en \
  		--trainpref ./iwslt14.tokenized.de-en/train \
  		--validpref ./iwslt14.tokenized.de-en/valid \
  		--testpref ./iwslt14.tokenized.de-en/test \
  		--destdir ./iwslt14.tokenized.de-en/
  ````
* 모델 학습
  ```bash
  fairseq-train ./iwslt14.tokenized.de-en/ \
  		--arch transformer_iwslt_de_en \
  		--optimizer adam \
  		--clip-norm 0.0 \
  		--lr 5e-4 \
  		--lr-scheduler inverse_sqrt \
  		--criterion label_smoothed_cross_entropy \
  		--max-tokens 4096 \
  		--max-epoch 15
  ```
* 예측 문장 생성 및 평가
  ```bash
  fairseq-generate ./iwslt14.tokenized.de-en \
  		--path ./checkpoints/checkpoint_best.pt \
  		--beam 5 \
  		--remove-bpe
  ```

### [선택 과제 3] Byte Pair Encoding Assignment

* 참고 논문 : [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)

  ![image](https://user-images.githubusercontent.com/35680202/133384471-7e66acc8-f220-467b-a1b2-d52404102ec2.png)

## 📖 논문 스터디

### ELMO

* 주제 : Deep Contextualized Word Representations
* 특징
  * 단어의 복잡한 특징 모델링 (syntax, semantics)
  * (동음이의어 등) 다양한 언어적 맥락 상에서 어떻게 사용되는지 학습 (polysemy)
* Related Work
  * 기존 : context-independent / ELMo : **context-dependent** => 문맥에 따라 다양한 표현 학습
  * **subword** & multi-sense information : 기존 연구들에서 성능이 좋았어서 ELMo도 사용한다. (ELMo가 여러 연구들의 결과를 한 곳에 모아주는 위치에 있음)
  * **Deep contextual representation** : pre-ELMo는 labeled 데이터로 학습 / ELMo는 unlabeled 데이터로 학습 가능!
* 구조
  * Char-CNN : n-gram을 모델링하는 역할 (아직까진 context-independent : 문법적 표현 학습)
  * biLM : forward LM + backward LM (context-dependent! : 의미론적 표현 학습)
    * 구현 : `nn.LSTM(E, H, num_layers=2, bidirectioanl=True)` (정말 간단)
  * Feature-based pre-training (not fine-tuning)
    * pre-trained된 부분을 건드리지 않는다.
* 참고 자료
  * [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)

### GPT-1

* 주제 : Generative Pre-Training
* Introduction
  * Unlabeled Data 사용
  * 문제 : 어떻게 학습? + 어떻게 전이?
  * 해결 : task-specific input adaptation
* Stage1 : Unsupervised pre-training
  * Transformer decoder 로 모델링 (Cross-Attention 제거하고 사용)
* Stage2 : Supervised fine-tuning
  * Task-specific input transformations
* Final Loss : Sum Over
  * LM Loss 사용

## 🌱 피어 세션 정리

* 선택과제2
  * architecture 변경 -> [lightconv](https://github.com/pytorch/fairseq/blob/master/fairseq/models/lightconv.py)
  * cliping norm
* 자료 공유
  * [huggingface tutorial](https://huggingface.co/course/chapter1)
    * 직접 pretrain 해보고 싶다면 : `transformers.BertForPreTraining`
  * [KcBERT](https://github.com/Beomi/KcBERT) & [KcBERT Pre-Training Corpus (Korean News Comments)](https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments)

## 🚀 학습 회고

* 오늘은 코어타임 끝나고 (정말 오랜만에) 공원 산책을 다녀왔는데 기분전환이 되어서 좋았다.
* 앞으로 운동까지는 아니더라도 매일 산책을 해도 좋겠다는 생각이 들었다.
* 즐겁게 건강하게 12월까지 달려보자!!!🤸‍♀️
