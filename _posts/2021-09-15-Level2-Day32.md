---
layout: post
title:  "[Boostcamp AI Tech] 32ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day32"
categories: "Boostcamp-AI-Tech"
tags: [7ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 32ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/15(ìˆ˜)
  - [x] ë…¼ë¬¸ ìŠ¤í„°ë””(ELMO, GPT-1) 13:00~14:00

## ğŸ“– ë…¼ë¬¸ ìŠ¤í„°ë””

### ELMO

* ì£¼ì œ : Deep Contextualized Word Representations
* íŠ¹ì§•
  * ë‹¨ì–´ì˜ ë³µì¡í•œ íŠ¹ì§• ëª¨ë¸ë§ (syntax, semantics)
  * (ë™ìŒì´ì˜ì–´ ë“±) ë‹¤ì–‘í•œ ì–¸ì–´ì  ë§¥ë½ ìƒì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ í•™ìŠµ (polysemy)
* Related Work
  * ê¸°ì¡´ : context-independent / ELMo : **context-dependent** => ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ
  * **subword** & multi-sense information : ê¸°ì¡´ ì—°êµ¬ë“¤ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ì•˜ì–´ì„œ ELMoë„ ì‚¬ìš©í•œë‹¤. (ELMoê°€ ì—¬ëŸ¬ ì—°êµ¬ë“¤ì˜ ê²°ê³¼ë¥¼ í•œ ê³³ì— ëª¨ì•„ì£¼ëŠ” ìœ„ì¹˜ì— ìˆìŒ)
  * **Deep contextual representation** : pre-ELMoëŠ” labeled ë°ì´í„°ë¡œ í•™ìŠµ / ELMoëŠ” unlabeled ë°ì´í„°ë¡œ í•™ìŠµ ê°€ëŠ¥!
* êµ¬ì¡°
  * Char-CNN : n-gramì„ ëª¨ë¸ë§í•˜ëŠ” ì—­í•  (ì•„ì§ê¹Œì§„ context-independent : ë¬¸ë²•ì  í‘œí˜„ í•™ìŠµ)
  * biLM : forward LM + backward LM (context-dependent! : ì˜ë¯¸ë¡ ì  í‘œí˜„ í•™ìŠµ)
    * êµ¬í˜„ : `nn.LSTM(E, H, num_layers=2, bidirectioanl=True)` (ì •ë§ ê°„ë‹¨)
  * Feature-based pre-training (not fine-tuning)
    * pre-trainedëœ ë¶€ë¶„ì„ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤.
* ì°¸ê³  ìë£Œ
  * [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)

### GPT-1

* ì£¼ì œ : Generative Pre-Training
* Introduction
  * Unlabeled Data ì‚¬ìš©
  * ë¬¸ì œ : ì–´ë–»ê²Œ í•™ìŠµ? + ì–´ë–»ê²Œ ì „ì´?
  * í•´ê²° : task-specific input adaptation
* Stage1 : Unsupervised pre-training
  * Transformer decoder ë¡œ ëª¨ë¸ë§ (Cross-Attention ì œê±°í•˜ê³  ì‚¬ìš©)
* Stage2 : Supervised fine-tuning
  * Task-specific input transformations
* Final Loss : Sum Over
  * LM Loss ì‚¬ìš©

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* ì„ íƒê³¼ì œ2
  * architecture ë³€ê²½ -> [lightconv](https://github.com/pytorch/fairseq/blob/master/fairseq/models/lightconv.py)
  * cliping norm
* ìë£Œ ê³µìœ 
  * [huggingface tutorial](https://huggingface.co/course/chapter1)
    * ì§ì ‘ pretrain í•´ë³´ê³  ì‹¶ë‹¤ë©´ : `transformers.BertForPreTraining`
  * [KcBERT](https://github.com/Beomi/KcBERT) & [KcBERT Pre-Training Corpus (Korean News Comments)](https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments)

## ğŸš€ í•™ìŠµ íšŒê³ 

* ì˜¤ëŠ˜ì€ ì½”ì–´íƒ€ì„ ëë‚˜ê³  (ì •ë§ ì˜¤ëœë§Œì—) ê³µì› ì‚°ì±…ì„ ë‹¤ë…€ì™”ëŠ”ë° ê¸°ë¶„ì „í™˜ì´ ë˜ì–´ì„œ ì¢‹ì•˜ë‹¤.
* ì•ìœ¼ë¡œ ìš´ë™ê¹Œì§€ëŠ” ì•„ë‹ˆë”ë¼ë„ ë§¤ì¼ ì‚°ì±…ì„ í•´ë„ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.
* ì¦ê²ê²Œ ê±´ê°•í•˜ê²Œ 12ì›”ê¹Œì§€ ë‹¬ë ¤ë³´ì!!!ğŸ¤¸â€â™€ï¸
