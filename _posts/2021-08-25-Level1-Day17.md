---
layout: post
title:  "[Boostcamp AI Tech] 17일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day17"
categories: "Boostcamp-AI-Tech"
tags: [4주차]
use_math: true
---

# 부스트캠프 17일차

## 📝 오늘 일정 정리

* 8/25 (수)
  - [x] 이미지 분류 강의
    - [x] (5강) Model 1
    - [x] (6강) Model 2
  - [x] 스페셜 미션 : Model

## 📚 강의 내용 정리

### [5강] Model 1 - Model with Pytorch

* Pytorch : Low-level, Pythonic, Flexibility
* Pytorch 모델의 모든 레이어는 `nn.Module` 클래스를 따른다.
  * `model.modules()`
    * `__init__()` 에서 정의한 또 다른 `nn.Module`
    * 모델을 정의하는 순간 그 모델에 연결된 모든 module을 확인할 수 있다.
  * `forward()`
    * 이 모델이 호출되었을 때 실행되는 함수
    * 모든 `nn.Module`은 `forward()` 함수를 가지므로, 내가 정의한 모델의 `forward()` 실행으로 각각의 모듈의 `forward()`가 실행된다.
  * Parameters
    * 모델에 정의되어 있는 modules가 가지고 있는, 계산에 쓰일 Parameter
    * data, grad, requires_grad 변수 등을 가지고 있다.
* Pythonic : (ex) `model.state_dict()`는 Python 의 Dictionary이다.

### [6강] Model 2 - Pretrained Model

* ImageNet : 대용량의 데이터셋 덕분에 컴퓨터 비전이 발전할 수 있었다.
* Pretrained Model
  * 목적 : 좋은 품질, 대용량의 데이터로 미리 학습한 모델을 내 목적에 맞게 다듬어서 사용
  * `torchvision.models` / `timm`
* Transfer Learning
  * CNN base : Input + CNN Backbone + Classifier(fc) -> Output
  * Backbone 모델의 task와 현재 task의 유사성을 고려하여 학습한다.
    * High Similarity : CNN Backbone을 Freeze하고 Classifer만 학습시킨다. (Feature extraction 방식)
    * Low Similarity : 전체 네트워크를 학습시킨다. (Fine-tuning 방식)
* 추가 자료
  * [7 Tips To Maximize PyTorch Performance](https://towardsdatascience.com/7-tips-for-squeezing-maximum-performance-from-pytorch-ca4a40951259)
    * DataLoader
      * `num_workers = 4 * num_GPU`
      * `pin_memory = True` & `torch.cuda.empty_cache()`
    * `.cpu()` / `.item()` / `.numpy()` 는 성능이 매우 느리므로 GPU 상에서 계산 그래프에서만 분리하고 싶을 때는 `.detach()` 를 사용하자
    * `.cuda()` 하지 말고, tensor를 만들 때 device를 지정해주자
  * [Most Common Neural Net PyTorch Mistakes](https://medium.com/missinglink-deep-learning-platform/most-common-neural-net-pytorch-mistakes-456560ada037)
    * train / eval 모드 바꿔주는 것 주의하자
    * `backward()` 전에 `zero_grad()` 해주는 것 잊지 말자

## 🔎 미션 수행 과정

* ResNet 모델 구조 직접 구현해보기
* Pretrained Model
  * Fine-tuning 해보기
  * Feature Extraction 해보기

## 🌱 피어 세션 정리

* 마스크 착용 상태 분류 대회는 ImageNet task 하고 다른 것 같다.
  * Pretrained model을 Freeze 안 하고 돌리는게 나을 것 같다.
  * 백본 모델 자체를 우리 task랑 비슷한 task를 학습시킨 모델로 가져와보자 ex) 고양이 종류 분류
* [Albumentations 예시](https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb)
  ```python
  albumentations_transform = A.Compose([
      A.Resize(256, 256), 
      A.CenterCrop(224, 224),
      A.HorizontalFlip(),
      A.Normalize(
          mean=[0.485, 0.456, 0.406],
          std=[0.229, 0.224, 0.225],
      ),
      ToTensorV2()
  ])
  ```
  * [RandomAugmentation](https://arxiv.org/abs/1909.13719) : https://github.com/ildoonet/pytorch-randaugment/blob/master/RandAugment/data.py
* 데이터 불균형 해결 방안
  * [imbalanced-learn](https://imbalanced-learn.org/stable/)
  * [torchsampler](https://github.com/ufoym/imbalanced-dataset-sampler)

## 🚀 학습 회고

* 마스크, 성별, 나이 분류 모델이 각각 90%가 넘어도 레이블만 합치면 성능이 별로고, 레이블을 18개로 해놓고 loss를 구했던 모델이 그나마 성능이 괜찮았다.
* 서버에서 노트북파일 복사해서 올리면 자꾸 셀들이 지워지거나 순서가 바뀐채로 올라간다. 파이썬 템플릿으로 옮겨야겠다.
* efficientnet-b3으로 학습한 결과가 지금 팀 내 1등인데, inception-resnet-v2도 근소한 차이로 2등이다. 앙상블 모델이 아니면 더 이상 모델을 갈아끼워서는 성능을 높이기 어려울 것 같다.
* 오늘 한 일 : 여러 모델 시도, augmentation 간단하게 시도
* 내일 목표 : 템플릿으로 옮기기, 실험관리 툴 사용하기, 앙상블 시도하기

