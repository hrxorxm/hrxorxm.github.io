---
layout: post
title:  "[Boostcamp AI Tech] 1일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day1"
categories: "Boostcamp-AI-Tech"
tags: [1주차]
use_math: true
---

# 부스트캠프 1일차

## 체크인/체크아웃 루틴

* 체크인 : 오전 9시 30분~10시 / 체크아웃 : 오후 7시 이후
* 부스트코스 학습 시작 & 종료
* 부스트코스 본인 인증 -실명 정보 삭제 후 인증
* 슬랙 체크인 & 체크아웃

## 오늘 일정 정리

* 주제 : Python Basics, AI Math
* 8/2 (월)
  - [x] Python: 필수과제 1,2,3
  - [x] AI Math: 필수퀴즈 1~4강

## MeetUP & 피어 세션 정리

* 팀명 : 아29야(아직 2ㄴ공지능 초보지만 9래두 야 너두 할수 있어)
* 그라운드룰
  * 호칭 : ~님
  * 모더레이터 역할 : 회의록, 업로드, 회의 진행
* 협업툴 정리
  * 줌 : 회의
  * 깃허브 : 과제 코드 리뷰
  * 노션 : 회의록

## 강의 복습 내용

* 벡터(vector)
  * 성분곱(Hadamard product) : 같은 모양을 가지는 벡터끼리의 곱
  * 노름(norm) : 원점에서부터의 거리
    * $L_1$​-노름 : 각 성분의 변화량의 절대값을 모두 더함
      * 예 : Robust 학습, Lasso 회귀
    * $L_2$​​-노름 : 유클리드 거리 계산 `np.linalg.norm`
      * 예 : Laplace 근사, Ridge 회귀
    * 노름의 종류에 따라 기하학적 성질이 달라진다.
    * 두 벡터 사이의 거리 : 벡터의 뺄셈, 노름 이용
    * 두 벡터 사이의 각도 : 내적(inner product), $L_2$​-노름 이용
  * 내적(inner product) `np.inner`
    * $<x, y> = \parallel x \parallel_2 \parallel y \parallel_2 \cos \theta$​​ : 정사영(orthogonal projection)된 벡터 $Proj(x)$​의 길이를 $\parallel y \parallel$ 만큼 조정한 값
    * 두 벡터의 유사도(similarity)를 측정하는데 사용 가능
* 행렬(matrix) : 벡터를 원소로 가지는 2차원 배열
  * 전치행렬(transpose matrix) : 행과 열의 인덱스가 바뀐 행렬 `x.T`
  * 행렬의 덧셈, 뺄셈, 성분곱, 스칼라곱은 벡터와 차이가 없다.
  * 행렬 곱셈(matrix multiplication) : i번째 행벡터와 j번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 계산
    * `np.inner`는 i번째 행벡터와 j번째 행벡터 사이의 내적을 구함
  * 역행렬(inverse matrix) : 행과 열의 숫자가 같고 행렬식(determinant)이 0이 아닌 경우 구할 수 있다. `np.linalg.inv(x)`
    * $A A^{-1} = A^{-1} A = I$
  * 유사역행렬(pseudo-inverse) 또는 무어-펜로즈(Moore-Penrose) 역행렬 `np.linalg.pinv(x)`
    * $n \geq m$​ 인 경우 $A^{+} = (A^{T} A)^{-1} A^{T}$, $A^{+}A = I$​
    * $n \leq m$ 인 경우 $A^{+} = A^{T} (A A^{T})^{-1}$, $AA^{+} = I$
  * 연립방정식 풀기 ($n \leq m$ 인 경우)
    * $Ax = b \Rightarrow x = A^{+}b = A^{T} (AA^{T})^{-1}b$​
    * 무어-펜로즈 역행렬을 이용하면 해를 하나 구할 수 있다.
  * 선형회귀분석 ($n \geq m$ 인 경우)
    * $X \beta = \hat{y} \approx y \Rightarrow \beta = X^{+}y = (X^{T}X)^{-1}X^{T}y$​​​   ($\underset{\beta}{min} \parallel y - \hat{y} \parallel_2$, 즉, $L_2$​-노름을 최소화)
    * 선형회귀분석은 연립방정식과 달리 행이 더 크므로 방정식을 푸는건 불가능하고, $y$에 근접하는 $\hat{y}$​​를 찾을 수 있다. (+추가 : y절편(intercept) 항을 직접 추가해야 한다.)
* 경사하강법
  * 용어
    * 미분(differentiation) : 변화율의 극한(limit) `sympy.diff`
    * 경사상승법(gradient ascent) : 미분값을 더함, 함수의 극댓값의 위치를 구할 때 사용
    * 경사하강법(gradient descent) : 미분값을 뺌, 함수의 극소값의 위치를 구할 때 사용
    * 편미분(partial differentiation) : 벡터가 입력인 다변수 함수일 때의 미분
    * 그레디언트(gradient) 벡터 : $\nabla f = (\partial_{x_1}f, \partial_{x_2}f, ..., \partial_{x_d}f)$
  * 목표 : 선형회귀의 목적식을 최소화하는 $\beta$를 찾아야 한다.
    * 목적식이 $\parallel y - X \beta \parallel_2$​ 일 때
      * 그레디언트 벡터 식
        * $\nabla_{\beta}\parallel y - X \beta \parallel_2 = (\partial_{\beta_1} \parallel y - X \beta \parallel_2, ..., \partial_{\beta_d} \parallel y - X \beta \parallel_2)$​ 이고,
        * $\partial_{\beta_k} \parallel y - X \beta \parallel_2 = \partial_{\beta_k}  \{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \sum_{j=1}^{d} X_{ij} \beta_{j})^2 \}^{1/2} = - \frac{X^T_k (y - X \beta)}{n \parallel y - X \beta \parallel_2}$​ 이므로,​​
        * $\nabla_{\beta}\parallel y - X \beta \parallel_2 = - \frac{X^T (y - X \beta)}{n \parallel y - X \beta \parallel_2}$​​
        * 복잡한 계산이지만 사실 $X \beta$​를 계수 $\beta$​에 대해 미분한 결과인 $X^T$​​만 곱해지는 것
      * 경사하강법 알고리즘 : $\beta^{(t+1)} \leftarrow \beta^{(t)} - \lambda \nabla_{\beta}\parallel y - X \beta^{(t)} \parallel$
    * 목적식이 $\parallel y - X \beta \parallel_2^2$​ 일 때
      * 그레디언트 벡터 식 : $\nabla_{\beta}\parallel y - X \beta \parallel_2^2 = - \frac{2}{n} X^T (y - X \beta)$
      * 경사하강법 알고리즘 : $\beta^{(t+1)} \leftarrow \beta^{(t)} + \frac{2 \lambda}{n} X^T (y - X \beta^{(t)})$​
  * 특징
    * 학습률(lr)과 학습횟수(epoch)가 중요한 하이퍼파라미터(hyperparameter)
    * convex한 함수에 대해서는 수렴 보장

* **확률적 경사하강법(stochastic gradient descent)**
  * 특징
    * 일부 데이터를 활용하여 업데이트
    * 연산자원을 효율적으로 활용 가능 (메모리 등 하드웨어 한계 극복 가능)
    * **non-convex 목적식을 최적화할 수 있다. (머신러닝 학습에 더 효율적)**
  * 원리 : 미니배치 연산
    * **목적식이 미니배치마다 조금씩 달라진다.**
    * 따라서 극소점/극대점이 바뀔 수 있다. (극소점에서 탈출할 수 있다.)

## 과제 수행 과정

* Assignment1 : numpy 이용
* Assignment2,3 : re 이용

## 학습 회고

* 생각보다 오래 앉아있는게 힘들다.
