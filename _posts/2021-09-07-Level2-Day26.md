---
layout: post
title:  "[Boostcamp AI Tech] 26일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day26"
categories: "Boostcamp-AI-Tech"
tags: [6주차]
use_math: true
---

# 부스트캠프 26일차

## 📝 오늘 일정 정리

* 9/7(화)
  - [x] NLP 이론 강의
    - [x] (03강) Basics of Recurrent Neural Networks (RNNs)
    - [x] (04강) LSTM, GRU
    - [x] [실습] Basic RNN, LSTM, GRU
    - [x] [필수 과제 2] RNN-based Language Model
    - [x] [필수 과제 3] Subword-level Language Model

## 📚 강의 내용 정리

### [3강] Recurrent Neural Network and Language Modeling

* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [CS231n(2017) Lecture10 RNN](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)

#### Vanilla RNN

* $h_t = f_W (h_{t-1}, x_t) = \tanh (W_{hh} h_{t-1} + W_{xh} x_t)$
* $y_t = W_{hy} h_t$

#### Types of RNNs

* one-to-one : Standard neural networks
* one-to-many : Image Captioning
* many-to-one : Sentiment Classification
* many-to-many
  * input sequence를 다 읽고 예측 : Machine Translation
  * 각 time step마다 예측 : POS tagging, Video classification on frame level

#### Character-level Language Model

* "hello" -> vocab=[h, e, l, o] -> ont-hot vector로 변환 -> training
* $h_t = f_W (h_{t-1}, x_t) = \tanh (W_{hh} h_{t-1} + W_{xh} x_t + b)$
* Logit $= W_{hy} h_t + b$ -> softmax 후 target char 예측
* Backpropagation through time (BPTT)
* Searching for Interpretable Cells : 필요한 정보가 어디에 저장되어있는지 파악하기 위해서, hidden state의 차원 하나를 고정하고, 그 값이 time step이 진행됨에 따라 어떻게 변하는지 확인 하기
* Vanishing/Exploding Gradient Problem in RNN

### [4강] LSTM and GRU

* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

#### Long Short-Term Memory (LSTM)

* 핵심 아이디어
  * long-term dependency 문제를 해결하기 위함 (단기 기억을 길게 기억할 수 있도록 개선)
  * cell state information을 변형없이 pass할 수 있도록 함
*  $\{ C_t, h_t \} = LSTM(x_t, C_{t-1}, h_{t-1})$
* $\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix} = \begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \tanh \end{pmatrix} W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}$
  * input gate : $f_i = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)$
  * forget gate : $f_t = \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)$
  * output gate : $f_o = \sigma (W_o \cdot [h_{t-1}, x_t] + b_o)$
  * gate gate : $\tilde{C_t} = \tanh (W_C \cdot [h_{t-1}, x_t] + b_C)$
* $C_t = f \odot C_{t-1} + i \odot g = f_t \cdot C_{t-1} + i_t \cdot \tilde{C_t}$
* $h_t = o_t \odot \tanh(C_t)$

#### Gated Recurrent Unit (GRU)

* 핵심 아이디어
  * LSTM의 모델 구조를 경량화
  * 동작원리는 비슷하지만 오직 hidden state만 존재
* $z_t = \sigma (W_z \cdot [h_{t-1}, x_t])$
* $r_t = \sigma (W_r \cdot [h_{t-1}, x_t])$
* $\tilde{h_t} = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t])$
* $h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h_t}$
* Backpropagation in LSTM & GRU : 덧셈 연산으로 인해 gradient 복사 효과로 인해서 gradient를 큰 변형없이 멀리 있는 time step까지 전달해줄 수 있다.

## 🔎 과제 수행 과정

* 필수과제 2 : RNN 계열 모델을 활용한 언어 모델
  * `Dictionary`: 데이터에 등장하는 어휘의 집합. 집합 내 어휘를 unique한 id에 mapping 한다.
  * `Corpus`: 모델의 학습, 테스트 과정에서 사용되는 입력을 준비합니다. 데이터를 load 하고 dictionary 를 생성한다. 데이터를 tokenize 하고 생성한 dictionary 를 이용해 각 단어(tokenize 된 output)를 id로 변환한다.
  * `RNNModel`: Encoder, RNN module, decoder 를 포함한 컨테이너 모듈 -> RNN forward 짜기 ([참고](https://github.com/pytorch/examples/blob/master/word_language_model/model.py))
  * loss 와 [perplexity score](https://wikidocs.net/21697) 를 모니터링하여 학습 현황 확인
* 필수과제 3 : Subword-level language model
  * subword : 하나의 단어를 여러개의 단위로 분리했을 때 하나의 단위
  * tokenization : 주어진 입력 데이터를 자연어처리 모델이 인식할 수 있는 단위로 변환해주는 방법
  * word tokenization : "단어"가 자연어처리 모델이 인식하는 단위
    * 문제 : word embedding을 사용하는 경우 training에 사용되는 text file의 크기가 커질수록 word embedding parameter는 더 커지게 되고 전체 parameter 대비 word embedding이 차지하는 비중은 매우 높아진다.
    * 해결 1 : character-level tokenization, but 이 방법 역시 지나치게 sequence 길이가 길다.
    * 해결 2 : **subword tokenization** (참고자료 : [Huggingface: subword-tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))

## 🌱 피어 세션 정리

* 강의 관련 논의
  * BPTT 이외에 RNN/LSTM/GRU의 구조를 유지하면서 gradient vanishing/exploding 문제를 완화할 수 있는 방법이 있을까요?
    * truncated-BPTT
    * weight 초기화 : xavier, kaiming
* 과제 관련 논의
  * pack_padded_sequence 의 결과가 무슨 뜻인지 : 문장길이대로 sorting 후 계산하니까 연산량을 효율적으로 만들 수 있다. ([관련 설명](https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html))
* 이번 주 목, 금 계획
  * 목요일까지 rnn 코드 구현 후 공유
  * 금요일까지 학습까지 완료 후 공유 ([네이버 영화리뷰 데이터셋](https://github.com/e9t/nsmc))

## 🚀 학습 회고

* RNN도 이론은 익숙한데 역시 코드로 구현하려니까 어려운 것 같다.
* 이번 주 목, 금에 구현 보충 학습을 하면서 확실하게 이해하도록 노력해야겠다.

