---
layout: post
title:  "[MLOps Study] 3. 코드 품질, 데이터 검증, 모델 분석"
subtitle:   "Naver Boostcamp AI Tech 구성원들과 함께하는 MLOps 스터디"
categories: "MLOps"
tags: []
use_math: true
---

# Week3

## 📝 이번 주 목표

- [x] [Section 2] 코드 품질, 데이터 검증, 모델 분석
  - [x] (1) 리서치 코드 품질 관리
  - [x] (2) 데이터 검증 - Tensorflow Data Validation
  - [x] (3) 머신러닝 모델 분석 What if tool

## 📚 강의 내용 정리

### (1) 리서치 코드 품질 관리

#### 리서치 코드 품질 문제

* 복사 붙여넣기로 인한 코드 중복 -> 코드 재사용성(추상화, 라이브러리) 높이기
* relative import로 인해 꼬인 import -> absolute import 사용하기
  1. Standard library imports
  2. Related third party imports
  3. Local application/library specific imports
* 버전관리, 실험관리가 안되서 연구결과 재연 불가능
* 너무 많은 전역 변수, 명확하기 않은 변수명 등

#### 린트, 유닛 테스트

* [`black`](https://pypi.org/project/black/) : indent, naming convention(snake_case)
* [`flake8`](https://pypi.org/project/flake8/) : python code linter tool
* [`mypy`](https://pypi.org/project/mypy/) : 타입 힌트(ex. `def fn(a: int) -> bool`)에 오류가 없는지 자동으로 확인 가능

#### 지속적 통합 실습

* 지속적 통합(CI, Continuous Integration) : 작은 단위의 작업, 빈번한 통합
* Github Actions 이용하기
* [[실습 레포](https://github.com/chris-chris/research-ci-tutorial)]
  * [린트 체크](https://github.com/chris-chris/research-ci-tutorial/blob/main/.github/workflows/lint.yml)
  * [유닛 테스트](https://github.com/chris-chris/research-ci-tutorial/blob/main/.github/workflows/coverage.yml) + [Code Climate](https://codeclimate.com/)
    * [Code Climate - Add Repo - Repo Settings - Test coverage] 에서 `TEST REPORTER ID` 복사한 후 `coverage.yml` 내의 `CODECLIMATE_REPO_TOKEN` 수정하기
      ```yaml
      name: Upload coverage to code climate
        env:
            CODECLIMATE_REPO_TOKEN: [MY TEST REPORTER ID]
        run: |
            codeclimate-test-reporter
      ```
    * [Code Climate - Repo Setting - GitHub] 에서 `Pull request comments`나 `Inline issue comments`를 활성화하면 자동으로 코드에 리뷰를 달아준다.
    * README.md 에 뱃지 추가도 가능
  * Settings - Branches - Branch protection rules [Add rule]
    - [x] Require status checks to pass before merging
      - [x] Require branches to be up to date before merging
  * 린트와 유닛 테스트를 통과하도록 코드를 수정해야 merge 가능!

### (2) 데이터 검증 : TFDV(TensorFlow Data Validation)

#### 데이터 검증과 TFDV

* 데이터 검증이 필요한 이유 : 머신러닝 시스템에서 데이터로 인한 장애는 파악하기 어렵다.
* TFDV(TensorFlow Data Validation) : Framework Independent 한 솔루션!
  * 기술 통계보기, 스키마 추론, 이상 항목 확인 및 수정
  * 데이터셋 드리프트 및 왜곡  확인
  * 훈련, 평가 및 제공 데이터셋의 일관성 확인

#### TFDV 실습

* [[실습 코드](https://github.com/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb)]
* 통계 계산 및 시각화
  * 학습 데이터 확인
    ```python
    train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)
    tfdv.visualize_statistics(train_stats)
    ```
  * 평가 데이터의 오류 확인
    ```python
    eval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA)
    tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,
                              lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')
    ```
* 스키마 추론과 스키마 환경
  * 스키마 추론 : 각 feature의 데이터 유형, 빈도, 도메인 등
    ```python
    schema = tfdv.infer_schema(statistics=train_stats)
    tfdv.display_schema(schema=schema)
    ```
  * 평가 데이터의 이상 데이터(Anomaly) 확인
    ```python
    anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)
    tfdv.display_anomalies(anomalies)
    ```
  * 스키마의 평가 이상 수정
    ```python
    # 도메인에서 가져와야 하는 최소 비율의 값 완화하기
    company = tfdv.get_feature(schema, 'company')
    company.distribution_constraints.min_domain_mass = 0.9
    
    # 도메인에 새 값 추가하기
    payment_type_domain = tfdv.get_domain(schema, 'payment_type')
    payment_type_domain.value.append('Prcard')
    
    # 스키마 변경 후 다시 평가 데이터의 이상 확인하기
    updated_anomalies = tfdv.validate_statistics(eval_stats, schema)
    tfdv.display_anomalies(updated_anomalies)
    ```
  * 제공 데이터의 이상 데이터 확인
    ```python
    options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)
    serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options)
    serving_anomalies = tfdv.validate_statistics(serving_stats, schema)
    tfdv.display_anomalies(serving_anomalies)
    ```
  * 스키마 환경 : 분리된 데이터셋을 위한 약간의 스키마 변형 도입 (ex. 지도학습 레이블 포함 관련)
    ```python
    # TRAINING 과 SERVING 환경에서 모두 기본적으로 제공된다.
    schema.default_environment.append('TRAINING')
    schema.default_environment.append('SERVING')
    
    # SERVING 환경에 'tips' feature를 무시하도록 설정한다.
    tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')
    
    # 다시 제공 데이터의 이상 확인하기
    serving_anomalies_with_env = tfdv.validate_statistics(
        serving_stats, schema, environment='SERVING')
    tfdv.display_anomalies(serving_anomalies_with_env)
    ```
  * 스키마 고정 (파일로 저장)
    ```python
    from tensorflow.python.lib.io import file_io
    from google.protobuf import text_format
    
    file_io.recursive_create_dir(OUTPUT_DIR)
    schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')
    tfdv.write_schema_text(schema, schema_file)
    ```
* 데이터 드리프트 및 스큐
  * 데이터 드리프트 : 범주형이나 연속형에 대해 드리프트가 허용 가능한 것(임계 거리)보다 높을 때 경고를 줄 수 있다.
  * 스키마 스큐(Schema Skew) : 학습 및 서빙 데이터가 동일한 스키마를 따르지 않을 때 발생
  * 특성 스큐(Feature Skew) : 모델이 학습하는 특성값이 서빙 시에 표시되는 특성값과 다를 때 발생
  * 분포 스큐(Distribution Skew) : 학습 데이터셋의 분포가 서빙 데이터셋의 분포와 크게 다를 때 방생
    ```python
    # Add skew comparator for 'payment_type' feature.
    payment_type = tfdv.get_feature(schema, 'payment_type')
    payment_type.skew_comparator.infinity_norm.threshold = 0.01
    
    # Add drift comparator for 'company' feature.
    company=tfdv.get_feature(schema, 'company')
    company.drift_comparator.infinity_norm.threshold = 0.001
    
    skew_anomalies = tfdv.validate_statistics(train_stats, schema,
                                              previous_statistics=eval_stats,
                                              serving_statistics=serving_stats)
    tfdv.display_anomalies(skew_anomalies)
    ```

### (3) 머신러닝 모델 분석 : WIT(What if tool)

#### What-If-Tool 소개

* 모델 분석을 데이터와 모델 관점에서 해줘야 하는데 디버깅이 상당히 어렵다.
* [What-If-Tool](https://pair-code.github.io/what-if-tool/) : 훈련된 ML 모델의 동작을 분석하는 시각화 기반 도구
  * Datapointer Editor : 각각의 데이터 포인트를 확인해볼 수 있다. 예측 결과의 변경도 확인 가능하다.
  * Performance and Fairness : 각 feature를 슬라이싱한 모델의 성능(ex. 나이대별 모델 정확도)을 확인할 수 있다. 얼마나 데이터의 분포가 고르게 되어있는지 확인도 가능
  * Feature : TFDV에서 stat을 뽑아보는 것과 유사하다.
* Google Cloud AI 를 사용한다면 바로 연동해서 실시간으로 모델 분석이 가능하다.

#### What-If-Tool 모델 분석 실습

* [[실습 내용](https://pair-code.github.io/what-if-tool/learn/tutorials/walkthrough/) \| [실습 코드](https://link.chris-chris.ai/ai-lecture-13)]
  * 가장 가까운 Counterfactuals 탐색 : 분류가 어떻게 변경되는지 확인 가능
  * 부분 종속성 플롯 탐색
  * 모델 성능 분석 : Confusion matrix, ROC curve 등
  * 비용 비율 및 결정 임계값 최적화
  * 데이터셋의 각 feature에 대한 값 분포
* [[Web demo](https://pair-code.github.io/what-if-tool/demos/image.html) \| [Notebook demo](https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/WIT_Smile_Detector.ipynb)]

## 📢 스터디 미팅

* [huggingface datasets](https://github.com/huggingface/datasets)
* [구글 AI 에코시스템](https://ai.google/tools/)

## 🚀 학습 회고

* 다른 내용은 조금 낮설지만 GitHub Action은 프로젝트할 때 바로 적용해서 해볼만한 것 같다!

