---
layout: post
title:  "[Boostcamp AI Tech] 6ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day6"
categories: "Boostcamp-AI-Tech"
tags: [2ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 6ì¼ì°¨

## ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 8/9 (ì›”)
  - [x] DL Basic
    - [x] (1ê°•) ë”¥ëŸ¬ë‹ ê¸°ë³¸ ìš©ì–´ ì„¤ëª… - Historical review
    - [x] (2ê°•) ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ - MLP
    - [x] [í•„ìˆ˜ ê³¼ì œ1] MLP Assignment
  - [x] ë…¼ë¬¸ ë°œí‘œ ì¤€ë¹„í•˜ê¸°

## ê°•ì˜ ë‚´ìš© ì •ë¦¬

### [1ê°•] ë”¥ëŸ¬ë‹ ê¸°ë³¸ ìš©ì–´ ì„¤ëª…

* ë”¥ëŸ¬ë‹ ì¤‘ìš” ìš”ì†Œ : data, model, loss function, optimization algorithm ë“±
  * loss function : ì´ë£¨ê³ ì í•˜ëŠ” ê²ƒì˜ ê·¼ì‚¬ì¹˜
    * ![image](https://user-images.githubusercontent.com/35680202/128652035-6e7f92a1-2929-4e2f-9b05-0873aefdc5d8.png)
* [Historical Review](https://dennybritz.com/blog/deep-learning-most-important-ideas/)
  * 2012 - AlexNet : ImageNet challenge ì—ì„œ ë”¥ëŸ¬ë‹ ê¸°ë²•ìœ¼ë¡œ ì²˜ìŒ 1ë“±
  * 2013 - DQN : ê°•í™”í•™ìŠµ QëŸ¬ë‹, ë”¥ë§ˆì¸ë“œ
  * 2014 - Encoder/Decoder : NMT(Neural Machine Translation) ê¸°ê³„ì–´ë²ˆì—­
  * 2014 - Adam Optimizer : ì›¬ë§Œí•˜ë©´ í•™ìŠµì´ ì˜ ëœë‹¤.
  * 2015 - GAN : ë„¤íŠ¸ì›Œí¬ê°€ generatorì™€ discriminator ë‘ê°œë¥¼ ë§Œë“¤ì–´ì„œ í•™ìŠµ
  * 2015 - Residual Networks : ë„¤íŠ¸ì›Œí¬ë¥¼ ê¹Šê²Œ ìŒ“ì„ ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì¤Œ
  * 2017 - Transformer (Attention Is All You Need) : ê¸°ì¡´ ë°©ë²•ë¡ ë“¤ì„ ëŒ€ì²´í•  ì •ë„ì˜ ì˜í–¥ë ¥
  * 2018 - BERT (Bidirectional Encoder Representations from Transformers) : 'fine-tuned' NLP models ë°œì „ ì‹œì‘
  * 2019 - BIG Language Models : fine-tuned NLP modelì˜ ëíŒì™•, OpenAI GPT-3
  * 2020 - Self-Supervised Learning : SimCLR (a simple framework for contrastive learning of visual representations)

### [2ê°•] ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ - MLP (Multi-Layer Perceptron)

* Neural networks are **function approximators** that stack affine tansformations  followed by nonlinear transformations : í–‰ë ¬ ê³±ê³¼ ë¹„ì„ í˜• ì—°ì‚°ì´ ë°˜ë³µë˜ë©´ì„œ, í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ëŠ” ëª¨ë¸
* Linear Neural Networks
  * $y = W^Tx + b$ì—ì„œ $W$ë¥¼ ì°¾ëŠ” ê²ƒì€ ì„œë¡œ ë‹¤ë¥¸ ë‘ ì°¨ì›ì—ì„œì˜ ì„ í˜•ë³€í™˜ì„ ì°¾ê² ë‹¤ëŠ” ê²ƒ
* Multi-Layer Perceptron
  * $y = W_2^Th = W_2^T W_1^T x$â€‹ ëŠ” linear neural networkì™€ ë‹¤ë¥¼ ë°”ê°€ ì—†ë‹¤.
  * $y = W_2^Th = W_2^T \rho(W_1^T x)$â€‹ì™€ ê°™ì€ Nonlinear transformì´ í•„ìš”í•˜ë‹¤.
  * Multilayer Feedforward Networks are Universal Approximators : ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ë ¥ì´ ê·¸ë§Œí¼ í¬ë‹¤. í•˜ì§€ë§Œ ì–´ë–»ê²Œ ì°¾ì„ì§€ëŠ” (ì•Œì•„ì„œí•´)
* [PyTorch official docs](https://pytorch.org/docs/stable/nn.html)

## í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

### ë”¥ëŸ¬ë‹ ë…¼ë¬¸ ë¦¬ë·° ìŠ¤í„°ë”” ì‹œì‘

* ëª©ì°¨
  1. [VGG-16](https://arxiv.org/abs/1409.1556) (2014)
  2. [Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf) (2015) ğŸ™‹â€â™€ï¸
  3. [GoogLeNet(Inception-v1)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) (2014)
  4. [ResNet-50](https://arxiv.org/abs/1512.03385) (2015)
  5. [Inception-v3](https://arxiv.org/abs/1512.00567v3) (2015)
  6. [XceptionNet](https://arxiv.org/abs/1610.02357) (2016)
  7. [DenseNet](https://arxiv.org/abs/1608.06993) (2017)
  8. [Inception-v4, Inception-ResNet](https://arxiv.org/abs/1602.07261v2) (2016) ğŸ™‹â€â™€ï¸
  9. [ResNeXt-50](https://arxiv.org/abs/1611.05431) (2017)
  10. [EfficientNet](https://arxiv.org/abs/1905.11946) (2019)
  11. [EfficientNet v2](https://arxiv.org/pdf/2104.00298.pdf) (2020)
* ë°©ë²•
  * í•˜ë£¨ì— ë‘ ëª…ì”© ë°œí‘œ
  * ë‚´ìš© : ì œì•ˆ ë°°ê²½ / ëª¨ë¸ êµ¬ì¡° / ì‹¤í—˜ ê²°ê³¼ ë“±ë“±
  * ë ˆë²¨ : êµ¬ì¡°ì— ëŒ€í•œ ì„¤ëª… ì¶©ë¶„íˆ
  * í˜•ì‹ : ë°œí‘œìë£ŒëŠ” ììœ ë¡­ê²Œ

## í•™ìŠµ íšŒê³ 

* ë…¼ë¬¸ ë¦¬ë·° ìŠ¤í„°ë””ë¥¼ íŒ€ì›ë“¤ê³¼ ê°™ì´ í•˜ê²Œ ë˜ì—ˆë‹¤. í˜¼ì ì½ì„ ìƒê°ì— ë§‰ë§‰í–ˆëŠ”ë° ë‹¤í–‰ì´ë‹¤.
* í•œë‹¬ ì „ì— ì½ì€ ì  ìˆëŠ” ë…¼ë¬¸ì¸ë°ë„ ì§€ê¸ˆ ì •ë¦¬í•˜ëŠ”ë° ê½¤ ê±¸ë ¸ë‹¤. ì—­ì‹œ ë°œí‘œ ì¤€ë¹„í•˜ë©´ì„œ ì½ëŠ” ê±°ë‘ í˜¼ì ì½ëŠ” ê±°ë‘ì€ ì™„ì „íˆ ë‹¤ë¥¸ ê²ƒ ê°™ë‹¤.
