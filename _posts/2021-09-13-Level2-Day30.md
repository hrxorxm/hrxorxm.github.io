---
layout: post
title:  "[Boostcamp AI Tech] 30일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day30"
categories: "Boostcamp-AI-Tech"
tags: [7주차]
use_math: true
---

# 부스트캠프 30일차

## 📝 오늘 일정 정리

* 9/13(월)
  - [x] NLP 이론 강의
    - [x] (7강) Transformer (1)
    - [x] (8강) Transformer (2)
  - [x] 논문 스터디(Bahdanau Attention) 13:00~14:00

## 📚 강의 내용 정리

### [7강] Transformer (1)

* RNN Family
  * RNN : Long-term dependency 문제
  * Bi-Directional RNNs : Forward RNN + Backward RNN -> concat

* Transformer
  * 목표
    * 더 이상 RNN, CNN 모듈을 사용하지 않고, Attention만을 이용한 모델 구조
  * 구조
    * Query와 Key는 같은 차원 수 $d_k$ 를 가지고, Value는 $d_v$ 차원이다.
    * Dot-Product Attention
      * 하나의 쿼리 $q$에 대해서 : $A(q, K, V) = \sum_i \frac{\exp(q \cdot k_i)}{\sum_j \exp(q \cdot k_j)} v_i$
      * 여러 쿼리 matrix $Q$에 대해서 : $A(Q, K, V) = softmax(QK^T) V$
        * 결과 차원 수 : $(\| Q \| \times d_k) \times (d_k \times \| K \|) \times (\| V \| \times d_v) = (\| Q \| \times d_v)$
        * $\| Q \|$와 $\| K \|$ 가 같을 필요는 없다. ($\| Q \|$는 그냥 쿼리 수일 뿐)
        * $\| K \|$ (Key의 개수)와 $\| V \|$ (Value의 개수)는 같아야 한다. 
        * Row-wise sorftmax : $QK^T$ 를 계산한 결과에서 row-wise하게 softmax를 취한다.
    * Scaled Dot-Product Attention
      ![image](https://user-images.githubusercontent.com/35680202/133083921-f3f4e76e-69f9-4506-b780-38d765d151ec.png)
      * $A(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V$
      * $\frac{1}{\sqrt{d_k}}$ : softmax 값이 쏠리지 않도록 함
* 참고 자료
  * [Attention is all you need](https://arxiv.org/abs/1706.03762)
  * [Illustrated Transformer](https://nlpinkorean.github.io/illustrated-transformer/)
  * [부스트캠프 2주차 Day9 내용 정리](https://hrxorxm.github.io/boostcamp-ai-tech/2021/08/12/Level1-Day9.html#h-8%EA%B0%95-sequential-models---transformer)

### [8강] Transformer (2)

* Multi-Head Attention
  * 목표
    * 동일한 sequence에 대해서 여러 측면에서 병렬적으로 정보를 뽑아오자!
    * 각 헤드가 서로 다른 정보들을 상호보완적으로 뽑아오는 역할
  * 구조
    ![image](https://user-images.githubusercontent.com/35680202/133083669-8e13a3eb-2761-4625-85de-7db25afa8fe0.png)
    * $MultiHead(Q,K,V) = Concat(head_1, ..., head_h) W^o$
      * where $head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)$

* 연산량 비교
  ![image](https://user-images.githubusercontent.com/35680202/133038565-17f04f54-c7a4-4e46-9d58-c42122016ca0.png)
  * 수식 설명
    * $n$ : sequence length / $d$ : dimension of representation
    * Self-Attention : $QK^T$에서, $(n \times d) \times (d \times n)$이므로 $O(n^2 \cdot d)$ 이다.
    * Recurrent : $W_{hh} \cdot h_{t-1}$ 에서 $(d \times d) \times (d \times 1)$ 이고, 매 time step에서 계산해줘야하기 때문에, $O(n \cdot d^2)$ 이다.
  * 특징
    * Self-Attention에서 입력 길이가 길어지면 메모리 사용량이 커진다. 하지만 (Sequential Operations 부분을 보면) 병렬화가 가능하기 때문에 학습은 더 빨리 진행될 수 있다.
    * Maximum Path Length : Long-term dependency와 직접적으로 관련, 가장 끝에 있는 단어가 가장 앞에 있는 단어의 정보를 참조하려면 RNN에서는 $n$번의 계산이 필요하지만, Self-Attention에서는 끝에 있는 단어라도 인접한 단어와 차이없이 필요한만큼의 정보를 바로 가져올 수 있다.

* Add (Residual Connection) & Normalize (Layer Normalization)
  ![image](https://user-images.githubusercontent.com/35680202/133040855-0f5e8447-3873-4dca-8f72-c0df35e29379.png)
  * Residual Connection : Gradient Vanishing 문제 해결, 학습 안정화 효과! 입력 벡터와 Self-Attention의 출력 벡터의 dimension이 동일하도록 유지해야한다.
  * Layer Normalization
    ![image](https://user-images.githubusercontent.com/35680202/133082991-c66ed00a-785b-415d-a3f3-bc675c4ea4f6.png)
    * $\mu^l = \frac{1}{H} \sum_{i=1}^{H} a_i^l$, $\sigma^l = \sqrt{\frac{1}{H} \sum_{i=1}^{H} (a_i^l - \mu^l)^2}$, $h_i = f(\frac{g_i}{\sigma_i} (a_i - \mu_i) + b_i)$
    * each word vectors(특정 단어를 표현하는 노드들)의 평균과 분산을 0, 1로 만든다.
    * 레이어의 각 노드별로 affine transform($y = ax + b$)을 적용시킨다.

* Positional Encoding
  * 순서를 특정지을 수 있는 상수 벡터를 입력 벡터에 더해준다.
  * sin, cos 등으로 이루어진 주기 함수를 이용한다.
  * $PE(pos, 2i) = \sin (pos / 10000^{2i / d_{model}})$
  * $PE(pos, 2i + 1) = \cos (pos / 10000^{2i / d_{model}})$

* Warm-up Learning Rate Scheduler
  ![image](https://user-images.githubusercontent.com/35680202/133088905-b876cdbd-9915-4ef0-a8ac-01201cf60b28.png)
  * $learning\_rate = d_{model}^{-0.5} \cdot \min (step^{-0.5} \cdot warmup\_steps^{-1.5})$

* Encoder Self-Attention Visualization : [예제](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)

* Decoder Masked Self-Attention
  * 예측과정을 생각해보면, 뒤에 나오는 단어는 모르는 상태로 예측해야한다.
  * 따라서 softmax 에 넣기 전에, 뒤에 나오는 단어에 대한 score값들을 `-inf`으로 처리하여, softmax 함수 후 값이 0이 될 수 있도록 한다.

* 추가 자료
  * [Attention is not Explanation](https://arxiv.org/pdf/1902.10186.pdf)
  * [Attention is not not Explanation](https://aclanthology.org/D19-1002.pdf)

## 📖 논문 스터디

### [Bahdanau Attention](https://arxiv.org/abs/1409.0473)

* 내용 (주요 변화 및 기여)
  * Phrase-based translation(PBSMT) -> Neural Machine Translation
  * 정렬과 번역을 동시에 학습하는 방식으로 기존 방식과 **비슷한** 성능을 내는데에 성공
* 문제
  * 입력이 길 때, 고정된 길이의 벡터로 완벽히 표현 가능할까? (bottleneck)
* 해결
  * 인코더에서 단어 한 개당 하나씩 정리해서(Query) 디코더에 넘겨주기
  * 각각에 대해서 구한 score를 key와 곱해서 value를 구한다.
  * 디코더의 각 스텝에서, value 벡터와 이전 스텝까지의 정리된 벡터를 concat(Bahdanau 방식) 해서 다음 입력으로 넣는다.
* 참고 자료
  * [코드 리뷰](https://github.com/bentrevett/pytorch-seq2seq)
  * [Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
  * [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
* 함께 보면 좋은 논문
  * [RNN Encoder Decoder](https://arxiv.org/abs/1406.1078)
  * [Sequence to Sequence](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
  * [GNMT](https://arxiv.org/abs/1609.08144)

## 🌱 피어 세션 정리

* 질의응답
  * $\| Q \|$와 $\| K \|$ 가 인코더와 디코더에서 어떻게 같고 다른지?
  * input sequence의 길이가 달라짐에 따라서 Q, K, V 의 row 수가 달라지는 것이 맞는지?
  * subword tokenization 의 성능이 좋은 이유가 뭔지?
  * 선택과제 1번 도움 : [huggingface models](https://huggingface.co/models?sort=downloads&search=ko)
* 일정
  * 월,화 : 강의 보고 토의
  * 수 : 선택 과제 1,2,3 도전해보고 토의
  * 목 : [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
  * 금 : [Pytorch Transformers from Scratch](https://youtu.be/U0s0f995w14)

## 🚀 학습 회고

* 오늘 Transformer를 다시 공부하면서 그 전에 조금 가려웠던 부분을 시원하게 긁은 것 같다.
* 내일 논문 스터디랑 실습 더 해보면서 이번 주에는 확실히 이해하고 싶다.

