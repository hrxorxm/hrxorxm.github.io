---
layout: post
title:  "[Boostcamp AI Tech] 30ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day30"
categories: "Boostcamp-AI-Tech"
tags: [7ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 30ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/13(ì›”)
  - [x] NLP ì´ë¡  ê°•ì˜
    - [x] (7ê°•) Transformer (1)
    - [x] (8ê°•) Transformer (2)
  - [x] ë…¼ë¬¸ ìŠ¤í„°ë””(Bahdanau Attention) 13:00~14:00

## ğŸ“š ê°•ì˜ ë‚´ìš© ì •ë¦¬

### [7ê°•] Transformer (1)

* RNN Family
  * RNN : Long-term dependency ë¬¸ì œ
  * Bi-Directional RNNs : Forward RNN + Backward RNN -> concat

* Transformer
  * ëª©í‘œ
    * ë” ì´ìƒ RNN, CNN ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , Attentionë§Œì„ ì´ìš©í•œ ëª¨ë¸ êµ¬ì¡°
  * êµ¬ì¡°
    * Queryì™€ KeyëŠ” ê°™ì€ ì°¨ì› ìˆ˜ $d_k$ ë¥¼ ê°€ì§€ê³ , ValueëŠ” $d_v$ ì°¨ì›ì´ë‹¤.
    * Dot-Product Attention
      * í•˜ë‚˜ì˜ ì¿¼ë¦¬ $q$ì— ëŒ€í•´ì„œ : $A(q, K, V) = \sum_i \frac{\exp(q \cdot k_i)}{\sum_j \exp(q \cdot k_j)} v_i$
      * ì—¬ëŸ¬ ì¿¼ë¦¬ matrix $Q$ì— ëŒ€í•´ì„œ : $A(Q, K, V) = softmax(QK^T) V$
        * ê²°ê³¼ ì°¨ì› ìˆ˜ : $(\| Q \| \times d_k) \times (d_k \times \| K \|) \times (\| V \| \times d_v) = (\| Q \| \times d_v)$
        * $\| Q \|$ì™€ $\| K \|$ ê°€ ê°™ì„ í•„ìš”ëŠ” ì—†ë‹¤. ($\| Q \|$ëŠ” ê·¸ëƒ¥ ì¿¼ë¦¬ ìˆ˜ì¼ ë¿)
        * $\| K \|$ (Keyì˜ ê°œìˆ˜)ì™€ $\| V \|$ (Valueì˜ ê°œìˆ˜)ëŠ” ê°™ì•„ì•¼ í•œë‹¤. 
        * Row-wise sorftmax : $QK^T$ ë¥¼ ê³„ì‚°í•œ ê²°ê³¼ì—ì„œ row-wiseí•˜ê²Œ softmaxë¥¼ ì·¨í•œë‹¤.
    * Scaled Dot-Product Attention
      ![image](https://user-images.githubusercontent.com/35680202/133083921-f3f4e76e-69f9-4506-b780-38d765d151ec.png)
      * $A(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V$
      * $\frac{1}{\sqrt{d_k}}$ : softmax ê°’ì´ ì ë¦¬ì§€ ì•Šë„ë¡ í•¨
* ì°¸ê³  ìë£Œ
  * [Attention is all you need](https://arxiv.org/abs/1706.03762)
  * [Illustrated Transformer](https://nlpinkorean.github.io/illustrated-transformer/)
  * [ë¶€ìŠ¤íŠ¸ìº í”„ 2ì£¼ì°¨ Day9 ë‚´ìš© ì •ë¦¬](https://hrxorxm.github.io/boostcamp-ai-tech/2021/08/12/Level1-Day9.html#h-8%EA%B0%95-sequential-models---transformer)

### [8ê°•] Transformer (2)

* Multi-Head Attention
  * ëª©í‘œ
    * ë™ì¼í•œ sequenceì— ëŒ€í•´ì„œ ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ì •ë³´ë¥¼ ë½‘ì•„ì˜¤ì!
    * ê° í—¤ë“œê°€ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë“¤ì„ ìƒí˜¸ë³´ì™„ì ìœ¼ë¡œ ë½‘ì•„ì˜¤ëŠ” ì—­í• 
  * êµ¬ì¡°
    ![image](https://user-images.githubusercontent.com/35680202/133083669-8e13a3eb-2761-4625-85de-7db25afa8fe0.png)
    * $MultiHead(Q,K,V) = Concat(head_1, ..., head_h) W^o$
      * where $head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)$

* ì—°ì‚°ëŸ‰ ë¹„êµ
  ![image](https://user-images.githubusercontent.com/35680202/133038565-17f04f54-c7a4-4e46-9d58-c42122016ca0.png)
  * ìˆ˜ì‹ ì„¤ëª…
    * $n$ : sequence length / $d$ : dimension of representation
    * Self-Attention : $QK^T$ì—ì„œ, $(n \times d) \times (d \times n)$ì´ë¯€ë¡œ $O(n^2 \cdot d)$ ì´ë‹¤.
    * Recurrent : $W_{hh} \cdot h_{t-1}$ ì—ì„œ $(d \times d) \times (d \times 1)$ ì´ê³ , ë§¤ time stepì—ì„œ ê³„ì‚°í•´ì¤˜ì•¼í•˜ê¸° ë•Œë¬¸ì—, $O(n \cdot d^2)$ ì´ë‹¤.
  * íŠ¹ì§•
    * Self-Attentionì—ì„œ ì…ë ¥ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì»¤ì§„ë‹¤. í•˜ì§€ë§Œ (Sequential Operations ë¶€ë¶„ì„ ë³´ë©´) ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì€ ë” ë¹¨ë¦¬ ì§„í–‰ë  ìˆ˜ ìˆë‹¤.
    * Maximum Path Length : Long-term dependencyì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨, ê°€ì¥ ëì— ìˆëŠ” ë‹¨ì–´ê°€ ê°€ì¥ ì•ì— ìˆëŠ” ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ë ¤ë©´ RNNì—ì„œëŠ” $n$ë²ˆì˜ ê³„ì‚°ì´ í•„ìš”í•˜ì§€ë§Œ, Self-Attentionì—ì„œëŠ” ëì— ìˆëŠ” ë‹¨ì–´ë¼ë„ ì¸ì ‘í•œ ë‹¨ì–´ì™€ ì°¨ì´ì—†ì´ í•„ìš”í•œë§Œí¼ì˜ ì •ë³´ë¥¼ ë°”ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.

* Add (Residual Connection) & Normalize (Layer Normalization)
  ![image](https://user-images.githubusercontent.com/35680202/133040855-0f5e8447-3873-4dca-8f72-c0df35e29379.png)
  * Residual Connection : Gradient Vanishing ë¬¸ì œ í•´ê²°, í•™ìŠµ ì•ˆì •í™” íš¨ê³¼! ì…ë ¥ ë²¡í„°ì™€ Self-Attentionì˜ ì¶œë ¥ ë²¡í„°ì˜ dimensionì´ ë™ì¼í•˜ë„ë¡ ìœ ì§€í•´ì•¼í•œë‹¤.
  * Layer Normalization
    ![image](https://user-images.githubusercontent.com/35680202/133082991-c66ed00a-785b-415d-a3f3-bc675c4ea4f6.png)
    * $\mu^l = \frac{1}{H} \sum_{i=1}^{H} a_i^l$, $\sigma^l = \sqrt{\frac{1}{H} \sum_{i=1}^{H} (a_i^l - \mu^l)^2}$, $h_i = f(\frac{g_i}{\sigma_i} (a_i - \mu_i) + b_i)$
    * each word vectors(íŠ¹ì • ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ëŠ” ë…¸ë“œë“¤)ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ 0, 1ë¡œ ë§Œë“ ë‹¤.
    * ë ˆì´ì–´ì˜ ê° ë…¸ë“œë³„ë¡œ affine transform($y = ax + b$)ì„ ì ìš©ì‹œí‚¨ë‹¤.

* Positional Encoding
  * ìˆœì„œë¥¼ íŠ¹ì •ì§€ì„ ìˆ˜ ìˆëŠ” ìƒìˆ˜ ë²¡í„°ë¥¼ ì…ë ¥ ë²¡í„°ì— ë”í•´ì¤€ë‹¤.
  * sin, cos ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì£¼ê¸° í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.
  * $PE(pos, 2i) = \sin (pos / 10000^{2i / d_{model}})$
  * $PE(pos, 2i + 1) = \cos (pos / 10000^{2i / d_{model}})$

* Warm-up Learning Rate Scheduler
  ![image](https://user-images.githubusercontent.com/35680202/133088905-b876cdbd-9915-4ef0-a8ac-01201cf60b28.png)
  * $learning\_rate = d_{model}^{-0.5} \cdot \min (step^{-0.5} \cdot warmup\_steps^{-1.5})$

* Encoder Self-Attention Visualization : [ì˜ˆì œ](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)

* Decoder Masked Self-Attention
  * ì˜ˆì¸¡ê³¼ì •ì„ ìƒê°í•´ë³´ë©´, ë’¤ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ëª¨ë¥´ëŠ” ìƒíƒœë¡œ ì˜ˆì¸¡í•´ì•¼í•œë‹¤.
  * ë”°ë¼ì„œ softmax ì— ë„£ê¸° ì „ì—, ë’¤ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì— ëŒ€í•œ scoreê°’ë“¤ì„ `-inf`ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬, softmax í•¨ìˆ˜ í›„ ê°’ì´ 0ì´ ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

* ì¶”ê°€ ìë£Œ
  * [Attention is not Explanation](https://arxiv.org/pdf/1902.10186.pdf)
  * [Attention is not not Explanation](https://aclanthology.org/D19-1002.pdf)

## ğŸ“– ë…¼ë¬¸ ìŠ¤í„°ë””

### [Bahdanau Attention](https://arxiv.org/abs/1409.0473)

* ë‚´ìš© (ì£¼ìš” ë³€í™” ë° ê¸°ì—¬)
  * Phrase-based translation(PBSMT) -> Neural Machine Translation
  * ì •ë ¬ê³¼ ë²ˆì—­ì„ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ê³¼ **ë¹„ìŠ·í•œ** ì„±ëŠ¥ì„ ë‚´ëŠ”ë°ì— ì„±ê³µ
* ë¬¸ì œ
  * ì…ë ¥ì´ ê¸¸ ë•Œ, ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„°ë¡œ ì™„ë²½íˆ í‘œí˜„ ê°€ëŠ¥í• ê¹Œ? (bottleneck)
* í•´ê²°
  * ì¸ì½”ë”ì—ì„œ ë‹¨ì–´ í•œ ê°œë‹¹ í•˜ë‚˜ì”© ì •ë¦¬í•´ì„œ(Query) ë””ì½”ë”ì— ë„˜ê²¨ì£¼ê¸°
  * ê°ê°ì— ëŒ€í•´ì„œ êµ¬í•œ scoreë¥¼ keyì™€ ê³±í•´ì„œ valueë¥¼ êµ¬í•œë‹¤.
  * ë””ì½”ë”ì˜ ê° ìŠ¤í…ì—ì„œ, value ë²¡í„°ì™€ ì´ì „ ìŠ¤í…ê¹Œì§€ì˜ ì •ë¦¬ëœ ë²¡í„°ë¥¼ concat(Bahdanau ë°©ì‹) í•´ì„œ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë„£ëŠ”ë‹¤.
* ì°¸ê³  ìë£Œ
  * [ì½”ë“œ ë¦¬ë·°](https://github.com/bentrevett/pytorch-seq2seq)
  * [Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
  * [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
* í•¨ê»˜ ë³´ë©´ ì¢‹ì€ ë…¼ë¬¸
  * [RNN Encoder Decoder](https://arxiv.org/abs/1406.1078)
  * [Sequence to Sequence](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
  * [GNMT](https://arxiv.org/abs/1609.08144)

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* ì§ˆì˜ì‘ë‹µ
  * $\| Q \|$ì™€ $\| K \|$ ê°€ ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œ ì–´ë–»ê²Œ ê°™ê³  ë‹¤ë¥¸ì§€?
  * input sequenceì˜ ê¸¸ì´ê°€ ë‹¬ë¼ì§ì— ë”°ë¼ì„œ Q, K, V ì˜ row ìˆ˜ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì´ ë§ëŠ”ì§€?
  * subword tokenization ì˜ ì„±ëŠ¥ì´ ì¢‹ì€ ì´ìœ ê°€ ë­”ì§€?
  * ì„ íƒê³¼ì œ 1ë²ˆ ë„ì›€ : [huggingface models](https://huggingface.co/models?sort=downloads&search=ko)
* ì¼ì •
  * ì›”,í™” : ê°•ì˜ ë³´ê³  í† ì˜
  * ìˆ˜ : ì„ íƒ ê³¼ì œ 1,2,3 ë„ì „í•´ë³´ê³  í† ì˜
  * ëª© : [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
  * ê¸ˆ : [Pytorch Transformers from Scratch](https://youtu.be/U0s0f995w14)

## ğŸš€ í•™ìŠµ íšŒê³ 

* ì˜¤ëŠ˜ Transformerë¥¼ ë‹¤ì‹œ ê³µë¶€í•˜ë©´ì„œ ê·¸ ì „ì— ì¡°ê¸ˆ ê°€ë ¤ì› ë˜ ë¶€ë¶„ì„ ì‹œì›í•˜ê²Œ ê¸ì€ ê²ƒ ê°™ë‹¤.
* ë‚´ì¼ ë…¼ë¬¸ ìŠ¤í„°ë””ë‘ ì‹¤ìŠµ ë” í•´ë³´ë©´ì„œ ì´ë²ˆ ì£¼ì—ëŠ” í™•ì‹¤íˆ ì´í•´í•˜ê³  ì‹¶ë‹¤.

