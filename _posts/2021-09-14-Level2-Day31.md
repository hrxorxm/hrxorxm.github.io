---
layout: post
title:  "[Boostcamp AI Tech] 31ì¼ì°¨ í•™ìŠµì •ë¦¬"
subtitle:   "Naver Boostcamp AI Tech Level1 Day31"
categories: "Boostcamp-AI-Tech"
tags: [7ì£¼ì°¨]
use_math: true
---

# ë¶€ìŠ¤íŠ¸ìº í”„ 31ì¼ì°¨

## ğŸ“ ì˜¤ëŠ˜ ì¼ì • ì •ë¦¬

* 9/14(í™”)
  - [x] NLP ì´ë¡  ê°•ì˜
    - [x] (9ê°•) Self-supervised Pre-training Models
    - [x] (10ê°•) Other Self-supervised Pre-training Models
  - [x] ë…¼ë¬¸ ìŠ¤í„°ë””(Transformer) 13:00~14:00

## ğŸ“š ê°•ì˜ ë‚´ìš© ì •ë¦¬

### [9ê°•] Self-supervised Pre-training Models

#### [GPT-1](https://openai.com/blog/language-unsupervised/)

![image](https://user-images.githubusercontent.com/35680202/133105769-53936ee6-02ac-4c86-8c60-654759fcf37e.png)

* íŠ¹ì§•
  * ë‹¤ì–‘í•œ special tokens(`<S>, <E>, $`)ë¥¼ ì œì•ˆí•¨ (ë‹¤ì–‘í•œ taskë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ìœ„í•´)
  * Attention blockì„ ì´ 12ê°œ ìŒ“ìŒ
* Text Prediction
  * Pretraining task
  * ì²« ë‹¨ì–´ë¶€í„° ê·¸ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” íƒœìŠ¤í¬
  * => masked self-attention ì„ ì‚¬ìš©í•¨!
* Task Classifier
  * Downstream task : Classification / Entailment / Similarity / Multiple Choice
  * Task Classifierë¥¼ ìƒˆ ë ˆì´ì–´ë¡œ ê°ˆì•„ë¼ìš°ê³  Transformerì™€ í•¨ê»˜ ë‹¤ì‹œ í•™ìŠµì‹œí‚¨ë‹¤. (í•™ìŠµëœ Transformer ë¶€ë¶„ì€ learning rate ì‘ê²Œ ì£¼ê¸°)
  * `Extract` í† í° : sequence ëì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼, ì£¼ì–´ì§„ ì—¬ëŸ¬ ì…ë ¥ ë¬¸ì¥ë“¤ë¡œë¶€í„° íƒœìŠ¤í¬ì— í•„ìš”ë¡œ í•˜ëŠ” ì—¬ëŸ¬ ì •ë³´ë“¤ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•œë‹¤.
* í•œê³„ : only use left context or right context

#### [ELMo](https://arxiv.org/abs/1802.05365)

* Transformer ëŒ€ì‹  Bi-LSTM ì‚¬ìš©

#### [BERT](https://arxiv.org/abs/1810.04805)

![image](https://user-images.githubusercontent.com/35680202/133125898-e00f260e-65c8-46ae-ba3c-1eaf4071d96e.png)

* Pre-training tasks in BERT
  * Masked Language Model(MLM)
    * ì…ë ¥ í† í°ë“¤ ì¤‘ì—ì„œ k %ë¥¼ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì´ ë§ˆìŠ¤í‚¹ëœ í† í°ë“¤ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ (use k=15%)
    * kê°€ ë„ˆë¬´ í¬ë©´ ë¬¸ë§¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ì •ë³´ê°€ ë¶€ì¡±í•´ì§€ê³ , kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµ íš¨ìœ¨ì´ ë„ˆë¬´ ë–¨ì–´ì§„ë‹¤.
    * downstream taskë¥¼ ìˆ˜í–‰í•  ë•ŒëŠ” `[MASK]`ë¼ëŠ” í† í°ì´ ë“±ì¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—,  80%ë§Œ `[MASK]` í† í°ìœ¼ë¡œ ì¹˜í™˜í•˜ê³ , 10%ëŠ” ëœë¤ í† í°, 10%ëŠ” ì›ë˜ í† í°ì„ ì‚¬ìš©í•œë‹¤.
  * Next Sentence Prediction(NSP)
    * ë¬¸ì¥ levelì—ì„œì˜ taskì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ê¸°ë²•
    * `[SEP]` : ë¬¸ì¥ì„ êµ¬ë¶„í•˜ëŠ”, ë¬¸ì¥ì˜ ëì„ ì•Œë¦¬ëŠ” í† í°
    * `[CLS]` : ë¬¸ì¥ ë˜ëŠ” ì—¬ëŸ¬ ë¬¸ì¥ì—ì„œì˜ ì˜ˆì¸¡ íƒœìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ëŠ” í† í° (GPT-1ì˜ Extract í† í° ì—­í• )

* BERT Summary
  * L : Self-Attention block ìˆ˜ / H : ì¸ì½”ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ / A : Attention Head ìˆ˜
  * BERT BASE : L=12, H=768, A=12
  * BERT LARGE : L=24, H=1024, A=16

* Input Representation
  ![image](https://user-images.githubusercontent.com/35680202/133120845-813d104c-f0a8-436c-b0ad-81e0e292b065.png)
  * (Subword ë‹¨ìœ„) WordPiece embeddings
  * positional **embedding** vector ë„ í•™ìŠµê³¼ì •ì„ í†µí•´ êµ¬í•œë‹¤! (positional **encoding** (X))
  * Segment embedding : ë¬¸ì¥ êµ¬ë¶„

* BERT ì™€ GPT-1 ë¹„êµ

  |                           |                           GPT-1                           |                  BERT                   |
  | :-----------------------: | :-------------------------------------------------------: | :-------------------------------------: |
  |    Training data size     |                  BookCorpus (800M words)                  | BookCorpus and Wikipedia (2,500M words) |
  |        Batch size         |                       32,000 words                        |              128,000 words              |
  | Task-specific fine-tuning | same learning rate (5e-5) for all fine-tuning experiments | task-specific fine-tuning learning rate |



* BERTë¡œ í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ Task
  * Machine Reading Comprehension (MRC)
    * ê¸°ê³„ ë…í•´ì— ê¸°ë°˜í•œ ì§ˆì˜ì‘ë‹µ(Question Answering)
    * [SQuAD: Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) : ëŒ€í‘œì ì¸ ë°ì´í„°
    * SQuAD 1.1 : ë‹µì´ ìˆëŠ” ê³³ì˜ start ë¶€ë¶„ê³¼ end ë¶€ë¶„ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ FC layerë¥¼ ê°ê° ë‘¬ì„œ í•™ìŠµí•œë‹¤.
    * SQuAD 2.0 : ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ì—†ëŠ” ê²½ìš°ê¹Œì§€ í¬í•¨í•´ì„œ í•™ìŠµ, ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë¨¼ì € ë‹µì´ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ `[CLS]` í† í°ì„ ì´ìš©í•˜ì—¬ ì²´í¬í•œ í›„, ë‹µì´ ìˆë‹¤ë©´ ê·¸ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
  * On SWAG
    * [SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference](https://leaderboard.allenai.org/swag/submissions/public)
    * ì£¼ì–´ì§„ ë¬¸ì¥ ë‹¤ìŒì— ë‚˜íƒ€ë‚ ë²•í•œ ì ì ˆí•œ ë¬¸ì¥ì„ ì„ íƒí•˜ëŠ” íƒœìŠ¤í¬
    * ê° í›„ë³´ì— ëŒ€í•´ì„œ ì£¼ì–´ì§„ ë¬¸ì¥ê³¼ concatí•œ í›„ BERTë¥¼ í†µí•´ ì¸ì½”ë”©í•˜ê³  ë‚˜ì˜¨ `[CLS]` í† í°ì„ ì´ìš©í•´ì„œ logitì„ êµ¬í•œë‹¤.
* BERT: Ablation Study
  * ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ ë” í‚¤ìš¸ìˆ˜ë¡ ì—¬ëŸ¬ downstream taskì— ëŒ€í•œ ì„±ëŠ¥ì´ ëŠì„ì—†ì´ ì¢‹ì•„ì§„ë‹¤.

### [10ê°•] Other Self-supervised Pre-training Models

#### [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

* **Language Models are Unsupervised Multitask Learner**s
* Motivation : [decaNLP](https://decanlp.com/)
  * Multitask Learning as Question Answering
  * ë‹¤ì–‘í•œ taskë“¤ì„ ë‹¤ ì§ˆì˜ì‘ë‹µ í˜•íƒœë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì—°êµ¬ ì‚¬ë¡€
* Datasets
  * í•™ìŠµ ë°ì´í„°ì…‹ì„ ì¢€ ë” ì¢‹ì€ í€„ë¦¬í‹°ì—ë‹¤ê°€ ì–‘ë„ ëŠ˜ë ¸ë‹¤. (Reddit ì»¤ë®¤ë‹ˆí‹° ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì¡°ê±´ì ìœ¼ë¡œ í¬ë¡¤ë§)
  * Preprocess : (Subword ë‹¨ìœ„) Byte pair encoding (BPE)
* Modification
  * GPT-1ì—ì„œ ë ˆì´ì–´ë¥¼ ë” ë§ì´ ìŒ“ì•˜ë‹¤. (pretrain taskëŠ” ë˜‘ê°™ë‹¤)
  * Layer normalizationì˜ ìœ„ì¹˜ê°€ ì˜®ê²¨ì§
  * ë ˆì´ì–´ë¥¼ random initializationí•  ë•Œ, ë ˆì´ì–´ì˜ ì¸ë±ìŠ¤ì— ë°˜ë¹„ë¡€í•˜ê²Œ ë” ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ë„ë¡ í•œë‹¤. (ìœ„ìª½ ë ˆì´ì–´ë“¤ì˜ ì—­í• ì´ ì¤„ì–´ë“¤ ìˆ˜ ìˆë„ë¡)
* ì—¬ëŸ¬ downstream taskê°€ language ìƒì„± taskì—ì„œì˜ (fine-tuning ê³¼ì • ì—†ì´) zero-shot settingìœ¼ë¡œ ë‹¤ ë‹¤ë¤„ì§ˆ ìˆ˜ ìˆë‹¤.
  * Conversation question answering dataset(CoQA)
  * Summarization : `TL;DR`ì„ ë§Œë‚˜ë©´ ê·¸ ì•ìª½ ê¸€ì„ í•œ ì¤„ ìš”ì•½í•œë‹¤.
  * Translation : ë§ˆì°¬ê°€ì§€ë¡œ ë²ˆì—­í•˜ê³  ì‹¶ì€ ë¬¸ì¥ ë’¤ì— `in French`ì´ëŸ° ë¬¸êµ¬ë¥¼ ë„£ì–´ì£¼ë©´ ë²ˆì—­í•œë‹¤.
* ì˜ˆì‹œ : [How to Build OpenAIâ€™s GPT-2: â€œ The AI That Was Too Dangerous to Releaseâ€](https://blog.floydhub.com/gpt2/)

#### [GPT-3](https://arxiv.org/abs/2005.14165)

* **Language Models are Few-Shot Learners**
  * Zero-shot : íƒœìŠ¤í¬ì˜ ìì—°ì–´ ì„¤ëª…ë§Œ ì£¼ì–´ì§„ ì±„ ë‹µì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
  * One-shot : íƒœìŠ¤í¬ ì„¤ëª…ê³¼ ë”ë¶ˆì–´ í•˜ë‚˜ì˜ íƒœìŠ¤í¬ exampleì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ
  * Few-shot : íƒœìŠ¤í¬ì˜ few exampleì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ
    ![image](https://user-images.githubusercontent.com/35680202/133131557-527ebc0b-3cbb-4c01-a056-d6b36a94b9b9.png)
  * ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ í‚¤ìš¸ìˆ˜ë¡ ì„±ëŠ¥ì˜ gapì´ í›¨ì”¬ ë” í¬ê²Œ ì˜¬ë¼ê°„ë‹¤. (í° ëª¨ë¸ì„ ì‚¬ìš©í• ìˆ˜ë¡ ë™ì ì¸ ì ì‘ëŠ¥ë ¥ì´ í›¨ì”¬ ë” ë›°ì–´ë‚˜ë‹¤.)
* GPT-2 ë³´ë‹¤ ë” ë§ì€ Self-Attention block, ë” ë§ì€ íŒŒë¼ë¯¸í„° ìˆ˜, ë” ë§ì€ ë°ì´í„°, ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ

#### [ALBERT](https://arxiv.org/abs/1909.11942)

* A Lite BERT for Self-supervised Learning of Language Representations (ê²½ëŸ‰í™”ëœ í˜•íƒœì˜ BERT)
* Factorized Embedding Parameterization
  * $V$ = Vocabulary size (ex. 500)
  * $H$ = Hidden-state dimension (ex. 100)
  * $E$ = Word embedding dimension (ex. 15)
  * íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
    * BERT : $V \times H = 500 \times 100$
    * ALBERT : $(V \times E) + (E \times H) = 500 \times 15 + 15 \times 100$ (dimensionì„ ëŠ˜ë ¤ì£¼ëŠ” ë ˆì´ì–´ í•˜ë‚˜ë¥¼ ì¶”ê°€í•¨)

* Cross-layer Parameter Sharing
  ![image](https://user-images.githubusercontent.com/35680202/133187651-6b843036-d0ea-4883-b3d9-ed83f559120b.png)
  * Shared-FFN : ë ˆì´ì–´ë“¤ ê°„ì— feed-forward ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°ë§Œ ê³µìœ í•˜ê¸°
  * Shared-attention : ë ˆì´ì–´ë“¤ ê°„ì— attention íŒŒë¼ë¯¸í„°ë§Œ ê³µìœ í•˜ê¸°
  * All-shared : Both of them, íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì œì¼ ì‘ìœ¼ë©´ì„œë„ ì„±ëŠ¥ì˜ í•˜ë½í­ì´ í¬ì§€ ì•Šë‹¤.

* Sentence Order Prediction (For Performance)
  ![image](https://user-images.githubusercontent.com/35680202/133188559-881c4eb0-0952-4bdd-9b0a-bc2e438d8e81.png)
  * ë™ê¸° : Next Sentence Prediction pretraining taskê°€ ë„ˆë¬´ ì‰½ë‹¤. (ì‹¤íš¨ì„±ì´ ì—†ë‹¤) Negative samplingì„ í†µí•´ì„œ ë³„ë„ì˜ ë¬¸ì„œì—ì„œ ë½‘ì€ ë‘ ë¬¸ì¥ ê°„ì— ê³‚ì¹˜ëŠ” ë‹¨ì–´ë‚˜ ì£¼ì œê°€ ê±°ì˜ ì—†ì„ ê°€ëŠ¥ì„±ì´ í¬ê¸° ë•Œë¬¸ì´ë‹¤.
  * ë°©ë²• : ì´ì–´ì§€ëŠ” ë‘ ë¬¸ì¥ì„ ê°€ì ¸ì™€ì„œ ë‘ ë¬¸ì¥ ìˆœì„œë¥¼ ë°”ê¾¼ í›„ ìˆœì„œê°€ ì˜¬ë°”ë¥¸ì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
  * ê²°ê³¼ : ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì¡Œë‹¤!

#### [ELECTRA](https://arxiv.org/abs/2003.10555)

![image](https://user-images.githubusercontent.com/35680202/133189810-d20573ad-d5da-428e-a1f5-1329b1c349c1.png)

* Pre-training Text Encoders as Discriminators Rather Than Generators
* Efficiently Learning as Encoder that Classifies Token Replacements Accurately
* GAN(Generative adversarial network)ì—ì„œ ì‚¬ìš©í•œ ì•„ì´ë””ì–´ì—ì„œ ì°©ì•ˆí•˜ì—¬ ë‘ê°€ì§€ ëª¨ë¸ì´ ì ëŒ€ì  í•™ìŠµ(Adversarial learning) ì§„í–‰
  * Generator : BERTì™€ ë¹„ìŠ·í•œ ëª¨ë¸ë¡œ í•™ìŠµ
  * Discriminator : Generatorê°€ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì›ë˜ ìˆì—ˆë˜ ë‹¨ì–´ì¸ì§€ íŒë³„í•˜ëŠ” íƒœìŠ¤í¬ ìˆ˜í–‰ (ê° ë‹¨ì–´ë³„ë¡œ binary classification) -> ì´ ëª¨ë¸ì„ pretrainëœ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ downstream taskì—ì„œ fine-tuningí•˜ì—¬ ì‚¬ìš©í•œë‹¤.

#### Light-weight Models

- ê²½ëŸ‰í™” ëª¨ë¸ë“¤ : ì†Œí˜• ë””ë°”ì´ìŠ¤ì—ì„œ ë¡œë“œí•´ì„œ ê³„ì‚° ê°€ëŠ¥!
- [DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, NeurIPS Workshop'19](https://arxiv.org/abs/1910.01108)
  - huggingfaceì—ì„œ ë°œí‘œ
  - student model(ê²½ëŸ‰í™”ëœ ëª¨ë¸)ì´ teacher model(í° ì‚¬ì´ì¦ˆ ëª¨ë¸)ì„ ì˜ ëª¨ì‚¬í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµ ì§„í–‰ => **ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)**
- [TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLPâ€™20](https://arxiv.org/abs/1909.10351)
  - ì—­ì‹œ Knowledge Distillationë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ëŸ‰í™”
  - teacher modelì˜ target distributionì„ ëª¨ì‚¬í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼, **ì¤‘ê°„ ê²°ê³¼ë¬¼ë“¤ê¹Œì§€ë„ ìœ ì‚¬í•´ì§€ë„ë¡ í•™ìŠµì„ ì§„í–‰** : ì„ë² ë”© ë ˆì´ì–´, Self-Attention blockì´ ê°€ì§€ëŠ” $W_Q, W_K, W_V$ (Attention matrix), ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” hidden state vector ë“±
  - dimensionê°„ì˜ mismatchê°€ ìˆëŠ” ê²½ìš° ì°¨ì› ë³€í™˜ì„ ìœ„í•œ (í•™ìŠµ ê°€ëŠ¥í•œ) FC layerë¥¼ í•˜ë‚˜ ë‘¬ì„œ í•´ê²°!

#### Fusing Knowledge Graph into Language Model

- BERTê°€ ì–¸ì–´ì  íŠ¹ì„±ì„ ì˜ ì´í•´í•˜ê³  ìˆëŠ”ì§€ ë¶„ì„
  - ì£¼ì–´ì§„ ë¬¸ì¥, ê¸€ì´ ìˆì„ ë•Œ ë¬¸ë§¥ì„ ì˜ íŒŒì•…í•˜ê³  ë‹¨ì–´ë“¤ê°„ì˜ ìœ ì‚¬ë„, ê´€ê³„ë¥¼ ì˜ íŒŒì•…í•œë‹¤.
  - ì£¼ì–´ì§„ ë¬¸ì¥ì— í¬í•¨ë˜ì–´ìˆì§€ ì•Šì€ ì¶”ê°€ì ì¸ ì •ë³´ê°€ í•„ìš”í•  ë•ŒëŠ” íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ëª»í•œë‹¤.
- Knowledge Graph : ì´ ì„¸ìƒì— ì¡´ì¬í•˜ëŠ” ë‹¤ì–‘í•œ ê°œë…ì´ë‚˜ ê°œì²´ë“¤ì„ ì˜ ì •ì˜í•˜ê³  ê·¸ë“¤ê°„ì˜ ê´€ê³„ë¥¼ ì˜ ì •í˜•í™”í•´ì„œ ë§Œë“¤ì–´ë‘” ê²ƒ
- => ì™¸ë¶€ ì •ë³´ë¥¼ Knowledge Graphë¡œ ì˜ ì •ì˜í•˜ê³  Language Modelê³¼ ê²°í•©í•˜ëŠ” ì—°êµ¬ê°€ ì§„í–‰ë¨
- [ERNIE: Enhanced Language Representation with Informative Entities, ACL'19](https://arxiv.org/abs/1905.07129)
- [KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning, EMNLP'19](https://arxiv.org/abs/1909.02151)

## ğŸ“– ë…¼ë¬¸ ìŠ¤í„°ë””

### [Transformer](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

- ë‚´ìš© : [Sequence Transduction with Recurrent Neural Networks](https://arxiv.org/abs/1211.3711) ë¥¼ Self-Attentionë§Œìœ¼ë¡œ í•´ê²°í•œë‹¤! (Self-Attentionì„ ì†Œê°œí•œ ë…¼ë¬¸ì€ ì•„ë‹˜)
  - [ì°¸ê³  ë¸”ë¡œê·¸ : Sequence prediction - transductive learning](https://dos-tacos.github.io/translation/transductive-learning/)
- êµ¬ì¡°
  - Input Embeddings + Positional Encodings
    - [Positional Encoding ì½”ë“œ êµ¬í˜„](https://gist.github.com/jinmang2/c2d41939ee9800b9d9d8d485059870cb)
  - Self-Attention
    - [Visualizing the Inner Workings of Attention](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)
  - Multi-Head Attention
    - Self-Attentionì€ symmetric í•˜ì§€ ì•Šë‹¤.
    - ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•˜ì§€ë§Œ ì–´ëŠ ì •ë„ ê³µí†µì ì¸ ë¶€ë¶„ì„ í•™ìŠµí•œë‹¤.
      - Multi-Headë¥¼ í•©ì¹œ ì´í›„ì˜ Attention MatrixëŠ” row-rankë‹¤.
      - ì„œë¡œ ì¢…ì†ì¸ ë²¡í„°ë“¤ì´ ëŠ˜ì–´ë‚œë‹¤. (ê³µí†µëœ í‘œí˜„ì„ ë°°ìš´ë‹¤.)
    - [Visualizing Multi-Head Attention](https://docs.dgl.ai/en/0.4.x/tutorials/models/4_old_wines/7_transformer.html#multi-head-attention)
  - Layer Normalization : PLM transfer learningì˜ key ingredientì´ë‹¤. ì¦‰, fine-tuningí•  ë•Œ ì¤‘ìš”í•˜ë‹¤.
  - Residual Connection & Feed Forward Network ê°€ ì—†ìœ¼ë©´ attention matrixê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ 1-rank matrixë¡œ ìˆ˜ë ´í•œë‹¤. (í•™ìŠµì´ ê±°ì˜ ì•ˆ ëœë‹¤.)
  - Decoder : Autoregressive Model
  - Masked Self-Attention
  - Cross-Attention : Bahdanau Attentionì—ì„œ í•™ìŠµí•œ Attention Mechanismê³¼ ë™ì¼
  - $O(n^2 \cdot d)$ì´ê¸° ë•Œë¬¸ì— ì…ë ¥ ê¸¸ì´ n ì´ ê¸¸ì–´ì§€ë©´ transformer bottleneckì´ ë°œìƒí•œë‹¤.
  - Transformer í•™ìŠµì—ì„œ Warm-up ì´ ì§„ì§œ í•„ìˆ˜ë‹¤.
    - Post-Layer Normalization êµ¬ì¡°ê°€ ë¬¸ì œì˜€ë‹¤! => layernorm(x + f(x))
    - ìˆœì„œë¥¼ ë°”ê¾¸ë©´ ê´œì°®ë‹¤. => x + f(layernorm(x))
  - í•µì‹¬ : ìˆ˜ ë§ì€ ë…¼ë¬¸ì—ì„œ Transformer êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ëª¨ë“ ê±¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤!
- í† ì˜ ë‚´ìš© : layernormì—ì„œ normalizeëŠ” ë‹¨ì–´ë³„ë¡œ ì§„í–‰ë˜ê³ , affineì€ dimensionë³„ë¡œ ì§„í–‰ëœë‹¤.
  - [allenaiì—ì„œì˜ êµ¬í˜„](https://github.com/allenai/allennlp/blob/main/allennlp/modules/layer_norm.py)

## ğŸŒ± í”¼ì–´ ì„¸ì…˜ ì •ë¦¬

* ì„ íƒê³¼ì œ 2ë²ˆ ì„œë²„ì—ì„œì˜ í™˜ê²½ì„¤ì • ë„ì›€ ë§í¬
  * [fastBPE ì„¤ì¹˜](https://www.gitmemory.com/issue/glample/fastBPE/45/674878162)

* ALBERT ì—ì„œ ìª¼ê°œê³  ë‹¤ì‹œ ëŠ˜ë¦¬ëŠ” ë¶€ë¶„ì´ ëª¨ë‘ 2) ì„ë² ë”© ë ˆì´ì–´ ì—ì„œ ì¼ì–´ë‚˜ëŠ”ê±¸ê¹Œ? ì•„ë‹ˆë©´ 2) ì—ì„œ ìª¼ê°œê³ , 5) ì—ì„œ Zë¥¼ ë§Œë“  ì´í›„ì— ë‹¤ì‹œ ëŠ˜ë¦¬ëŠ”ê±¸ê¹Œ?
  ![image](https://user-images.githubusercontent.com/35680202/133232129-dce679c4-ade3-402e-b0ce-3afd143d7f1a.png)
  * ë‹µë³€ : [Visual Paper Summary: ALBERT](https://amitness.com/2020/02/albert-visual-summary/) => 2) ì„ë² ë”© ë ˆì´ì–´ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì´ë‹¤!

## ğŸš€ í•™ìŠµ íšŒê³ 

* ë°°ìš°ë©´ ë°°ìš¸ìˆ˜ë¡ ì°¸ ë°°ìš¸ê²Œ ë§ë‹¤ê³  ëŠë‚€ë‹¤. ê·¸ë˜ë„ ë‚´ìš©ì´ ë‹¤ ì¬ë°Œì–´ì„œ ë‹¤í–‰ì´ë‹¤.
* ì´ë²ˆ ì£¼ ê°•ì˜ë³¼ê±´ ë‹¤ ëë‚¬ì§€ë§Œ ê³„ì† `why`ì— ì´ˆì²¨ë§ì¶°ì„œ ìì„¸íˆ ëœ¯ì–´ë³´ë©° ê³µë¶€í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.

