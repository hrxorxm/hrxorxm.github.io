---
layout: post
title:  "[Boostcamp AI Tech] 31일차 학습정리"
subtitle:   "Naver Boostcamp AI Tech Level1 Day31"
categories: "Boostcamp-AI-Tech"
tags: [7주차]
use_math: true
---

# 부스트캠프 31일차

## 📝 오늘 일정 정리

* 9/14(화)
  - [x] NLP 이론 강의
    - [x] (9강) Self-supervised Pre-training Models
    - [x] (10강) Other Self-supervised Pre-training Models
  - [x] 논문 스터디(Transformer) 13:00~14:00

## 📚 강의 내용 정리

### [9강] Self-supervised Pre-training Models

#### [GPT-1](https://openai.com/blog/language-unsupervised/)

![image](https://user-images.githubusercontent.com/35680202/133105769-53936ee6-02ac-4c86-8c60-654759fcf37e.png)

* 특징
  * 다양한 special tokens(`<S>, <E>, $`)를 제안함 (다양한 task를 처리할 수 있는 모델을 위해)
  * Attention block을 총 12개 쌓음
* Text Prediction
  * Pretraining task
  * 첫 단어부터 그 다음 단어를 순차적으로 예측하는 태스크
  * => masked self-attention 을 사용함!
* Task Classifier
  * Downstream task : Classification / Entailment / Similarity / Multiple Choice
  * Task Classifier를 새 레이어로 갈아끼우고 Transformer와 함께 다시 학습시킨다. (학습된 Transformer 부분은 learning rate 작게 주기)
  * `Extract` 토큰 : sequence 끝을 나타내는 것 뿐만 아니라, 주어진 여러 입력 문장들로부터 태스크에 필요로 하는 여러 정보들을 추출하는 역할을 한다.
* 한계 : only use left context or right context

#### [ELMo](https://arxiv.org/abs/1802.05365)

* Transformer 대신 Bi-LSTM 사용

#### [BERT](https://arxiv.org/abs/1810.04805)

![image](https://user-images.githubusercontent.com/35680202/133125898-e00f260e-65c8-46ae-ba3c-1eaf4071d96e.png)

* Pre-training tasks in BERT
  * Masked Language Model(MLM)
    * 입력 토큰들 중에서 k %를 마스킹하여 이 마스킹된 토큰들을 예측하도록 학습 (use k=15%)
    * k가 너무 크면 문맥을 파악하기 위한 정보가 부족해지고, k가 너무 작으면 학습 효율이 너무 떨어진다.
    * downstream task를 수행할 때는 `[MASK]`라는 토큰이 등장하지 않기 때문에,  80%만 `[MASK]` 토큰으로 치환하고, 10%는 랜덤 토큰, 10%는 원래 토큰을 사용한다.
  * Next Sentence Prediction(NSP)
    * 문장 level에서의 task에 대응하기 위한 기법
    * `[SEP]` : 문장을 구분하는, 문장의 끝을 알리는 토큰
    * `[CLS]` : 문장 또는 여러 문장에서의 예측 태스트를 수행하는 역할을 담당하는 토큰 (GPT-1의 Extract 토큰 역할)

* BERT Summary
  * L : Self-Attention block 수 / H : 인코딩 벡터의 차원 수 / A : Attention Head 수
  * BERT BASE : L=12, H=768, A=12
  * BERT LARGE : L=24, H=1024, A=16

* Input Representation
  ![image](https://user-images.githubusercontent.com/35680202/133120845-813d104c-f0a8-436c-b0ad-81e0e292b065.png)
  * (Subword 단위) WordPiece embeddings
  * positional **embedding** vector 도 학습과정을 통해 구한다! (positional **encoding** (X))
  * Segment embedding : 문장 구분

* BERT 와 GPT-1 비교

  |                           |                           GPT-1                           |                  BERT                   |
  | :-----------------------: | :-------------------------------------------------------: | :-------------------------------------: |
  |    Training data size     |                  BookCorpus (800M words)                  | BookCorpus and Wikipedia (2,500M words) |
  |        Batch size         |                       32,000 words                        |              128,000 words              |
  | Task-specific fine-tuning | same learning rate (5e-5) for all fine-tuning experiments | task-specific fine-tuning learning rate |



* BERT로 할 수 있는 대표적인 Task
  * Machine Reading Comprehension (MRC)
    * 기계 독해에 기반한 질의응답(Question Answering)
    * [SQuAD: Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) : 대표적인 데이터
    * SQuAD 1.1 : 답이 있는 곳의 start 부분과 end 부분을 예측하기 위한 FC layer를 각각 둬서 학습한다.
    * SQuAD 2.0 : 질문에 대한 답이 없는 경우까지 포함해서 학습, 질문에 대해서 먼저 답이 있는지 없는지 `[CLS]` 토큰을 이용하여 체크한 후, 답이 있다면 그 위치를 찾는 방식으로 학습
  * On SWAG
    * [SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference](https://leaderboard.allenai.org/swag/submissions/public)
    * 주어진 문장 다음에 나타날법한 적절한 문장을 선택하는 태스크
    * 각 후보에 대해서 주어진 문장과 concat한 후 BERT를 통해 인코딩하고 나온 `[CLS]` 토큰을 이용해서 logit을 구한다.
* BERT: Ablation Study
  * 모델 사이즈를 더 키울수록 여러 downstream task에 대한 성능이 끊임없이 좋아진다.

### [10강] Other Self-supervised Pre-training Models

#### [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

* **Language Models are Unsupervised Multitask Learner**s
* Motivation : [decaNLP](https://decanlp.com/)
  * Multitask Learning as Question Answering
  * 다양한 task들을 다 질의응답 형태로 변환할 수 있다는 연구 사례
* Datasets
  * 학습 데이터셋을 좀 더 좋은 퀄리티에다가 양도 늘렸다. (Reddit 커뮤니티 웹사이트에서 조건적으로 크롤링)
  * Preprocess : (Subword 단위) Byte pair encoding (BPE)
* Modification
  * GPT-1에서 레이어를 더 많이 쌓았다. (pretrain task는 똑같다)
  * Layer normalization의 위치가 옮겨짐
  * 레이어를 random initialization할 때, 레이어의 인덱스에 반비례하게 더 작은 값으로 초기화되도록 한다. (위쪽 레이어들의 역할이 줄어들 수 있도록)
* 여러 downstream task가 language 생성 task에서의 (fine-tuning 과정 없이) zero-shot setting으로 다 다뤄질 수 있다.
  * Conversation question answering dataset(CoQA)
  * Summarization : `TL;DR`을 만나면 그 앞쪽 글을 한 줄 요약한다.
  * Translation : 마찬가지로 번역하고 싶은 문장 뒤에 `in French`이런 문구를 넣어주면 번역한다.
* 예시 : [How to Build OpenAI’s GPT-2: “ The AI That Was Too Dangerous to Release”](https://blog.floydhub.com/gpt2/)

#### [GPT-3](https://arxiv.org/abs/2005.14165)

* **Language Models are Few-Shot Learners**
  * Zero-shot : 태스크의 자연어 설명만 주어진 채 답을 예측하는 것
  * One-shot : 태스크 설명과 더불어 하나의 태스크 example을 보여주는 것
  * Few-shot : 태스크의 few example을 보여주는 것
    ![image](https://user-images.githubusercontent.com/35680202/133131557-527ebc0b-3cbb-4c01-a056-d6b36a94b9b9.png)
  * 모델 사이즈를 키울수록 성능의 gap이 훨씬 더 크게 올라간다. (큰 모델을 사용할수록 동적인 적응능력이 훨씬 더 뛰어나다.)
* GPT-2 보다 더 많은 Self-Attention block, 더 많은 파라미터 수, 더 많은 데이터, 더 큰 배치 사이즈

#### [ALBERT](https://arxiv.org/abs/1909.11942)

* A Lite BERT for Self-supervised Learning of Language Representations (경량화된 형태의 BERT)
* Factorized Embedding Parameterization
  * $V$ = Vocabulary size (ex. 500)
  * $H$ = Hidden-state dimension (ex. 100)
  * $E$ = Word embedding dimension (ex. 15)
  * 파라미터 수 비교
    * BERT : $V \times H = 500 \times 100$
    * ALBERT : $(V \times E) + (E \times H) = 500 \times 15 + 15 \times 100$ (dimension을 늘려주는 레이어 하나를 추가함)

* Cross-layer Parameter Sharing
  ![image](https://user-images.githubusercontent.com/35680202/133187651-6b843036-d0ea-4883-b3d9-ed83f559120b.png)
  * Shared-FFN : 레이어들 간에 feed-forward 네트워크의 파라미터만 공유하기
  * Shared-attention : 레이어들 간에 attention 파라미터만 공유하기
  * All-shared : Both of them, 파라미터 수가 제일 작으면서도 성능의 하락폭이 크지 않다.

* Sentence Order Prediction (For Performance)
  ![image](https://user-images.githubusercontent.com/35680202/133188559-881c4eb0-0952-4bdd-9b0a-bc2e438d8e81.png)
  * 동기 : Next Sentence Prediction pretraining task가 너무 쉽다. (실효성이 없다) Negative sampling을 통해서 별도의 문서에서 뽑은 두 문장 간에 곂치는 단어나 주제가 거의 없을 가능성이 크기 때문이다.
  * 방법 : 이어지는 두 문장을 가져와서 두 문장 순서를 바꾼 후 순서가 올바른지 예측하는 것
  * 결과 : 성능이 더 좋아졌다!

#### [ELECTRA](https://arxiv.org/abs/2003.10555)

![image](https://user-images.githubusercontent.com/35680202/133189810-d20573ad-d5da-428e-a1f5-1329b1c349c1.png)

* Pre-training Text Encoders as Discriminators Rather Than Generators
* Efficiently Learning as Encoder that Classifies Token Replacements Accurately
* GAN(Generative adversarial network)에서 사용한 아이디어에서 착안하여 두가지 모델이 적대적 학습(Adversarial learning) 진행
  * Generator : BERT와 비슷한 모델로 학습
  * Discriminator : Generator가 예측한 단어가 원래 있었던 단어인지 판별하는 태스크 수행 (각 단어별로 binary classification) -> 이 모델을 pretrain된 모델로 사용하여 downstream task에서 fine-tuning하여 사용한다.

#### Light-weight Models

- 경량화 모델들 : 소형 디바이스에서 로드해서 계산 가능!
- [DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, NeurIPS Workshop'19](https://arxiv.org/abs/1910.01108)
  - huggingface에서 발표
  - student model(경량화된 모델)이 teacher model(큰 사이즈 모델)을 잘 모사할 수 있도록 학습 진행 => **지식 증류(Knowledge Distillation)**
- [TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLP’20](https://arxiv.org/abs/1909.10351)
  - 역시 Knowledge Distillation를 사용하여 경량화
  - teacher model의 target distribution을 모사하는 것 뿐만 아니라, **중간 결과물들까지도 유사해지도록 학습을 진행** : 임베딩 레이어, Self-Attention block이 가지는 $W_Q, W_K, W_V$ (Attention matrix), 결과로 나오는 hidden state vector 등
  - dimension간의 mismatch가 있는 경우 차원 변환을 위한 (학습 가능한) FC layer를 하나 둬서 해결!

#### Fusing Knowledge Graph into Language Model

- BERT가 언어적 특성을 잘 이해하고 있는지 분석
  - 주어진 문장, 글이 있을 때 문맥을 잘 파악하고 단어들간의 유사도, 관계를 잘 파악한다.
  - 주어진 문장에 포함되어있지 않은 추가적인 정보가 필요할 때는 효과적으로 활용하지 못한다.
- Knowledge Graph : 이 세상에 존재하는 다양한 개념이나 개체들을 잘 정의하고 그들간의 관계를 잘 정형화해서 만들어둔 것
- => 외부 정보를 Knowledge Graph로 잘 정의하고 Language Model과 결합하는 연구가 진행됨
- [ERNIE: Enhanced Language Representation with Informative Entities, ACL'19](https://arxiv.org/abs/1905.07129)
- [KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning, EMNLP'19](https://arxiv.org/abs/1909.02151)

## 📖 논문 스터디

### [Transformer](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

- 내용 : [Sequence Transduction with Recurrent Neural Networks](https://arxiv.org/abs/1211.3711) 를 Self-Attention만으로 해결한다! (Self-Attention을 소개한 논문은 아님)
  - [참고 블로그 : Sequence prediction - transductive learning](https://dos-tacos.github.io/translation/transductive-learning/)
- 구조
  - Input Embeddings + Positional Encodings
    - [Positional Encoding 코드 구현](https://gist.github.com/jinmang2/c2d41939ee9800b9d9d8d485059870cb)
  - Self-Attention
    - [Visualizing the Inner Workings of Attention](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)
  - Multi-Head Attention
    - Self-Attention은 symmetric 하지 않다.
    - 독립적으로 학습하지만 어느 정도 공통적인 부분을 학습한다.
      - Multi-Head를 합친 이후의 Attention Matrix는 row-rank다.
      - 서로 종속인 벡터들이 늘어난다. (공통된 표현을 배운다.)
    - [Visualizing Multi-Head Attention](https://docs.dgl.ai/en/0.4.x/tutorials/models/4_old_wines/7_transformer.html#multi-head-attention)
  - Layer Normalization : PLM transfer learning의 key ingredient이다. 즉, fine-tuning할 때 중요하다.
  - Residual Connection & Feed Forward Network 가 없으면 attention matrix가 기하급수적으로 1-rank matrix로 수렴한다. (학습이 거의 안 된다.)
  - Decoder : Autoregressive Model
  - Masked Self-Attention
  - Cross-Attention : Bahdanau Attention에서 학습한 Attention Mechanism과 동일
  - $O(n^2 \cdot d)$이기 때문에 입력 길이 n 이 길어지면 transformer bottleneck이 발생한다.
  - Transformer 학습에서 Warm-up 이 진짜 필수다.
    - Post-Layer Normalization 구조가 문제였다! => layernorm(x + f(x))
    - 순서를 바꾸면 괜찮다. => x + f(layernorm(x))
  - 핵심 : 수 많은 논문에서 Transformer 구조를 사용하지만 모든걸 그대로 사용하지는 않는다!
- 토의 내용 : layernorm에서 normalize는 단어별로 진행되고, affine은 dimension별로 진행된다.
  - [allenai에서의 구현](https://github.com/allenai/allennlp/blob/main/allennlp/modules/layer_norm.py)

## 🌱 피어 세션 정리

* 선택과제 2번 서버에서의 환경설정 도움 링크
  * [fastBPE 설치](https://www.gitmemory.com/issue/glample/fastBPE/45/674878162)

* ALBERT 에서 쪼개고 다시 늘리는 부분이 모두 2) 임베딩 레이어 에서 일어나는걸까? 아니면 2) 에서 쪼개고, 5) 에서 Z를 만든 이후에 다시 늘리는걸까?
  ![image](https://user-images.githubusercontent.com/35680202/133232129-dce679c4-ade3-402e-b0ce-3afd143d7f1a.png)
  * 답변 : [Visual Paper Summary: ALBERT](https://amitness.com/2020/02/albert-visual-summary/) => 2) 임베딩 레이어에서 일어나는 일이다!

## 🚀 학습 회고

* 배우면 배울수록 참 배울게 많다고 느낀다. 그래도 내용이 다 재밌어서 다행이다.
* 이번 주 강의볼건 다 끝났지만 계속 `why`에 초첨맞춰서 자세히 뜯어보며 공부하려고 노력해야겠다는 생각이 들었다.

